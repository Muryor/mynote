#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
export_image_jobs.py - ä» converted_exam.tex æå– IMAGE_TODO å—ç”Ÿæˆ JSONL

åŠŸèƒ½ï¼š
1. è§£æ TeX æ–‡ä»¶ä¸­çš„ IMAGE_TODO_START/END æ³¨é‡Šå—
2. æå–å›¾ç‰‡å…ƒæ•°æ®ï¼ˆid, path, width, inline, context ç­‰ï¼‰
3. ç”Ÿæˆ image_jobs.jsonl ä¾› AI Agent æ‰¹é‡å¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•ä¸ªæ–‡ä»¶
    python tools/images/export_image_jobs.py \\
        --files content/exams/auto/nanjing2026/converted_exam.tex \\
        --output word_to_tex/output/nanjing2026_image_jobs.jsonl

    # å¤šä¸ªæ–‡ä»¶
    python tools/images/export_image_jobs.py \\
        --files content/exams/auto/*/converted_exam.tex \\
        --output word_to_tex/output/all_image_jobs.jsonl

è¾“å‡ºæ ¼å¼ï¼ˆJSONLï¼‰ï¼š
    æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
    - id: å›¾ç‰‡å”¯ä¸€æ ‡è¯†ç¬¦
    - exam_slug: è¯•å· slug
    - tex_file: æ¥æº TeX æ–‡ä»¶è·¯å¾„
    - question_index: é¢˜å·
    - sub_index: å°é—®ç¼–å·
    - path: å›¾ç‰‡ç›¸å¯¹è·¯å¾„
    - width_pct: å®½åº¦ç™¾åˆ†æ¯”ï¼ˆæ•´æ•°ï¼‰
    - inline: æ˜¯å¦ä¸ºå†…è”å›¾ç‰‡ï¼ˆå¸ƒå°”å€¼ï¼‰
    - context_before: å›¾ç‰‡å‰æ–‡
    - context_after: å›¾ç‰‡åæ–‡
    - todo_block_start_line: IMAGE_TODO_START è¡Œå·ï¼ˆ1-basedï¼‰
    - todo_block_end_line: IMAGE_TODO_END è¡Œå·ï¼ˆ1-basedï¼‰
"""

import re
import json
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Tuple


def parse_kv_line(line: str) -> Dict[str, str]:
    """è§£æ IMAGE_TODO_START è¡Œä¸­çš„ key=value å¯¹

    Args:
        line: IMAGE_TODO_START æ³¨é‡Šè¡Œ

    Returns:
        é”®å€¼å¯¹å­—å…¸

    Example:
        >>> parse_kv_line("% IMAGE_TODO_START id=test-Q1-img1 path=media/img.png width=60% inline=false")
        {'id': 'test-Q1-img1', 'path': 'media/img.png', 'width': '60%', 'inline': 'false'}
    """
    kv_dict = {}
    # åŒ¹é… key=value æ¨¡å¼ï¼Œvalue å¯ä»¥åŒ…å«è·¯å¾„å­—ç¬¦
    pattern = r'(\w+)=([\w\-./]+%?)'
    matches = re.findall(pattern, line)
    for key, value in matches:
        kv_dict[key] = value
    return kv_dict


def extract_context_line(line: str, prefix: str) -> Optional[str]:
    """æå– CONTEXT_BEFORE/AFTER è¡Œçš„å†…å®¹

    Args:
        line: æ³¨é‡Šè¡Œ
        prefix: "CONTEXT_BEFORE" æˆ– "CONTEXT_AFTER"

    Returns:
        ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œå¦‚æœä¸åŒ¹é…åˆ™è¿”å› None
    """
    pattern = rf'%\s*{prefix}:\s*(.*)$'
    match = re.match(pattern, line)
    if match:
        return match.group(1).strip()
    return None


def extract_slug_from_path(tex_file: Path) -> str:
    """ä» TeX æ–‡ä»¶è·¯å¾„æå– exam_slug

    Args:
        tex_file: TeX æ–‡ä»¶è·¯å¾„

    Returns:
        exam_slugï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› "unknown"

    Example:
        >>> extract_slug_from_path(Path("content/exams/auto/nanjing2026/converted_exam.tex"))
        'nanjing2026'
    """
    # å°è¯•ä»è·¯å¾„ä¸­æå– auto/<slug>/ æ¨¡å¼
    parts = tex_file.parts
    try:
        auto_idx = parts.index('auto')
        if auto_idx + 1 < len(parts):
            return parts[auto_idx + 1]
    except (ValueError, IndexError):
        pass

    # å¦‚æœæ— æ³•ä»è·¯å¾„æå–ï¼Œè¿”å›æ–‡ä»¶åï¼ˆå»æ‰æ‰©å±•åï¼‰
    return tex_file.stem.replace('_converted', '').replace('_exam', '')


def extract_slug_from_id(image_id: str) -> str:
    """ä»å›¾ç‰‡ ID æå– exam_slug

    Args:
        image_id: å›¾ç‰‡ IDï¼Œæ ¼å¼å¦‚ "nanjing2026-Q3-img1"

    Returns:
        exam_slug

    Example:
        >>> extract_slug_from_id("nanjing2026-Q3-img1")
        'nanjing2026'
    """
    # ID æ ¼å¼ï¼š<slug>-Q<n>-img<m>
    match = re.match(r'^(.+?)-Q\d+', image_id)
    if match:
        return match.group(1)
    return "unknown"


def parse_image_todos(tex_file: Path) -> List[Dict]:
    """è§£æ TeX æ–‡ä»¶ä¸­çš„æ‰€æœ‰ IMAGE_TODO å—

    Args:
        tex_file: TeX æ–‡ä»¶è·¯å¾„

    Returns:
        å›¾ç‰‡ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ªå­—å…¸
    """
    if not tex_file.exists():
        print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {tex_file}")
        return []

    with open(tex_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    image_jobs = []
    i = 0
    n = len(lines)

    while i < n:
        line = lines[i].strip()

        # æŸ¥æ‰¾ IMAGE_TODO_START
        if line.startswith('% IMAGE_TODO_START'):
            start_line = i + 1  # 1-based line number

            # è§£æ key=value å¯¹
            kv_dict = parse_kv_line(line)

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'id' not in kv_dict:
                print(f"âš ï¸  è­¦å‘Š: {tex_file}:{start_line} ç¼ºå°‘ id å­—æ®µï¼Œè·³è¿‡")
                i += 1
                continue

            # åˆå§‹åŒ–ä»»åŠ¡å¯¹è±¡
            job = {
                'id': kv_dict.get('id', 'unknown'),
                'exam_slug': extract_slug_from_id(kv_dict.get('id', '')),
                'tex_file': str(tex_file),
                'question_index': int(kv_dict.get('question_index', 0)),
                'sub_index': int(kv_dict.get('sub_index', 1)),
                'path': kv_dict.get('path', ''),
                'width_pct': int(kv_dict.get('width', '60%').rstrip('%')),
                'inline': kv_dict.get('inline', 'false').lower() == 'true',
                'context_before': '',
                'context_after': '',
                'todo_block_start_line': start_line,
                'todo_block_end_line': 0
            }

            # å¦‚æœä» ID æ— æ³•æå– slugï¼Œå°è¯•ä»è·¯å¾„æå–
            if job['exam_slug'] == 'unknown':
                job['exam_slug'] = extract_slug_from_path(tex_file)

            # ç»§ç»­è¯»å–åç»­è¡Œï¼ŒæŸ¥æ‰¾ CONTEXT å’Œ IMAGE_TODO_END
            i += 1
            while i < n:
                current_line = lines[i].strip()

                # æå– CONTEXT_BEFORE
                context_before = extract_context_line(current_line, 'CONTEXT_BEFORE')
                if context_before:
                    job['context_before'] = context_before
                    i += 1
                    continue

                # æå– CONTEXT_AFTER
                context_after = extract_context_line(current_line, 'CONTEXT_AFTER')
                if context_after:
                    job['context_after'] = context_after
                    i += 1
                    continue

                # æ‰¾åˆ° IMAGE_TODO_END
                if current_line.startswith('% IMAGE_TODO_END'):
                    job['todo_block_end_line'] = i + 1  # 1-based
                    image_jobs.append(job)
                    break

                i += 1

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° IMAGE_TODO_ENDï¼Œè®°å½•è­¦å‘Š
            if job['todo_block_end_line'] == 0:
                print(f"âš ï¸  è­¦å‘Š: {tex_file}:{start_line} IMAGE_TODO_START æ²¡æœ‰åŒ¹é…çš„ ENDï¼Œè·³è¿‡")
                continue

        i += 1

    return image_jobs


def export_image_jobs(tex_files: List[Path], output_file: Path) -> int:
    """å¯¼å‡ºæ‰€æœ‰å›¾ç‰‡ä»»åŠ¡åˆ° JSONL æ–‡ä»¶

    Args:
        tex_files: TeX æ–‡ä»¶åˆ—è¡¨
        output_file: è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„

    Returns:
        å¯¼å‡ºçš„ä»»åŠ¡æ•°é‡
    """
    all_jobs = []

    for tex_file in tex_files:
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {tex_file}")
        jobs = parse_image_todos(tex_file)
        print(f"   æ‰¾åˆ° {len(jobs)} ä¸ªå›¾ç‰‡ä»»åŠ¡")
        all_jobs.extend(jobs)

    if not all_jobs:
        print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ä»»åŠ¡")
        # åˆ›å»ºç©ºæ–‡ä»¶
        output_file.parent.mkdir(parents=True, exist_ok=True)
        output_file.write_text('', encoding='utf-8')
        return 0

    # å†™å…¥ JSONL
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for job in all_jobs:
            f.write(json.dumps(job, ensure_ascii=False) + '\n')

    print(f"\nâœ… æˆåŠŸå¯¼å‡º {len(all_jobs)} ä¸ªå›¾ç‰‡ä»»åŠ¡åˆ°: {output_file}")
    return len(all_jobs)


def main():
    parser = argparse.ArgumentParser(
        description='ä» converted_exam.tex æå– IMAGE_TODO å—ç”Ÿæˆ JSONL',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # å•ä¸ªæ–‡ä»¶
  python tools/images/export_image_jobs.py \\
      --files content/exams/auto/nanjing2026/converted_exam.tex \\
      --output word_to_tex/output/nanjing2026_image_jobs.jsonl

  # å¤šä¸ªæ–‡ä»¶ï¼ˆä½¿ç”¨ globï¼‰
  python tools/images/export_image_jobs.py \\
      --files content/exams/auto/*/converted_exam.tex \\
      --output word_to_tex/output/all_image_jobs.jsonl

  # è‡ªåŠ¨è¾“å‡ºåˆ°åŒç›®å½•
  python tools/images/export_image_jobs.py \\
      --files content/exams/auto/nanjing2026/converted_exam.tex
        """
    )

    parser.add_argument(
        '--files',
        nargs='+',
        type=Path,
        required=True,
        help='è¦å¤„ç†çš„ TeX æ–‡ä»¶åˆ—è¡¨'
    )

    parser.add_argument(
        '--output',
        type=Path,
        default=None,
        help='è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šä¸ç¬¬ä¸€ä¸ªè¾“å…¥æ–‡ä»¶åŒç›®å½•çš„ image_jobs.jsonlï¼‰'
    )

    args = parser.parse_args()

    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output is None:
        # é»˜è®¤è¾“å‡ºåˆ°ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„åŒç›®å½•
        first_file = args.files[0]
        args.output = first_file.parent / 'image_jobs.jsonl'

    print("â”" * 60)
    print("ğŸ“¸ IMAGE_TODO å¯¼å‡ºå·¥å…·")
    print("â”" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {len(args.files)} ä¸ª")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print()

    # å¯¼å‡ºä»»åŠ¡
    count = export_image_jobs(args.files, args.output)

    print("\n" + "â”" * 60)
    if count > 0:
        print(f"âœ… å¯¼å‡ºå®Œæˆï¼Œå…± {count} ä¸ªå›¾ç‰‡ä»»åŠ¡")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. AI Agent è¯»å– image_jobs.jsonl")
        print("  2. å¯¹æ¯ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨ view å·¥å…·æŸ¥çœ‹å›¾ç‰‡")
        print("  3. ç”Ÿæˆå¯¹åº”çš„ TikZ ä»£ç ")
        print("  4. æ›¿æ¢ TeX æ–‡ä»¶ä¸­çš„ TODO å ä½ç¬¦")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ä»»åŠ¡")

    return 0 if count >= 0 else 1


if __name__ == '__main__':
    exit(main())
