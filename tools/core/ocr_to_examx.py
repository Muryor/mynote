#!/usr/bin/env python3
# -*- coding: utf-8 -*-
r"""
ocr_to_examx_v1.9.py - v1.9.3 æ”¹è¿›ç‰ˆ

ğŸ†• v1.9.3 é»‘ç®±æµ‹è¯•ä¿®å¤ï¼ˆ2025-01-XXï¼‰ï¼š
1. âœ… ä¿®å¤ T008/T017 æ ¹æœ¬åŸå› ï¼špreprocess_multiline_math é”™è¯¯åˆå¹¶
   - é—®é¢˜ï¼š$$P$$ï¼Œ$$B$$ é”™è¯¯åˆå¹¶ä¸º $$Pï¼ŒB$$ï¼Œå¯¼è‡´åµŒå¥—å®šç•Œç¬¦
   - ä¿®å¤ï¼šåªå¯¹å†’å·ï¼ˆï¼šï¼‰åˆ†éš”çš„æ¨¡å¼è¿›è¡Œåˆå¹¶ï¼ˆæ ‡ç­¾ï¼šå…¬å¼ï¼‰
   - ç§»é™¤ï¼šé€—å·ï¼ˆï¼Œï¼‰ã€é¡¿å·ï¼ˆã€ï¼‰ã€å¥å·ï¼ˆã€‚ï¼‰ã€åˆ†å·ï¼ˆï¼›ï¼‰åˆ†éš”çš„åˆå¹¶
   - ç»“æœï¼š$$P$$ï¼Œ$$B$$ â†’ \(P\)ï¼Œ\(B\)ï¼ˆæ­£ç¡®ä¿æŒç‹¬ç«‹ï¼‰

ğŸ†• v1.9.2 é»‘ç®±æµ‹è¯•ä¿®å¤ï¼ˆ2025-11-28ï¼‰ï¼š
1. âœ… ä¿®å¤ T008 å®šç•Œç¬¦ä¸å¹³è¡¡é—®é¢˜ï¼ˆP0 - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - é—®é¢˜ï¼šè·¨è¡Œæ•°å­¦ç¯å¢ƒï¼ˆarray/casesï¼‰å¯¼è‡´å®šç•Œç¬¦ä¸å¹³è¡¡
   - ä¿®å¤ï¼šå¢å¼º balance_delimiters() æ”¯æŒè·¨è¡Œç¯å¢ƒæ£€æµ‹
   - ä¿®å¤ï¼šå¤„ç† \therefore \( ç­‰ç¬¦å·åç›´æ¥è·Ÿæ•°å­¦æ¨¡å¼çš„æƒ…å†µ
   - æ”¹è¿›ï¼šå…¨å±€å¹³è¡¡æ£€æŸ¥å’Œæ™ºèƒ½ä¿®å¤
2. âœ… ä¿®å¤ T016 LaTeX ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰é—®é¢˜ï¼ˆP2ï¼‰
   - é—®é¢˜ï¼š& ç­‰å­—ç¬¦åœ¨éæ•°å­¦æ¨¡å¼ä¸‹æœªæ­£ç¡®è½¬ä¹‰
   - ä¿®å¤ï¼šå¢å¼º escape_latex_special() ä¿æŠ¤æ•°å­¦æ¨¡å¼å†…çš„ &
   - æ”¹è¿›ï¼šæ­£ç¡®å¤„ç† tabular/array/matrix ç¯å¢ƒä¸­çš„åˆ—åˆ†éš”ç¬¦
3. âœ… ä¿®å¤ T017 æ•°å­¦æ¨¡å¼å†…ä¸­æ–‡æ ‡ç‚¹é—®é¢˜ï¼ˆP2ï¼‰
   - é—®é¢˜ï¼šæ•°å­¦æ¨¡å¼å†…æ®‹ç•™å…¨è§’æ ‡ç‚¹ï¼ˆé€—å·ã€åˆ†å·ç­‰ï¼‰
   - ä¿®å¤ï¼šå¢å¼º normalize_punctuation_in_math() è¿­ä»£å¤„ç†
   - æ–°å¢ï¼šé¢å¤–çš„é—æ¼æ ‡ç‚¹æ£€æµ‹å’Œè½¬æ¢é€»è¾‘

ğŸ†• v1.9.1 å…³é”®ä¿®å¤ï¼ˆ2025-11-26ï¼‰ï¼š
1. âœ… ä¿®å¤åå‘å®šç•Œç¬¦ - å†’å·æ¨¡å¼ï¼ˆP0 - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - é—®é¢˜ï¼š\)ï¼šå…¬å¼\( æ¨¡å¼å¯¼è‡´ 160 ä¸ªåå‘å®šç•Œç¬¦æ¡ˆä¾‹
   - ä¿®å¤ï¼šå¢å¼º fix_reversed_delimiters() æ£€æµ‹å†’å·åçš„å…¬å¼ç¼ºå°‘ \(
   - æ¨¡å¼ï¼š\)ï¼š\sqrt{3}x - y = 0\) â†’ \)ï¼š\(\sqrt{3}x - y = 0\)
   - æ”¹è¿›ï¼šæ™ºèƒ½åˆ¤æ–­æ•°å­¦å†…å®¹ï¼Œé¿å…è¯¯ä¿®å¤
2. âœ… æ¸…ç† \right. åçš„å¼‚å¸¸å­—ç¬¦ï¼ˆP0ï¼‰
   - é—®é¢˜ï¼š\right.\ å’Œ \right.\\ æ¨¡å¼å¯¼è‡´å®šç•Œç¬¦ä¸å¹³è¡¡
   - ä¿®å¤ï¼šå¢å¼º fix_right_boundary_errors() é¢„å¤„ç†æ¸…ç†åæ–œæ å¼‚å¸¸
   - æ¨¡å¼ï¼š\right.\ ï¼Œ\therefore â†’ \right.\)ï¼Œ\therefore
   - æ”¹è¿›ï¼šå‡å°‘å®šç•Œç¬¦å·®å€¼ä» +25 åˆ°æ¥è¿‘ 0
3. âœ… å®Œå–„æ•°å­¦æ¨¡å¼å†…ä¸­æ–‡æ ‡ç‚¹è½¬æ¢ï¼ˆP1ï¼‰
   - é—®é¢˜ï¼š23 å¤„ä¸­æ–‡æ ‡ç‚¹æ®‹ç•™åœ¨æ•°å­¦æ¨¡å¼å†…
   - ä¿®å¤ï¼šå¢å¼º normalize_punctuation_in_math() æ·»åŠ å®Œæ•´æ ‡ç‚¹æ˜ å°„
   - æ–°å¢ï¼šé¡¿å·ã€å†’å·ã€å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç­‰
   - ä¿æŠ¤ï¼š\text{}, \mbox{}, \mathrm{}, \textbf{}, \textit{} å†…çš„ä¸­æ–‡
4. âœ… ä¿®å¤ tabular ç¯å¢ƒç¼ºå¤±åˆ—æ ¼å¼ï¼ˆP1ï¼‰
   - é—®é¢˜ï¼š\begin{tabularend{center} ç¼ºå°‘åˆ—æ ¼å¼å‚æ•°
   - æ–°å¢ï¼šfix_tabular_environments() å‡½æ•°
   - ä¿®å¤ï¼šè‡ªåŠ¨æ¨æ–­åˆ—æ•°å¹¶æ·»åŠ é»˜è®¤æ ¼å¼ {|c|c|...}
   - æ”¹è¿›ï¼šé¿å… LaTeX ç¼–è¯‘é”™è¯¯
5. âœ… æ¸…ç† CONTEXT æ³¨é‡Šæ±¡æŸ“ï¼ˆP1ï¼‰
   - é—®é¢˜ï¼šCONTEXT åŒ…å« LaTeX ç¯å¢ƒå‘½ä»¤ï¼Œé•¿åº¦è¶…è¿‡ 80 å­—ç¬¦
   - ä¿®å¤ï¼šå¢å¼º clean_context() å‡½æ•°
   - æ”¹è¿›ï¼šæœ€å¤§é•¿åº¦ä» 50 å¢åŠ åˆ° 80 å­—ç¬¦
   - æ¸…ç†ï¼šå°† \begin{...} å’Œ \end{...} æ›¿æ¢ä¸º [ENV_START/END]

ğŸ†• v1.8.7 ç²¾å‡†ä¿®å¤ï¼ˆ2025-11-21ï¼‰ï¼š
1. âœ… æ•°å­¦å®šç•Œç¬¦ç»Ÿè®¡å¿½ç•¥æ³¨é‡Šï¼ˆP0 - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - é—®é¢˜ï¼šæ³¨é‡Šä¸­çš„ \( / \) è¢«è®¡å…¥å…¨å±€ç»Ÿè®¡ï¼Œé€ æˆè™šå‡ diff
   - ä¿®å¤ï¼švalidate_math_integrity() æŒ‰è¡Œæ‰«æï¼Œå…ˆå»æ‰ % æ³¨é‡Šå†ç»Ÿè®¡
   - æ”¹è¿›ï¼šç»Ÿè®¡æ›´åŠ çœŸå®ï¼Œä¸å—æ³¨é‡Šè¡Œå¹²æ‰°
2. âœ… æ£€æµ‹åå‘æ•°å­¦å®šç•Œç¬¦æ¨¡å¼ï¼ˆP1ï¼‰
   - é—®é¢˜ï¼š\) åœ¨ \( å‰é¢çš„è¡Œéš¾ä»¥å®šä½
   - æ–°å¢ï¼šè¡Œçº§æ£€æµ‹é€»è¾‘ï¼Œæ‰¾å‡º idx_close < idx_open çš„è¡Œ
   - è¾“å‡ºï¼šè¡Œå· + è¡Œå†…å®¹ç‰‡æ®µï¼Œæ–¹ä¾¿äººå·¥å®¡æŸ¥
3. âœ… æçª„è‡ªåŠ¨ä¿®å¤ç‰¹å®šåå‘æ¨¡å¼ï¼ˆP1ï¼‰
   - æ–°å¢ï¼šfix_specific_reversed_pairs() å‡½æ•°
   - æ¨¡å¼ Aï¼šæ±‚ç‚¹\)X_{2}\(æ‰€æœ‰å¯èƒ½çš„åæ ‡ â†’ æ±‚ç‚¹\(X_{2}\)æ‰€æœ‰å¯èƒ½çš„åæ ‡
   - æ¨¡å¼ Bï¼šå…¶ä¸­\)x_{i} â†’ å…¶ä¸­ x_{i}ï¼ˆåˆ é™¤ä¸åŒ¹é…çš„ \)ï¼‰
   - å®‰å…¨æ€§ï¼šåªé’ˆå¯¹ç²¾ç¡®åŒ¹é…çš„æ¨¡å¼ï¼Œä¸å½±å“å…¶ä»–å†…å®¹

ğŸ†• v1.8.6 å…³é”®ä¿®å¤ï¼ˆ2025-11-21ï¼‰ï¼š
1. âœ… æ”¶ç´§ fix_right_boundary_errors è¡Œä¸ºï¼ˆP0 - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - é—®é¢˜ï¼šæ—§ç‰ˆæ— æ¡ä»¶è¡¥ \)ï¼Œå¯¼è‡´å…¨å±€ \( / \) diff é•¿æœŸç»´æŒåœ¨ -18
   - ä¿®å¤ï¼šæŒ‰é€å­—ç¬¦æ‰«æï¼Œä»…åœ¨è¡Œå†…å­˜åœ¨æœªé—­åˆ \( æ—¶æ‰è¡¥ \)
   - æ–°å¢ï¼šhas_unmatched_open() è¾…åŠ©å‡½æ•°åˆ¤æ–­è¡Œå†…å¹³è¡¡
   - ä¿ç•™ï¼šæ¨¡å¼3ï¼ˆ\right.ï¼Œåˆ™\) â†’ \right.\)ï¼Œåˆ™ï¼‰åªè°ƒæ•´é¡ºåºï¼Œä¸æ”¹å˜æ•°é‡
2. âœ… æ–°å¢ balance_array_and_cases_env åå¤„ç†ï¼ˆP0ï¼‰
   - é—®é¢˜ï¼šarray/cases ç¯å¢ƒä¸å¹³è¡¡ï¼Œå¤šå‡º 1 ä¸ª \end{array} / \end{cases}
   - ä¿®å¤ï¼šä½¿ç”¨æ ˆåŒ¹é…ç®—æ³•ï¼Œåˆ é™¤æ²¡æœ‰åŒ¹é… \begin çš„ \end
   - ä¸è‡ªåŠ¨ç”Ÿæˆæ–°çš„ \beginï¼Œåªåˆ é™¤å¤šä½™çš„ \end
3. âœ… æ–°å¢ validate_brace_balance å…¨å±€èŠ±æ‹¬å·æ£€æŸ¥ï¼ˆP1ï¼‰
   - é—®é¢˜ï¼šLine 555 æœ‰å¤šä½™çš„ }ï¼Œä¸åˆ©äºå¿«é€Ÿå®šä½
   - æ–°å¢ï¼šæŒ‰è¡Œæ‰«æï¼Œå¿½ç•¥æ³¨é‡Šå’Œè½¬ä¹‰çš„ \{ \}
   - è¾“å‡ºï¼šè¡Œå· + é”™è¯¯ç±»å‹ï¼ˆbalance went negative / EOF imbalanceï¼‰
   - ä¸è‡ªåŠ¨ä¿®å¤ï¼Œä»…è¾“å‡ºæ—¥å¿—æ–¹ä¾¿äººå·¥å®šä½
4. âœ… å¢å¼º validate_math_integrity æ—¥å¿—ï¼ˆP1ï¼‰
   - æ–°å¢ï¼šä¼˜å…ˆè¾“å‡ºåŒ…å« \right.ã€arrayã€casesã€é¢˜å·æ ‡è®°çš„æ ·æœ¬
   - æ–°å¢ï¼š_has_priority_keywords() æ£€æµ‹å…³é”®è¯
   - æ–°å¢ï¼š_get_line_number() è¾“å‡ºè¡Œå·
   - æ”¹è¿›ï¼šæ ·æœ¬æ ¼å¼ä¸º "Line X: ..." æ–¹ä¾¿å®šä½
5. âœ… å¢å¼ºé¢˜å¹²ç¼ºå¤±æ£€æµ‹æ—¥å¿—ï¼ˆP1ï¼‰
   - æ–°å¢ï¼šè¾“å‡ºé¢˜å‹ã€é¢˜å·ã€åŸå§‹ Markdown ç‰‡æ®µï¼ˆå‰ 3 è¡Œï¼‰
   - æ”¹è¿›ï¼šå¤šè¡Œæ ¼å¼åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿äººå·¥å›çœ‹ Markdown
   - ä¿ç•™ï¼šç°æœ‰ _is_likely_stem å¯å‘å¼é€»è¾‘

ğŸ†• v1.8.5 å…³é”®ä¿®å¤ï¼ˆ2025-11-21ï¼‰ï¼š
1. âœ… å¢å¼º \right. è¾¹ç•Œæ£€æµ‹ï¼ˆP0 - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - æ–°å¢ï¼šæ£€æµ‹ \right. åçš„å•ç¾å…ƒç¬¦å· $
   - æ–°å¢ï¼šæ£€æµ‹ \right. åç›´æ¥è·Ÿä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼›ï¼šç­‰ï¼‰
   - æ–°å¢ï¼šæ™ºèƒ½åˆ¤æ–­ \right.\) å·²æ­£ç¡®é—­åˆçš„æƒ…å†µ
   - ä¿®å¤ï¼š8ä¸ªé¢˜ç›®ä¸­çš„ \right. è¾¹ç•Œé”™è¯¯
2. âœ… åå¤„ç†ä¿®å¤ \right. è¾¹ç•Œé”™è¯¯ï¼ˆP0 - å…œåº•æ–¹æ¡ˆï¼‰
   - æ–°å¢ï¼šfix_right_boundary_errors() å‡½æ•°
   - ä¿®å¤ï¼š\right. åç›´æ¥è·Ÿä¸­æ–‡æ ‡ç‚¹ï¼ˆç¼ºå°‘ \)ï¼‰
   - ä¿®å¤ï¼šarray/cases ç¯å¢ƒåçš„ \right. è¾¹ç•Œé”™è¯¯
   - ä¿®å¤ï¼š\right.ï¼Œåˆ™\) æ¨¡å¼ï¼ˆ\) ä½ç½®é”™è¯¯ï¼‰
3. âœ… IMAGE_TODO å—æ ¼å¼éªŒè¯å’Œä¿®å¤ï¼ˆP0ï¼‰
   - æ–°å¢ï¼švalidate_and_fix_image_todo_blocks() å‡½æ•°
   - ä¿®å¤ï¼šIMAGE_TODO_END åçš„å¤šä½™èŠ±æ‹¬å·
   - ä¿®å¤ï¼šIMAGE_TODO_START è¡Œæœ«çš„å¤šä½™å­—ç¬¦
   - è‡ªåŠ¨æ£€æµ‹å¹¶æŠ¥å‘Šæ ¼å¼é”™è¯¯
4. âœ… å¢å¼ºé¢˜å¹²è¯†åˆ«è§„åˆ™ï¼ˆP1ï¼‰
   - æ–°å¢ï¼šé¢˜å‹åˆ¤æ–­ï¼ˆè§£ç­”é¢˜/é€‰æ‹©é¢˜/å¡«ç©ºé¢˜ï¼‰
   - æ–°å¢ï¼šåŠ¨æ€è°ƒæ•´é•¿åº¦é˜ˆå€¼å’Œå…³é”®è¯
   - æ–°å¢ï¼šå…³é”®è¯æ£€æŸ¥ï¼ˆå·²çŸ¥ã€è®¾ã€å¦‚å›¾ã€è¯æ˜ç­‰ï¼‰
   - æ”¹è¿›ï¼šç»¼åˆåˆ¤æ–­é€»è¾‘ï¼ˆé¢˜å‹ + å…³é”®è¯ + é•¿åº¦ï¼‰
   - æ–°å¢ï¼šä» \section å‘½ä»¤æ¨æ–­é¢˜å‹

ğŸ†• v1.8.4 é‡è¦ä¿®å¤ï¼ˆ2025-11-21ï¼‰ï¼š
1. âœ… ä¿®å¤åˆå¹¶é¢˜ç›®ç»“æ„é—®é¢˜ï¼šé¢˜å¹² vs å°é—®è¯†åˆ«ï¼ˆP0ï¼‰
   - é—®é¢˜ï¼šç›¸åŒé¢˜å·åˆå¹¶åï¼Œæ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¾ç¤ºä¸º \item
   - ä¿®å¤ï¼šç¬¬ä¸€ä¸ª \item è½¬ä¸ºé¢˜å¹²ï¼Œåç»­åŒ…è£¹åœ¨ enumerate ä¸­
   - æ–°å¢ï¼šfix_merged_questions_structure åå¤„ç†å‡½æ•°
2. âœ… å¢å¼ºé¢˜å¹²è¯†åˆ«é€»è¾‘ï¼ˆP1ï¼‰
   - æ–°å¢ï¼š_is_likely_stem å¯å‘å¼åˆ¤æ–­å‡½æ•°
   - æ£€æµ‹ï¼šå­—æ•°ã€å°é—®æ ‡è®°ï¼ˆâ‘ â‘¡â‘¢ã€(1)(2)ç­‰ï¼‰ã€åç»­å†…å®¹
   - é¿å…ï¼šè¯¯å°†çœŸå®å°é—®è¯†åˆ«ä¸ºé¢˜å¹²
3. âœ… ä¿®å¤ IMAGE_TODO è·¯å¾„è½¬ä¹‰é—®é¢˜ï¼ˆP0ï¼‰
   - é—®é¢˜ï¼šè·¯å¾„ä¸­çš„ä¸‹åˆ’çº¿å¯¼è‡´ LaTeX ç¼–è¯‘é”™è¯¯
   - ä¿®å¤ï¼šè‡ªåŠ¨è½¬ä¹‰ _ ä¸º \_

ğŸ†• v1.8 P0/P1 ä¿®å¤ï¼ˆ2025-11-20ï¼‰ï¼š
1. âœ… ä¿®å¤æ•°å­¦æ¨¡å¼è¾¹ç•Œè§£æé”™è¯¯ï¼š\right.\ $$ â†’ \right.\) ï¼ˆP0ï¼‰
   - ä¿®å¤åˆ†æ®µå‡½æ•°/çŸ©é˜µåç´§è·Ÿæ–‡æœ¬æ—¶çš„æ•°å­¦æ¨¡å¼é—­åˆé—®é¢˜
   - é¿å…ä¸­æ–‡æ–‡æœ¬è¢«é”™è¯¯åœ°æ”¾å…¥æ•°å­¦æ¨¡å¼
2. âœ… å¢å¼ºé¢˜å¹²ç¼ºå¤±æ£€æµ‹ï¼šè‡ªåŠ¨æ’å…¥ TODO æ³¨é‡Šï¼ˆP1ï¼‰
   - æ£€æµ‹ç›´æ¥ä» \item å¼€å§‹çš„é¢˜ç›®
   - åœ¨ \begin{question} åè‡ªåŠ¨æ·»åŠ è­¦å‘Šæ³¨é‡Š

v1.7 æ”¹è¿›ï¼ˆ2025-11-20ï¼‰ï¼š
1. âœ… é¢˜å¹²æ£€æµ‹ä¸è­¦å‘Šï¼šæ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®ï¼ˆç›´æ¥ä» \item å¼€å§‹ï¼‰
2. âœ… æ¸…ç† Markdown å›¾ç‰‡å±æ€§æ®‹ç•™ï¼šåˆ é™¤ height="..." å’Œ width="..." æ®‹ç•™
3. âœ… å°é—®ç¼–å·æ ¼å¼ç»Ÿä¸€ï¼šä¸è‡ªåŠ¨æ·»åŠ  \mathrmï¼Œä½¿ç”¨æ™®é€šæ–‡æœ¬
4. âœ… IMAGE_TODO å—åä¸æ·»åŠ ç©ºè¡Œï¼šä¼˜åŒ–æ ¼å¼
5. âœ… \explain ä¸­çš„ç©ºè¡Œè‡ªåŠ¨å¤„ç†ï¼šç©ºè¡Œæ›¿æ¢ä¸º \par

v1.6 P0 ä¿®å¤ï¼ˆ2025-11-19ï¼‰ï¼š
1. âœ… ä¿®å¤æ•°ç»„ç¯å¢ƒé—­åˆé”™è¯¯ï¼ˆ\right.\\) â†’ \right.\)ï¼‰
2. âœ… æ¸…ç†å›¾ç‰‡å±æ€§æ®‹ç•™ï¼ˆ{width="..." height="..."}ï¼‰

ç‰ˆæœ¬ï¼šv1.9.1
æ—¥æœŸï¼š2025-11-26
"""

import re
import argparse
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from enum import Enum, auto  # å¼•å…¥æšä¸¾æ”¯æŒï¼ˆçŠ¶æ€æœºéœ€è¦ï¼‰

# ==================== æ•°å­¦çŠ¶æ€æœºï¼ˆæ¥è‡ª ocr_to_examx_complete.pyï¼‰ ====================
# æ³¨æ„ï¼šæ­¤çŠ¶æ€æœºå®Œå…¨å–ä»£åŸå…ˆåŸºäºæ­£åˆ™çš„ smart_inline_math / sanitize_math ç­‰ç®¡çº¿ã€‚
# æ—§å‡½æ•°ä¿ç•™ä½†æ ‡è®°ä¸º DEPRECATEDï¼Œä¸»æµç¨‹ä¸å†è°ƒç”¨ï¼Œé¿å…ç›¸äº’å¹²æ‰°ã€‚

# åº”è¯¥ä»æ•°å­¦æ¨¡å¼ç§»å‡ºçš„ä¸­æ–‡è¯æ±‡
CHINESE_MATH_SEPARATORS = {
    'connectors': ['å³', 'ä¸', 'æˆ–', 'ä¸”', 'æ•…', 'åˆ™', 'æ‰€ä»¥', 'å› æ­¤', 'å› ä¸º', 'ç”±äº', 'æ ¹æ®', 'æ˜¾ç„¶', 'å¯çŸ¥', 'å¯å¾—', 'äºæ˜¯', 'ä»è€Œ'],
    'math_objects': ['ç›´çº¿', 'æ›²çº¿', 'å¹³é¢', 'å‡½æ•°', 'æ–¹ç¨‹', 'åœ†', 'ç‚¹', 'æ¤­åœ†', 'åŒæ›²çº¿', 'æŠ›ç‰©çº¿', 'å‘é‡', 'çŸ©é˜µ', 'é›†åˆ', 'åŒºé—´'],
    'verbs': ['è®¾', 'ä»¤', 'è‹¥', 'å½“', 'æ—¶', 'æœ‰', 'å¾—', 'çŸ¥', 'è¿‡', 'å–', 'ä½œ'],
}

class TokenType(Enum):
    TEXT = auto()
    DOLLAR_SINGLE = auto()
    DOLLAR_DOUBLE = auto()
    LATEX_OPEN = auto()
    LATEX_CLOSE = auto()
    RIGHT_BOUNDARY = auto()
    NEWLINE = auto()
    EOF = auto()


class MathStateMachine:
    r"""æ•°å­¦æ¨¡å¼çŠ¶æ€æœº - ç»Ÿä¸€è§£æ/è§„èŒƒæ‰€æœ‰æ•°å­¦å®šç•Œç¬¦

    è®¾è®¡ç›®æ ‡ï¼š
    1. æ”¯æŒæ··åˆå‡ºç°çš„ $ ... $ã€$$ ... $$ã€\( ... \) ä»¥åŠ OCR ç”Ÿæˆçš„ \right. $$ ç­‰ç•¸å½¢è¾¹ç•Œ
    2. å°†æ‰€æœ‰æ˜¾ç¤º/è¡Œå†…æ•°å­¦ç»Ÿä¸€è§„èŒƒä¸ºè¡Œå†…å½¢å¼ï¼š\( ... \)ï¼ˆä¸ examx åŒ…å…¼å®¹ï¼‰
    3. ä¿æŒå·²æœ‰æ­£ç¡®çš„ \( ... \) / \) ä¸è¢«äºŒæ¬¡åŒ…è£¹
    4. é˜²æ­¢è·¨è¡Œå•ç¾å…ƒæœªé—­åˆé€ æˆåå¹¶åç»­æ–‡æœ¬
    """

    def preprocess_multiline_math(self, text: str) -> str:
        """é¢„å¤„ç†å¤šè¡Œæ•°å­¦ç¯å¢ƒï¼ˆä¿®å¤ P0-001, P0-002ï¼‰

        å¤„ç†è·¨å¤šè¡Œçš„ $$...array/cases...$$ å—ï¼Œé¿å…è¢«é€è¡Œæ‹†æ•£
        
        ğŸ†• v1.9.3ï¼šä¿®å¤ T008/T017 é—®é¢˜
        - åªåˆå¹¶å†’å·åˆ†éš”çš„ $$æ ‡ç­¾$$ï¼š$$å…¬å¼$$ æ¨¡å¼
        - ä¸åˆå¹¶é€—å·/é¡¿å·åˆ†éš”çš„ç‹¬ç«‹å˜é‡ $$P$$ï¼Œ$$B$$ æ¨¡å¼
        """
        # ğŸ†• ä¿®å¤ P0-001a: åªåˆå¹¶å†’å·åˆ†éš”çš„æ¨¡å¼ï¼ˆæ ‡ç­¾ï¼šå…¬å¼ï¼‰
        # ä¾‹å¦‚: $$C$$ï¼š$$x^{2}$$ â†’ $$Cï¼šx^{2}$$
        # ğŸ”§ v1.9.3: ç§»é™¤é€—å·/é¡¿å·/å¥å·/åˆ†å·ï¼Œè¿™äº›åˆ†éš”çš„åº”è¯¥ä¿æŒç‹¬ç«‹
        text = re.sub(r'\$\$([^$]+)\$\$([ï¼š])\$\$([^$]+)\$\$', r'$$\1\2\3$$', text)

        # ğŸ†• ä¿®å¤ P0-002: å¤„ç† \right.\ $$ è·¨è¡Œè¾¹ç•Œæ¨¡å¼
        # æƒ…å†µ1: \right.\ $$ ï¼ˆåæ–œæ +ç©ºæ ¼+åŒç¾å…ƒï¼‰
        # æ³¨æ„ï¼š\ æ˜¯ä¸¤ä¸ªå­—ç¬¦ï¼šåæ–œæ å’Œç©ºæ ¼ï¼Œ\left\{ æ˜¯backslash-left-backslash-brace
        # (?:\\[\{\[\(])? è¡¨ç¤ºå¯é€‰çš„ "\{" æˆ– "\[" æˆ– "\("
        pattern_backslash_space = re.compile(
            r'\$\$\s*\\left(?:\\[\{\[\(])?\s*\\begin\{(array|cases|matrix|pmatrix|bmatrix|vmatrix)\}.*?\\end\{\1\}\s*\\right(?:\\[\}\]\)])?\.?\s*\\ \$\$',
            re.DOTALL
        )
        
        def extract_content(match_obj):
            # Extract the \left...\right. part
            content = re.search(r'\\left.*?\\right(?:\\[\}\]\)])?\.?', match_obj.group(0), re.DOTALL)
            return r'\(' + content.group(0) + r'\)'
        
        text = pattern_backslash_space.sub(extract_content, text)

        # æƒ…å†µ2: \right.\\ $$ ï¼ˆåŒåæ–œæ +ç©ºæ ¼+åŒç¾å…ƒï¼‰
        pattern_double_backslash = re.compile(
            r'\$\$\s*\\left(?:\\[\{\[\(])?\s*\\begin\{(array|cases|matrix|pmatrix|bmatrix|vmatrix)\}.*?\\end\{\1\}\s*\\right(?:\\[\}\]\)])?\.?\s*\\\\ \$\$',
            re.DOTALL
        )
        text = pattern_double_backslash.sub(extract_content, text)

        # åŒ¹é… $$...$$ å—ï¼ŒåŒ…æ‹¬è·¨è¡Œçš„ array/cases/matrix ç¯å¢ƒï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        pattern = re.compile(
            r'\$\$\s*\\left(?:\\[\{\[\(])?\s*\\begin\{(array|cases|matrix|pmatrix|bmatrix|vmatrix)\}.*?\\end\{\1\}\s*\\right(?:\\[\}\]\)])?\.?\s*\$\$',
            re.DOTALL
        )

        def replace_multiline(match):
            # Extract just the \left...\right. part
            content = re.search(r'\\left.*?\\right(?:\\[\}\]\)])?\.?', match.group(0), re.DOTALL)
            return r'\(' + content.group(0) + r'\)'

        return pattern.sub(replace_multiline, text)

    def tokenize(self, text: str) -> List:
        tokens = []
        i = 0
        n = len(text)
        while i < n:
            # ğŸ”¥ v1.8.6ï¼šå¢å¼º \right. åçš„ OCR è¾¹ç•Œæ£€æµ‹ï¼ˆä¿®å¤ P0-001ï¼‰
            # å¤„ç† \right. åå¯èƒ½è·Ÿéšçš„å„ç§ç•¸å½¢æ ¼å¼ï¼š
            # - \right. $$
            # - \right. $
            # - \right.\ $$  ï¼ˆåæ–œæ ç©ºæ ¼ï¼ŒP0-CRITICALï¼‰
            # - \right.\\ $$  ï¼ˆåŒåæ–œæ ç©ºæ ¼ï¼‰
            # - \right.  $$  ï¼ˆå¤šä¸ªç©ºæ ¼ï¼‰
            # - \right.ï¼Œï¼ˆç›´æ¥è·Ÿä¸­æ–‡æ ‡ç‚¹ï¼‰
            if text[i:].startswith(r'\right.'):
                j = i + 7  # è·³è¿‡ \right.
                found_boundary = False

                # ğŸ†• v1.9.3ï¼šä¿®å¤ \right.\ $$ å¤„ç†
                # ç”Ÿæˆ RIGHT_BOUNDARY token åï¼Œè¿˜è¦ç”Ÿæˆ DOLLAR_DOUBLE token
                # è¿™æ · process å‡½æ•°æ‰èƒ½æ­£ç¡®è¯†åˆ«æ•°å­¦æ¨¡å¼çš„ç»“æŸ

                # æƒ…å†µ1ï¼š\right.\ $$ï¼ˆåæ–œæ +ç©ºæ ¼+åŒç¾å…ƒï¼ŒP0-CRITICALï¼‰
                if j < n - 3 and text[j:j+4] == r'\ $$':
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    tokens.append((TokenType.DOLLAR_DOUBLE, '$$', j + 2))  # æ·»åŠ ç»“æŸç¬¦
                    i = j + 4
                    found_boundary = True

                # æƒ…å†µ2ï¼š\right.\\ $$ï¼ˆåŒåæ–œæ +ç©ºæ ¼+åŒç¾å…ƒï¼‰
                elif j < n - 4 and text[j:j+5] == r'\\ $$':
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    tokens.append((TokenType.DOLLAR_DOUBLE, '$$', j + 3))  # æ·»åŠ ç»“æŸç¬¦
                    i = j + 5
                    found_boundary = True

                # æƒ…å†µ3ï¼š\right. $$ï¼ˆç©ºæ ¼+åŒç¾å…ƒï¼‰
                elif j < n - 1 and text[j] == ' ':
                    # è·³è¿‡å¤šä¸ªç©ºæ ¼
                    k = j
                    while k < n and text[k] == ' ':
                        k += 1
                    if k < n - 1 and text[k:k+2] == '$$':
                        tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                        tokens.append((TokenType.DOLLAR_DOUBLE, '$$', k))  # æ·»åŠ ç»“æŸç¬¦
                        i = k + 2
                        found_boundary = True

                # æƒ…å†µ4ï¼š\right.$$ï¼ˆç›´æ¥è·ŸåŒç¾å…ƒï¼Œæ— ç©ºæ ¼ï¼‰
                elif j < n - 1 and text[j:j+2] == '$$':
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    tokens.append((TokenType.DOLLAR_DOUBLE, '$$', j))  # æ·»åŠ ç»“æŸç¬¦
                    i = j + 2
                    found_boundary = True

                # æƒ…å†µ5ï¼š\right. $ï¼ˆå•ç¾å…ƒï¼‰- è¿™ç§æƒ…å†µæ¯”è¾ƒç‰¹æ®Šï¼Œä¿æŒåŸæ ·
                elif j < n and text[j] == '$' and (j + 1 >= n or text[j+1] != '$'):
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    tokens.append((TokenType.DOLLAR_SINGLE, '$', j))  # æ·»åŠ ç»“æŸç¬¦
                    i = j + 1
                    found_boundary = True

                # æƒ…å†µ6ï¼š\right.\)ï¼ˆå·²ç»æ­£ç¡®é—­åˆï¼‰
                elif j < n - 1 and text[j:j+2] == r'\)':
                    # è¿™æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼Œä¿æŒåŸæ ·
                    tokens.append((TokenType.TEXT, r'\right.', i))
                    i += 7
                    found_boundary = True

                # æƒ…å†µ7ï¼š\right. åç›´æ¥è·Ÿä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼›ï¼šç­‰ï¼‰
                elif j < n and text[j] in 'ï¼Œã€‚ï¼›ï¼šã€ï¼ï¼Ÿ':
                    # OCR é”™è¯¯ï¼šç¼ºå°‘é—­åˆç¬¦å·
                    # æ’å…¥ \right.\) æ¥é—­åˆæ•°å­¦æ¨¡å¼ï¼Œæ ‡ç‚¹ä¿æŒåœ¨æ•°å­¦æ¨¡å¼å¤–
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    i = j  # ä¸è·³è¿‡æ ‡ç‚¹ï¼Œè®©åç»­å¤„ç†å°†å…¶ä½œä¸ºæ™®é€šæ–‡æœ¬
                    found_boundary = True

                if not found_boundary:
                    # ä¸æ˜¯è¾¹ç•Œé”™è¯¯ï¼Œä¿æŒåŸæ ·
                    tokens.append((TokenType.TEXT, r'\right.', i))
                    i += 7

                if found_boundary:
                    continue

            # $$ æ˜¾ç¤ºæ•°å­¦
            if i < n - 1 and text[i:i+2] == '$$':
                tokens.append((TokenType.DOLLAR_DOUBLE, '$$', i))
                i += 2
                continue

            # å• $ è¡Œå†…æ•°å­¦
            if text[i] == '$':
                tokens.append((TokenType.DOLLAR_SINGLE, '$', i))
                i += 1
                continue

            # \( ä¸ \)
            if i < n - 1 and text[i:i+2] == r'\(':
                tokens.append((TokenType.LATEX_OPEN, r'\(', i))
                i += 2
                continue
            if i < n - 1 and text[i:i+2] == r'\)':
                tokens.append((TokenType.LATEX_CLOSE, r'\)', i))
                i += 2
                continue

            # æ™®é€šæ–‡æœ¬å—æ”¶é›†
            j = i
            while j < n:
                if text[j] in '$\n':
                    break
                if j < n - 1 and text[j:j+2] in [r'\(', r'\)', '$$']:
                    break
                if text[j:].startswith(r'\right.'):
                    break
                j += 1
            if j > i:
                tokens.append((TokenType.TEXT, text[i:j], i))
                i = j
            else:
                tokens.append((TokenType.TEXT, text[i], i))
                i += 1
        return tokens

    def fix_malformed_patterns(self, text: str) -> str:
        """ä¿®å¤æ ¼å¼é”™è¯¯çš„æ•°å­¦æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆ v1.9.2ï¼‰
        
        ğŸ†• v1.9.2: å¤„ç†æ›´å¤šçš„ç•¸å½¢æ¨¡å¼
        - åµŒå¥—å®šç•Œç¬¦ï¼š\(P,B\(ï¼Œ\)C,D\) â†’ \(P,B\)ï¼Œ\(C,D\)
        - åå‘åµŒå¥—ï¼š\)...\( â†’ ä¿®æ­£ä¸ºæ­£ç¡®é¡ºåº
        """
        import re

        # 1. åˆ é™¤ç©ºæ•°å­¦æ¨¡å¼ \(\)
        text = re.sub(r'\\\(\s*\\\)', '', text)

        # 2. ä¿®å¤è¿ç»­å®šç•Œç¬¦ï¼ˆè¿­ä»£å¤„ç†ï¼Œæœ€å¤š3æ¬¡ï¼‰
        for _ in range(3):
            before = text
            # \(\( â†’ \(
            text = re.sub(r'\\\(\\\(', r'\\(', text)
            # \)\) â†’ \)
            text = re.sub(r'\\\)\\\)', r'\\)', text)
            if text == before:
                break

        # 3. ä¿®å¤é”™è¯¯åµŒå¥— \((\) â†’ (
        text = re.sub(r'\\\(\(\\\)', '(', text)

        # 4. ä¿®å¤ \)(\( â†’ )(  (é”™è¯¯çš„å®šç•Œç¬¦åŒ…è£¹æ‹¬å·)
        text = re.sub(r'\\\)\(\\\(', ')(', text)
        
        # ğŸ†• v1.9.2: ä¿®å¤åµŒå¥—å®šç•Œç¬¦ä¸­çš„ä¸­æ–‡æ ‡ç‚¹
        # æ¨¡å¼: \(æ ‡ç‚¹\) â†’ æ ‡ç‚¹ (å½“æ ‡ç‚¹æ˜¯ç‹¬ç«‹çš„æ•°å­¦å—æ—¶)
        # ä¾‹å¦‚: \(P,B\(ï¼Œ\)C,D\) ä¸­çš„ \(ï¼Œ\) åº”è¯¥å˜æˆ ï¼Œ
        chinese_punct = ['ï¼Œ', 'ã€‚', 'ï¼›', 'ï¼š', 'ã€', 'ï¼', 'ï¼Ÿ']
        for punct in chinese_punct:
            # åŒ¹é… \(æ ‡ç‚¹\) æ¨¡å¼ï¼ˆæ ‡ç‚¹å•ç‹¬åœ¨æ•°å­¦å—ä¸­ï¼‰
            pattern = r'\\\(' + re.escape(punct) + r'\\\)'
            text = re.sub(pattern, punct, text)
        
        # ä¿®å¤ \(ï¼Œ\therefore\) è¿™ç±»æ¨¡å¼ â†’ ï¼Œ\(\therefore\)
        text = re.sub(r'\\\(([ï¼Œã€‚ï¼›ï¼šã€ï¼ï¼Ÿ])\\\\therefore\\\)', r'\1\\(\\therefore\\)', text)
        
        # ä¿®å¤ \(ï¼Œ\) åé¢ç´§è·Ÿå†…å®¹çš„æƒ…å†µï¼ˆå¯èƒ½æ˜¯åµŒå¥—é”™è¯¯ï¼‰
        # \(å†…å®¹\(ï¼Œ\)å†…å®¹\) â†’ \(å†…å®¹\)ï¼Œ\(å†…å®¹\)
        for punct in chinese_punct:
            pattern = rf'(\\\([^)]+)\\\({re.escape(punct)}\\\)([^)]+\\\))'
            replacement = r'\1\\)' + punct + r'\\(\2'
            for _ in range(3):
                new_text = re.sub(pattern, replacement, text)
                if new_text == text:
                    break
                text = new_text

        return text

    def normalize_punctuation_in_math(self, text: str) -> str:
        r"""è§„èŒƒåŒ–æ•°å­¦æ¨¡å¼å†…çš„å…¨è§’æ ‡ç‚¹ï¼ˆå¢å¼ºç‰ˆ v1.9.1ï¼‰

        ğŸ†• v1.9.1ï¼šæ·»åŠ æ›´å®Œæ•´çš„ä¸­æ–‡æ ‡ç‚¹æ˜ å°„
        - é¡¿å·ã€å†’å·ã€å¥å·ç­‰
        - ä¿æŠ¤ \text{}, \mbox{}, \mathrm{} å†…çš„ä¸­æ–‡æ ‡ç‚¹
        ğŸ†• P1-003ï¼šæ‰©å±•æ ‡ç‚¹æ˜ å°„åˆ—è¡¨ï¼Œæ·»åŠ $$...$$å¤„ç†
        """
        import re

        # æ ‡ç‚¹æ›¿æ¢æ˜ å°„ï¼ˆåœ¨æ•°å­¦æ¨¡å¼å†…ä½¿ç”¨åŠè§’ï¼‰
        punct_map = {
            'ï¼Œ': ',',
            'ï¼›': ';',
            'ï¼š': ':',
            'ï¼ˆ': '(',
            'ï¼‰': ')',
            'ã€': ',',  # é¡¿å·è½¬ä¸ºé€—å·
            'ã€‚': '.',
            'ï¼': '!',
            'ï¼Ÿ': '?',
            'ã€': '[',
            'ã€‘': ']',
            'ã€”': '[',
            'ã€•': ']',
            'ã€Œ': '"',
            'ã€': '"',
        }

        def replace_in_math(match):
            content = match.group(1)
            # ä¸å¤„ç† \text{}, \mbox{}, \mathrm{} å†…çš„å†…å®¹
            protected = []
            def save_text(m):
                protected.append(m.group(0))
                return f"@@TEXT_{len(protected)-1}@@"

            # ä¿æŠ¤å„ç§æ–‡æœ¬å‘½ä»¤
            content = re.sub(r'\\text\{[^}]*\}', save_text, content)
            content = re.sub(r'\\mbox\{[^}]*\}', save_text, content)
            content = re.sub(r'\\mathrm\{[^}]*\}', save_text, content)
            content = re.sub(r'\\textbf\{[^}]*\}', save_text, content)
            content = re.sub(r'\\textit\{[^}]*\}', save_text, content)

            # æ›¿æ¢å…¨è§’æ ‡ç‚¹
            for full, half in punct_map.items():
                content = content.replace(full, half)

            # æ¢å¤ä¿æŠ¤çš„å†…å®¹
            for i, p in enumerate(protected):
                content = content.replace(f"@@TEXT_{i}@@", p)

            return r'\(' + content + r'\)'
        
        # å¤„ç† \(...\) å†…çš„æ ‡ç‚¹ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…ï¼Œæ”¯æŒåµŒå¥—æ‹¬å·ï¼‰
        def replace_in_math_v2(match):
            content = match.group(1)
            # ä¸å¤„ç† \text{}, \mbox{}, \mathrm{} å†…çš„å†…å®¹
            protected = []
            def save_text(m):
                protected.append(m.group(0))
                return f"@@TEXT_{len(protected)-1}@@"

            # ä¿æŠ¤å„ç§æ–‡æœ¬å‘½ä»¤
            content = re.sub(r'\\text\{[^}]*\}', save_text, content)
            content = re.sub(r'\\mbox\{[^}]*\}', save_text, content)
            content = re.sub(r'\\mathrm\{[^}]*\}', save_text, content)
            content = re.sub(r'\\textbf\{[^}]*\}', save_text, content)
            content = re.sub(r'\\textit\{[^}]*\}', save_text, content)

            # æ›¿æ¢å…¨è§’æ ‡ç‚¹
            for full, half in punct_map.items():
                content = content.replace(full, half)

            # æ¢å¤ä¿æŠ¤çš„å†…å®¹
            for i, p in enumerate(protected):
                content = content.replace(f"@@TEXT_{i}@@", p)

            return r'\(' + content + r'\)'
        
        # ğŸ†• v1.9.2: ä½¿ç”¨åŸºäºä½ç½®çš„åŒ¹é…æ¥å¤„ç†åµŒå¥—æ‹¬å·
        def process_all_math_blocks(text: str) -> str:
            """é€ä¸ªå¤„ç†æ‰€æœ‰æ•°å­¦å—ï¼Œæ”¯æŒåµŒå¥—æ‹¬å·"""
            result = []
            i = 0
            n = len(text)
            
            while i < n:
                # æŸ¥æ‰¾ \(
                if i < n - 1 and text[i:i+2] == r'\(':
                    # æ‰¾åˆ°å¯¹åº”çš„ \)
                    start = i
                    depth = 1
                    j = i + 2
                    
                    while j < n - 1 and depth > 0:
                        if text[j:j+2] == r'\(':
                            depth += 1
                            j += 2
                        elif text[j:j+2] == r'\)':
                            depth -= 1
                            if depth == 0:
                                break
                            j += 2
                        else:
                            j += 1
                    
                    if depth == 0:
                        # æˆåŠŸåŒ¹é…ï¼Œå¤„ç†å†…å®¹
                        math_content = text[start+2:j]
                        
                        # ä¿æŠ¤ \text{} ç­‰
                        protected = []
                        def save_text(m):
                            protected.append(m.group(0))
                            return f"@@TEXT_{len(protected)-1}@@"
                        
                        processed = re.sub(r'\\text\{[^}]*\}', save_text, math_content)
                        processed = re.sub(r'\\mbox\{[^}]*\}', save_text, processed)
                        processed = re.sub(r'\\mathrm\{[^}]*\}', save_text, processed)
                        
                        # æ›¿æ¢å…¨è§’æ ‡ç‚¹
                        for full, half in punct_map.items():
                            processed = processed.replace(full, half)
                        
                        # æ¢å¤ä¿æŠ¤çš„å†…å®¹
                        for idx, p in enumerate(protected):
                            processed = processed.replace(f"@@TEXT_{idx}@@", p)
                        
                        result.append(r'\(' + processed + r'\)')
                        i = j + 2
                    else:
                        # æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„ \)ï¼Œä¿æŒåŸæ ·
                        result.append(text[i])
                        i += 1
                else:
                    result.append(text[i])
                    i += 1
            
            return ''.join(result)
        
        text = process_all_math_blocks(text)
        
        # ğŸ†• P1-003: åŒæ ·å¤„ç† $$...$$ å†…çš„æ ‡ç‚¹ï¼ˆè½¬æ¢å‰ï¼‰
        def replace_in_dollar(match):
            content = match.group(1)
            for full, half in punct_map.items():
                content = content.replace(full, half)
            return '$$' + content + '$$'
        
        text = re.sub(r'\$\$([^$]+)\$\$', replace_in_dollar, text)

        return text

    def split_colon_from_math(self, text: str) -> str:
        r"""åˆ†ç¦»æ•°å­¦æ¨¡å¼å†…çš„ä¸­æ–‡å†’å·
        
        æ¨¡å¼ï¼š\(æ ‡ç­¾ï¼šå…¬å¼\) â†’ \(æ ‡ç­¾\)ï¼š\(å…¬å¼\)
        """
        import re

        # æ¨¡å¼1: \(å•å­—æ¯ï¼šå…¬å¼\)
        pattern1 = r'\\\(([A-Za-z])ï¼š([^)]+)\\\)'
        text = re.sub(pattern1, r'\\(\1\\)ï¼š\\(\2\\)', text)
        
        # æ¨¡å¼2: \(å˜é‡_ä¸‹æ ‡ï¼šå…¬å¼\)
        pattern2 = r'\\\(([a-z]_\{[^}]+\})ï¼š([^)]+)\\\)'
        text = re.sub(pattern2, r'\\(\1\\)ï¼š\\(\2\\)', text)
        
        # æ¨¡å¼3: \(å˜é‡ä¸‹æ ‡ï¼šå…¬å¼\) (æ— èŠ±æ‹¬å·)
        pattern3 = r'\\\(([a-z]_\d+)ï¼š([^)]+)\\\)'
        text = re.sub(pattern3, r'\\(\1\\)ï¼š\\(\2\\)', text)
        
        return text
    
    def fix_math_symbol_chinese_boundary(self, text: str) -> str:
        r"""ä¿®å¤æ•°å­¦ç¬¦å·åç›´æ¥è·Ÿä¸­æ–‡çš„è¾¹ç•Œé—®é¢˜
        
        å¤„ç†æ¨¡å¼ï¼š\(symbolä¸­æ–‡...\) â†’ \(symbol\)ä¸­æ–‡...\)
        """
        import re
        
        # éœ€è¦åˆ†ç¦»çš„æ•°å­¦ç¬¦å·åˆ—è¡¨
        symbols = [
            r'\\therefore',
            r'\\because', 
            r'\\subset',
            r'\\supset',
            r'\\in',
            r'\\notin',
            r'\\cap',
            r'\\cup',
            r'\\parallel',
            r'\\perp',
            r'\\forall',
            r'\\exists',
            r'\\Rightarrow',
            r'\\Leftrightarrow',
            r'\\sim',
            r'\\cong',
            r'\\equiv',
        ]
        
        # å¤šæ¬¡è¿­ä»£å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šåŒ¹é…
        max_iterations = 5
        for _ in range(max_iterations):
            changed = False
            for sym in symbols:
                # åŒ¹é… \(å‰ç¼€symbolä¸­æ–‡åç¼€\) æ¨¡å¼
                # å…¶ä¸­ symbol åé¢ç›´æ¥è·Ÿä¸­æ–‡
                pattern = rf'(\\\()([^)]*?)({sym})([\u4e00-\u9fa5]+)([^)]*?)(\\\))'
                
                def replace_fn(m):
                    nonlocal changed
                    changed = True
                    
                    open_paren = m.group(1)   # \(
                    before = m.group(2)        # symbol å‰çš„å†…å®¹
                    symbol = m.group(3)        # æ•°å­¦ç¬¦å·
                    chinese = m.group(4)       # ä¸­æ–‡
                    after = m.group(5)         # ä¸­æ–‡åçš„å†…å®¹
                    close_paren = m.group(6)   # \)
                    
                    # é‡ç»„ï¼š\(å‰ç¼€+symbol\)ä¸­æ–‡\(åç¼€\)
                    result = ''
                    
                    # å‰ç¼€éƒ¨åˆ†
                    if before.strip():
                        result += open_paren + before + symbol + close_paren
                    else:
                        result += open_paren + symbol + close_paren
                    
                    # ä¸­æ–‡éƒ¨åˆ†ï¼ˆåœ¨æ•°å­¦æ¨¡å¼å¤–ï¼‰
                    result += chinese
                    
                    # åç¼€éƒ¨åˆ† - é€’å½’å¤„ç†
                    if after.strip():
                        result += open_paren + after + close_paren
                    
                    return result
                
                text = re.sub(pattern, replace_fn, text, flags=re.DOTALL)
            
            if not changed:
                break
        
        # æ¸…ç†ç©ºçš„æ•°å­¦æ¨¡å¼
        text = re.sub(r'\\\(\s*\\\)', '', text)
        
        return text

    def split_chinese_from_math(self, text: str) -> str:
        """å°†ä¸­æ–‡è¯æ±‡ä»æ•°å­¦æ¨¡å¼ä¸­åˆ†ç¦» - é‡å†™ç‰ˆ
        
        ç­–ç•¥ï¼šå°†å¼€å¤´å’Œç»“å°¾çš„ä¸­æ–‡ç§»åˆ°æ•°å­¦æ¨¡å¼å¤–éƒ¨ï¼Œè€Œä¸æ˜¯åœ¨å†…éƒ¨æ’å…¥å®šç•Œç¬¦
        """
        import re
        
        def process_math_block(match):
            content = match.group(1)
            original = match.group(0)
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½ï¼Œä¿æŒåŸæ ·
            if not content.strip():
                return original
            
            prefix = ''
            suffix = ''
            core = content
            
            # æ£€æµ‹å¹¶æå–å¼€å¤´çš„ä¸­æ–‡
            chinese_start = re.match(r'^([\u4e00-\u9fa5ï¼Œã€‚ï¼›ï¼šã€ï¼ï¼Ÿ\s]+)', core)
            if chinese_start:
                prefix = chinese_start.group(1)
                core = core[len(prefix):]
            
            # æ£€æµ‹å¹¶æå–ç»“å°¾çš„ä¸­æ–‡
            chinese_end = re.search(r'([\u4e00-\u9fa5ï¼Œã€‚ï¼›ï¼šã€ï¼ï¼Ÿ\s]+)$', core)
            if chinese_end:
                suffix = chinese_end.group(1)
                core = core[:-len(suffix)]
            
            # å¦‚æœæ ¸å¿ƒå†…å®¹è¢«å®Œå…¨ç§»é™¤ï¼Œè¯´æ˜åŸæœ¬å°±ä¸åº”è¯¥æ˜¯æ•°å­¦æ¨¡å¼
            if not core.strip():
                return prefix + suffix
            
            # é‡ç»„ï¼šä¸­æ–‡å‰ç¼€ + \(æ ¸å¿ƒå…¬å¼\) + ä¸­æ–‡åç¼€
            result = prefix + r'\(' + core + r'\)' + suffix
            
            # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„ç©ºæ•°å­¦æ¨¡å¼
            result = re.sub(r'\\\(\s*\\\)', '', result)
            
            return result
        
        # å¤„ç†æ‰€æœ‰ \(...\) å—
        return re.sub(r'\\\(([^)]*?)\\\)', process_math_block, text, flags=re.DOTALL)

    def balance_delimiters(self, text: str) -> str:
        """å¹³è¡¡æ•°å­¦å®šç•Œç¬¦ï¼ˆå¢å¼ºç‰ˆ v1.9.3ï¼‰
        
        ğŸ†• v1.9.3 ä¿®å¤:
        - ç§»é™¤äº†é”™è¯¯çš„ connector å‰æ·»åŠ  \) çš„é€»è¾‘
        - è¯¥é€»è¾‘å‡è®¾ \therefore ç­‰ç¬¦å·å‰ä¸€å®šæœ‰æ•°å­¦å†…å®¹éœ€è¦é—­åˆ
        - ä½†å®é™…ä¸Šè¿™äº›ç¬¦å·å¯èƒ½å‡ºç°åœ¨è¡Œé¦–ï¼Œå‰é¢æ˜¯æ™®é€šæ–‡æœ¬æˆ–ä¸­æ–‡æ ‡ç‚¹
        
        ğŸ†• v1.9.2 æ”¹è¿›:
        1. æ”¯æŒè·¨è¡Œæ•°å­¦ç¯å¢ƒï¼ˆarray/casesï¼‰çš„å¹³è¡¡æ£€æŸ¥
        3. å…¨å±€å¹³è¡¡æ£€æŸ¥å’Œä¿®å¤
        """
        import re

        # æ­¥éª¤1ï¼šå¤„ç†è·¨è¡Œæ•°å­¦ç¯å¢ƒ
        # æ£€æµ‹ \(\left\{ \begin{array} ä½†æ²¡æœ‰å¯¹åº”çš„ \end{array} \right.\)
        lines = text.split('\n')
        processed_lines = []
        pending_close = 0  # ç´¯ç§¯éœ€è¦é—­åˆçš„æ•°é‡
        in_multiline_math = False
        
        for i, line in enumerate(lines):
            if line.strip().startswith('%'):
                processed_lines.append(line)
                continue

            # æ£€æµ‹è·¨è¡Œæ•°å­¦ç¯å¢ƒå¼€å§‹
            if re.search(r'\\\(.*\\begin\{(array|cases|matrix|pmatrix|bmatrix)\}', line) and \
               not re.search(r'\\end\{(array|cases|matrix|pmatrix|bmatrix)\}.*\\\)', line):
                in_multiline_math = True
                pending_close += 1
            
            # æ£€æµ‹è·¨è¡Œæ•°å­¦ç¯å¢ƒç»“æŸ
            if in_multiline_math and re.search(r'\\end\{(array|cases|matrix|pmatrix|bmatrix)\}', line):
                # æ£€æŸ¥è¿™è¡Œæ˜¯å¦æœ‰ \)
                if not re.search(r'\\\)', line):
                    # åœ¨ \right. æˆ–è¡Œå°¾æ·»åŠ  \)
                    if r'\right.' in line:
                        line = line.replace(r'\right.', r'\right.\)')
                    else:
                        line = line + r'\)'
                    pending_close = max(0, pending_close - 1)
                in_multiline_math = False

            # åœ¨æ¯è¡Œå†…æ£€æŸ¥å¹³è¡¡ï¼ˆä»…å¯¹éè·¨è¡Œç¯å¢ƒï¼‰
            if not in_multiline_math:
                opens = list(re.finditer(r'\\\(', line))
                closes = list(re.finditer(r'\\\)', line))
                open_count = len(opens)
                close_count = len(closes)

                if open_count > close_count:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è·¨è¡Œå¼€å§‹
                    if not re.search(r'\\begin\{(array|cases|matrix)', line):
                        line = line + r'\)' * (open_count - close_count)
                elif close_count > open_count:
                    diff = close_count - open_count
                    for _ in range(diff):
                        line = re.sub(r'^([^\\]*)\\\)', r'\1', line, count=1)

            processed_lines.append(line)

        return '\n'.join(processed_lines)
    
    def final_cleanup(self, text: str) -> str:
        """æœ€ç»ˆæ¸…ç†å’ŒéªŒè¯ï¼ˆå¢å¼ºç‰ˆ v1.9.2ï¼‰
        
        ğŸ†• v1.9.2 æ”¹è¿›:
        1. å…¨å±€å®šç•Œç¬¦å¹³è¡¡ä¿®å¤
        2. è¯†åˆ«å¹¶ä¿®å¤å­¤ç«‹çš„æ•°å­¦å†…å®¹
        """
        import re
        
        if not text:
            return text
        
        # 1. æ¸…ç†æ®‹ç•™çš„ $$
        text = re.sub(r'\$\$', '', text)
        
        # 2. æ¸…ç†ç©ºçš„æ•°å­¦æ¨¡å¼
        text = re.sub(r'\\\(\s*\\\)', '', text)
        
        # 3. æ¸…ç†è¿ç»­çš„å®šç•Œç¬¦
        for _ in range(3):
            text = re.sub(r'\\\(\\\(', r'\\(', text)
            text = re.sub(r'\\\)\\\)', r'\\)', text)
        
        # 4. ğŸ†• å…¨å±€å®šç•Œç¬¦å¹³è¡¡ä¿®å¤
        open_count = len(re.findall(r'\\\(', text))
        close_count = len(re.findall(r'\\\)', text))
        
        if open_count != close_count:
            diff = open_count - close_count
            print(f"âš ï¸ è­¦å‘Šï¼šå®šç•Œç¬¦ä¸å¹³è¡¡ï¼\\( = {open_count}, \\) = {close_count}, diff = {diff}")
            
            # å°è¯•æ™ºèƒ½ä¿®å¤
            if diff > 0:
                # \( å¤šäº \)ï¼Œéœ€è¦æ·»åŠ  \)
                # æŸ¥æ‰¾å¯èƒ½ç¼ºå°‘ \) çš„ä½ç½®ï¼šè¡Œå°¾æœ‰ \( ä½†æ²¡æœ‰å¯¹åº”çš„ \)
                lines = text.split('\n')
                fixed_lines = []
                remaining_diff = diff
                
                for line in lines:
                    if remaining_diff > 0 and not line.strip().startswith('%'):
                        line_opens = len(re.findall(r'\\\(', line))
                        line_closes = len(re.findall(r'\\\)', line))
                        line_diff = line_opens - line_closes
                        
                        if line_diff > 0:
                            # åœ¨è¡Œå°¾æ·»åŠ ç¼ºå°‘çš„ \)
                            line = line + r'\)' * min(line_diff, remaining_diff)
                            remaining_diff -= line_diff
                    fixed_lines.append(line)
                
                text = '\n'.join(fixed_lines)
            elif diff < 0:
                # \) å¤šäº \(ï¼Œéœ€è¦ç§»é™¤å¤šä½™çš„ \) æˆ–æ·»åŠ  \(
                # æŸ¥æ‰¾è¡Œé¦–å­¤ç«‹çš„ \)
                lines = text.split('\n')
                fixed_lines = []
                remaining_diff = abs(diff)
                
                for line in lines:
                    if remaining_diff > 0 and not line.strip().startswith('%'):
                        # æ£€æŸ¥è¡Œé¦–æ˜¯å¦æœ‰å­¤ç«‹çš„ \)
                        while remaining_diff > 0 and re.match(r'^\s*\\\)', line):
                            line = re.sub(r'^(\s*)\\\)', r'\1', line, count=1)
                            remaining_diff -= 1
                    fixed_lines.append(line)
                
                text = '\n'.join(fixed_lines)
            
            # é‡æ–°éªŒè¯
            new_open = len(re.findall(r'\\\(', text))
            new_close = len(re.findall(r'\\\)', text))
            if new_open != new_close:
                print(f"âš ï¸ è‡ªåŠ¨ä¿®å¤åä»ä¸å¹³è¡¡ï¼š\\( = {new_open}, \\) = {new_close}")
            else:
                print(f"âœ… å®šç•Œç¬¦å·²è‡ªåŠ¨å¹³è¡¡ï¼š\\( = {new_open}, \\) = {new_close}")
        
        return text

    def fix_reversed_delimiters(self, text: str) -> str:
        r"""ä¿®å¤åå‘å®šç•Œç¬¦æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆ v1.9.1ï¼‰

        ä¿®å¤æ¨¡å¼ï¼š
        1. \)ï¼šå…¬å¼\( â†’ \)ï¼š\(å…¬å¼\)ï¼ˆå†’å·åçš„å…¬å¼ç¼ºå°‘å¼€å¯ç¬¦ï¼‰
        2. \)åŠ¨è¯\( â†’ ä¿æŒä¸å˜ï¼ˆå¯èƒ½æ˜¯æ­£ç¡®çš„ï¼‰
        3. å­¤ç«‹çš„ \) åˆ é™¤
        """
        import re
        lines = text.split('\n')
        fixed_lines = []

        for line in lines:
            # è·³è¿‡æ³¨é‡Šè¡Œ
            if line.strip().startswith('%'):
                fixed_lines.append(line)
                continue

            # ğŸ†• æ¨¡å¼1ï¼šä¿®å¤å†’å·åçš„å…¬å¼ç¼ºå°‘ \( çš„æƒ…å†µ
            # åŒ¹é…: \)ï¼šå…¬å¼å†…å®¹\( æˆ– \)ï¼šå…¬å¼å†…å®¹ï¼ˆè¡Œå°¾/æ ‡ç‚¹ï¼‰
            # ä¾‹å¦‚: ç›´çº¿\(l_{1}\)ï¼š\sqrt{3}x - y = 0\) â†’ ç›´çº¿\(l_{1}\)ï¼š\(\sqrt{3}x - y = 0\)
            def fix_colon_pattern(match):
                close_paren = match.group(1)  # \)
                colon = match.group(2)  # ï¼šæˆ–:
                formula = match.group(3)  # å…¬å¼å†…å®¹
                terminator = match.group(4)  # \( æˆ–æ ‡ç‚¹æˆ–è¡Œå°¾

                # æ£€æŸ¥å…¬å¼å†…å®¹æ˜¯å¦åŒ…å«æ•°å­¦ç¬¦å·ï¼ˆç¡®è®¤æ˜¯æ•°å­¦å…¬å¼ï¼‰
                if re.search(r'[a-zA-Z_\^{}\\\d=+\-*/]', formula):
                    # æ˜¯æ•°å­¦å†…å®¹ï¼Œéœ€è¦æ·»åŠ  \(
                    if terminator == r'\(':
                        # åé¢å·²æœ‰ \(ï¼Œæ›¿æ¢ä¸º \)
                        return f'{close_paren}{colon}\\({formula}\\)'
                    elif terminator in ['ï¼Œ', 'ã€‚', 'ï¼›', 'ã€', 'ï¼‰', '\n', '']:
                        # åé¢æ˜¯æ ‡ç‚¹æˆ–è¡Œå°¾ï¼Œæ·»åŠ  \(\)
                        return f'{close_paren}{colon}\\({formula}\\){terminator}'
                    else:
                        return match.group(0)
                else:
                    # ä¸æ˜¯æ•°å­¦å†…å®¹ï¼Œä¿æŒåŸæ ·
                    return match.group(0)

            # åŒ¹é…å†’å·æ¨¡å¼ï¼š\)ï¼š[å…¬å¼å†…å®¹][\(æˆ–æ ‡ç‚¹æˆ–è¡Œå°¾]
            pattern_colon = re.compile(
                r'(\\\))' +                           # æ•è·ç»„1: \)
                r'([ï¼š:])' +                          # æ•è·ç»„2: ä¸­æ–‡æˆ–è‹±æ–‡å†’å·
                r'([^\\(ï¼‰\n]{1,100}?)' +             # æ•è·ç»„3: å…¬å¼å†…å®¹ï¼ˆéè´ªå©ªï¼Œä¸åŒ…å«\(å’Œï¼‰ï¼‰
                r'(\\\(|[ï¼Œã€‚ï¼›ã€ï¼‰]|\n|$)'            # æ•è·ç»„4: \( æˆ–æ ‡ç‚¹æˆ–è¡Œå°¾
            )
            line = pattern_colon.sub(fix_colon_pattern, line)

            # ğŸ†• æ¨¡å¼2ï¼šé€è¡Œæ£€æŸ¥å®šç•Œç¬¦å¹³è¡¡ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
            opens = [(m.start(), r'\(') for m in re.finditer(r'\\\(', line)]
            closes = [(m.start(), r'\)') for m in re.finditer(r'\\\)', line)]

            all_delims = sorted(opens + closes, key=lambda x: x[0])

            depth = 0
            needs_fix = False
            for pos, delim in all_delims:
                if delim == r'\(':
                    depth += 1
                else:
                    depth -= 1
                    if depth < 0:
                        needs_fix = True
                        break

            if needs_fix:
                # ä¿®å¤ç­–ç•¥: ç§»é™¤è¡Œé¦–çš„å­¤ç«‹ \)
                line = re.sub(r'^([^\\\(]*?)\\\)', r'\1', line)

            fixed_lines.append(line)

        return '\n'.join(fixed_lines)

    def process(self, text: str) -> str:
        # é¢„å¤„ç†ï¼šä¿æŠ¤ä¸­æ–‡æ‹¬å·ï¼Œé¿å…ä¸æ•°å­¦æ‹¬å·æ··æ·†
        chinese_paren_map = {
            'ï¼ˆ': '@@ZH_PAREN_OPEN@@',
            'ï¼‰': '@@ZH_PAREN_CLOSE@@',
            'ã€': '@@ZH_BRACKET_OPEN@@',
            'ã€‘': '@@ZH_BRACKET_CLOSE@@',
        }
        for char, placeholder in chinese_paren_map.items():
            text = text.replace(char, placeholder)

        # ğŸ†• P0-001 & P1-005: åœ¨é¢„å¤„ç†å¤šè¡Œæ•°å­¦ä¹‹å‰ä¿®å¤é›†åˆå®šä¹‰å’ŒOCRé”™è¯¯
        text = fix_broken_set_definitions(text)
        text = fix_ocr_specific_errors(text)
        
        # å…ˆé¢„å¤„ç†å¤šè¡Œæ•°å­¦å—
        text = self.preprocess_multiline_math(text)
        # ç„¶åå¤„ç†å‰©ä½™çš„å•è¡Œå…¬å¼
        tokens = self.tokenize(text)
        out = []
        i = 0
        math_depth = 0  # è·Ÿè¸ªæ•°å­¦æ¨¡å¼æ·±åº¦

        while i < len(tokens):
            t_type, val, pos = tokens[i]

            # ğŸ”¥ v1.8.3ï¼šæ™ºèƒ½å¤„ç† \right. è¾¹ç•Œ
            if t_type == TokenType.RIGHT_BOUNDARY:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ•°å­¦æ¨¡å¼å†…ï¼ˆæœ‰æœªé—­åˆçš„ \(ï¼‰
                if math_depth > 0:
                    out.append(r'\right.\)')
                    math_depth -= 1
                else:
                    # ä¸åœ¨æ•°å­¦æ¨¡å¼å†…ï¼Œä¿æŒåŸæ ·ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ \right.ï¼‰
                    out.append(r'\right.')
                i += 1
                continue
            if t_type == TokenType.DOLLAR_DOUBLE:
                # æ”¶é›†ç›´åˆ°ä¸‹ä¸€ä¸ª $$ æˆ– RIGHT_BOUNDARY
                i += 1
                buf = []
                while i < len(tokens):
                    tt, tv, _ = tokens[i]
                    if tt == TokenType.DOLLAR_DOUBLE:
                        i += 1
                        break
                    # ğŸ†• v1.9.4ï¼šé‡åˆ° DOLLAR_SINGLEï¼Œè§†ä¸º $$ çš„é”™è¯¯ç»“æŸç¬¦ï¼ˆ$$...$æ¨¡å¼ï¼‰
                    # å°† $ ä½œä¸ºç»“æŸç¬¦ï¼Œä¸æ”¶é›†åˆ° buf ä¸­
                    if tt == TokenType.DOLLAR_SINGLE:
                        i += 1
                        break
                    # ğŸ†• v1.9.3ï¼šé‡åˆ° RIGHT_BOUNDARYï¼Œè¾“å‡ºå®ƒç„¶åæ£€æŸ¥ä¸‹ä¸€ä¸ªæ˜¯å¦æ˜¯ $$
                    if tt == TokenType.RIGHT_BOUNDARY:
                        buf.append(r'\right.')
                        i += 1
                        # æ£€æŸ¥ä¸‹ä¸€ä¸ª token æ˜¯å¦æ˜¯ $$ ç»“æŸç¬¦
                        if i < len(tokens) and tokens[i][0] == TokenType.DOLLAR_DOUBLE:
                            i += 1  # è·³è¿‡ç»“æŸçš„ $$
                            break
                        # ä¹Ÿæ£€æŸ¥ DOLLAR_SINGLEï¼ˆ$$...\right.$æ¨¡å¼ï¼‰
                        if i < len(tokens) and tokens[i][0] == TokenType.DOLLAR_SINGLE:
                            i += 1
                            break
                        continue
                    buf.append(tv)
                    i += 1
                out.append(r'\(' + ''.join(buf).strip() + r'\)')
                continue

            if t_type == TokenType.DOLLAR_SINGLE:
                i += 1
                buf = []
                while i < len(tokens):
                    tt, tv, _ = tokens[i]
                    if tt == TokenType.DOLLAR_SINGLE:
                        i += 1
                        break
                    # ç¦æ­¢è·¨è¡Œçš„å•ç¾å…ƒå»¶ä¼¸
                    if '\n' in tv:
                        out.append('$')
                        out.extend(buf)
                        break
                    buf.append(tv)
                    i += 1
                if buf:
                    out.append(r'\(' + ''.join(buf) + r'\)')
                continue

            if t_type == TokenType.LATEX_OPEN:
                out.append(val)
                math_depth += 1
                i += 1
                continue

            if t_type == TokenType.LATEX_CLOSE:
                out.append(val)
                math_depth = max(0, math_depth - 1)
                i += 1
                continue
            out.append(val)
            i += 1

        result = ''.join(out)

        # åå¤„ç†æ­¥éª¤ï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰
        result = self.fix_malformed_patterns(result)
        result = self.normalize_punctuation_in_math(result)
        result = self.split_colon_from_math(result)
        result = self.fix_math_symbol_chinese_boundary(result)
        result = self.split_chinese_from_math(result)
        result = self.balance_delimiters(result)
        result = self.final_cleanup(result)

        # ä¿®å¤åå‘å®šç•Œç¬¦
        result = self.fix_reversed_delimiters(result)

        # åå¤„ç†ï¼šæ¢å¤ä¸­æ–‡æ‹¬å·
        for char, placeholder in chinese_paren_map.items():
            result = result.replace(placeholder, char)

        return result


# å•ä¾‹å®ä¾‹ä¾›å…¨å±€è°ƒç”¨
math_sm = MathStateMachine()


# ==================== é…ç½® ====================

VERSION = "v1.9.6"

SECTION_MAP = {
    "ä¸€ã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å¤šé€‰é¢˜": "å¤šé€‰é¢˜",
    "ä¸‰ã€å¡«ç©ºé¢˜": "å¡«ç©ºé¢˜",
    "å››ã€è§£ç­”é¢˜": "è§£ç­”é¢˜",
}

META_PATTERNS = {
    "answer": r"^ã€ç­”æ¡ˆã€‘(.*)$",
    "difficulty": r"^ã€éš¾åº¦ã€‘([\d.]+)",
    "topics": r"^ã€çŸ¥è¯†ç‚¹ã€‘(.*)$",
    "analysis": r"^ã€åˆ†æã€‘(.*)$",
    "explain": r"^ã€è¯¦è§£ã€‘(.*)$",
    "diangjing": r"^ã€ç‚¹ç›ã€‘(.*)$",
    "dianjing_alt": r"^ã€ç‚¹è¯„ã€‘(.*)$",
}

# ğŸ†• æ‰©å±•å›¾ç‰‡æ£€æµ‹ï¼šæ”¯æŒç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ã€å¤šè¡Œå±æ€§å—
# åŒ¹é…ä¸¤ç§å½¢å¼ï¼š
#   1) å¸¦ID: ![@@@id](path){...}
#   2) æ— ID: ![](path){...}
# å±æ€§å—å¯è·¨å¤šè¡Œï¼Œå¯é€‰
IMAGE_PATTERN_WITH_ID = re.compile(
    r"!\[@@@([^\]]+)\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
IMAGE_PATTERN_NO_ID = re.compile(
    r"!\[\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
# å…¼å®¹æ—§ç‰ˆï¼ˆä¿ç•™ç”¨äºç®€å•åœºæ™¯ï¼‰
IMAGE_PATTERN = re.compile(r"!\[\]\((images/[^)]+)\)(?:\{width=(\d+)%\})?")

LATEX_SPECIAL_CHARS = {
    "%": r"\%",
    "&": r"\&",
    "#": r"\#",
    "~": r"\textasciitilde{}",
}

# è§£ææ ‡è®°è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼‰
ANALYSIS_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—',
    'ç»¼ä¸Š', 'æ•…', 'å³', 'åˆ™', 'å¯å¾—'
]


# æ›´ä¸¥æ ¼çš„è§£æèµ·å§‹è¯ï¼Œåªç”¨äºåˆ¤æ–­æ˜¯å¦è¿›å…¥è§£ææ®µè½ï¼ˆé¿å…åƒâ€œåˆ™â€è¿™æ ·åœ¨é¢˜å¹²ä¸­å‡ºç°æ—¶è¢«è¯¯åˆ¤ï¼‰
ANALYSIS_START_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—', 'ç»¼ä¸Š'
]

# ==================== æ–‡ä»¶å¤¹å¤„ç†å‡½æ•° ====================

def infer_figures_dir(input_md: str) -> str:
    """æ ¹æ® Markdown æ–‡ä»¶åæ¨æ–­å›¾ç‰‡ç›®å½•

    æ¨æ–­è§„åˆ™ï¼š
    1. æå– md_path.stem ä½œä¸º prefix
    2. å»é™¤å¸¸è§åç¼€ï¼ˆ_local, _preprocessed, _rawï¼‰
    3. æŒ‰é¡ºåºå°è¯•ä»¥ä¸‹å€™é€‰ç›®å½•ï¼š
       - word_to_tex/output/figures/{prefix}
       - word_to_tex/output/figures/{prefix}/media
    4. è¿”å›ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ç›®å½•ï¼Œéƒ½ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

    Args:
        input_md: Markdown æ–‡ä»¶è·¯å¾„

    Returns:
        æ¨æ–­å‡ºçš„å›¾ç‰‡ç›®å½•è·¯å¾„ï¼Œæˆ–ç©ºå­—ç¬¦ä¸²
    """
    md_path = Path(input_md)

    # æå–æ–‡ä»¶åå‰ç¼€ï¼ˆå»é™¤åç¼€ï¼‰
    prefix = md_path.stem

    # å»é™¤å¸¸è§çš„ Markdown æ–‡ä»¶åç¼€
    for suffix in ['_local', '_preprocessed', '_raw']:
        if prefix.endswith(suffix):
            prefix = prefix[:-len(suffix)]
            break

    # å€™é€‰ç›®å½•åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    candidates = [
        Path("word_to_tex/output/figures") / prefix,
        Path("word_to_tex/output/figures") / prefix / "media",
    ]

    # è¿”å›ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ç›®å½•
    for candidate in candidates:
        if candidate.exists() and candidate.is_dir():
            return str(candidate)

    # éƒ½ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    return ""


def detect_images_for_markdown(md_file: Path) -> Optional[Path]:
    """æ ¹æ® markdown æ–‡ä»¶æ¨æ–­å›¾ç‰‡ç›®å½•"""
    parent = md_file.parent
    candidates: List[Path] = []

    # å¸¸è§„ï¼šåŒçº§ images ç›®å½•
    candidates.append(parent / 'images')

    slug = None
    slug_match = re.match(r'(.+?)_(?:preprocessed|raw|local)\.md$', md_file.name)
    if slug_match:
        slug = slug_match.group(1)
    elif md_file.suffix == '.md':
        slug = md_file.stem

    figures_root = parent / 'figures'
    if slug:
        candidates.append(figures_root / slug / 'media')
        candidates.append(figures_root / slug)
    candidates.append(figures_root / 'media')
    candidates.append(figures_root)

    for cand in candidates:
        if cand.exists():
            return cand

    if slug and figures_root.exists():
        for cand in figures_root.glob(f"**/{slug}*"):
            media_dir = cand / 'media'
            if media_dir.exists():
                return media_dir
            if cand.is_dir():
                return cand

    return None


def find_markdown_and_images(input_path: Path) -> Tuple[Path, Optional[Path]]:
    """æ™ºèƒ½è¯†åˆ«è¾“å…¥è·¯å¾„"""
    input_path = Path(input_path).resolve()
    
    if input_path.is_file() and input_path.suffix == '.md':
        md_file = input_path
        return md_file, detect_images_for_markdown(md_file)
    
    if input_path.is_dir():
        md_files = list(input_path.glob('*_local.md'))
        if not md_files:
            md_files = list(input_path.glob('*.md'))
        
        if not md_files:
            raise FileNotFoundError(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        
        if len(md_files) > 1:
            print(f"âš ï¸  æ‰¾åˆ°å¤šä¸ª .md æ–‡ä»¶ï¼Œä½¿ç”¨ï¼š{md_files[0].name}")
        
        md_file = md_files[0]
        images_dir = detect_images_for_markdown(md_file)
        return md_file, images_dir
    
    raise ValueError(f"æ— æ•ˆçš„è¾“å…¥ï¼š{input_path}")


def copy_images_to_output(images_dir: Path, output_dir: Path) -> int:
    """å¤åˆ¶å›¾ç‰‡"""
    if images_dir is None or not images_dir.exists():
        return 0
    
    output_images_dir = output_dir / 'images'
    if output_images_dir.exists():
        shutil.rmtree(output_images_dir)
    
    shutil.copytree(images_dir, output_images_dir)
    return len(list(output_images_dir.glob('*')))


# ==================== LaTeX å¤„ç†å‡½æ•° ====================

def escape_latex_special(text: str, in_math_mode: bool = False) -> str:
    """è½¬ä¹‰ LaTeX ç‰¹æ®Šå­—ç¬¦ï¼ˆå¢å¼ºç‰ˆ v1.9.2ï¼‰
    
    ğŸ†• v1.9.2 æ”¹è¿›:
    1. æ­£ç¡®ä¿æŠ¤æ•°å­¦æ¨¡å¼å†…çš„ & ï¼ˆç”¨äº matrix/array åˆ—åˆ†éš”ï¼‰
    2. ä¿æŠ¤å·²è½¬ä¹‰çš„å­—ç¬¦ï¼ˆ\&, \%, \#ï¼‰
    3. ä¿æŠ¤ LaTeX å‘½ä»¤ï¼ˆ\text{}, \left, \right ç­‰ï¼‰
    """
    if not text:
        return text
        
    # ä¿æŠ¤å·²ç»è½¬ä¹‰çš„å­—ç¬¦
    protected_escaped = []
    def save_escaped(match):
        protected_escaped.append(match.group(0))
        return f"@@ESCAPED_{len(protected_escaped)-1}@@"
    
    # ä¿æŠ¤å·²è½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'\\[%&#~]', save_escaped, text)
    
    if in_math_mode:
        # åœ¨æ•°å­¦æ¨¡å¼å†…ï¼Œä¸è½¬ä¹‰ &ï¼ˆç”¨äº array/matrix åˆ—åˆ†éš”ï¼‰
        for char in ["%", "#", "~"]:
            if char in LATEX_SPECIAL_CHARS:
                text = text.replace(char, LATEX_SPECIAL_CHARS[char])
    else:
        # ä¿æŠ¤æ³¨é‡Š
        protected_comments = []
        def save_comment(match):
            protected_comments.append(match.group(0))
            return f"@@COMMENT_{len(protected_comments)-1}@@"
        text = re.sub(r'%.*$', save_comment, text, flags=re.MULTILINE)
        
        # ä¿æŠ¤æ•°å­¦æ¨¡å¼å†…çš„ &ï¼ˆç”¨äº array/matrix/tabularï¼‰
        protected_math = []
        def save_math(match):
            protected_math.append(match.group(0))
            return f"@@MATH_{len(protected_math)-1}@@"
        
        # ä¿æŠ¤ \(...\) å’Œ \[...\] å†…çš„å†…å®¹
        text = re.sub(r'\\\([^)]*\\\)', save_math, text, flags=re.DOTALL)
        text = re.sub(r'\\\[[^\]]*\\\]', save_math, text, flags=re.DOTALL)
        
        # ä¿æŠ¤ tabular/array/matrix ç¯å¢ƒ
        text = re.sub(r'\\begin\{(tabular|array|matrix|pmatrix|bmatrix|vmatrix|cases)\}.*?\\end\{\1\}', 
                       save_math, text, flags=re.DOTALL)
        
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        for char, escaped in LATEX_SPECIAL_CHARS.items():
            text = text.replace(char, escaped)
        
        # æ¢å¤ä¿æŠ¤çš„æ•°å­¦æ¨¡å¼
        for i, math_block in enumerate(protected_math):
            text = text.replace(f"@@MATH_{i}@@", math_block)
        
        # æ¢å¤ä¿æŠ¤çš„æ³¨é‡Š
        for i, comment in enumerate(protected_comments):
            text = text.replace(f"@@COMMENT_{i}@@", comment)
    
    # æ¢å¤ä¿æŠ¤çš„å·²è½¬ä¹‰å­—ç¬¦
    for i, escaped in enumerate(protected_escaped):
        text = text.replace(f"@@ESCAPED_{i}@@", escaped)
    
    # æ¸…ç†å¯èƒ½çš„å¼‚å¸¸æ¨¡å¼
    text = re.sub(r'\\\)([\u4e00-\u9fa5]{1,3})\\\(', r'\1', text)

    # ç»Ÿä¸€å¸¸è§æ•°å­¦ç¬¦å·çš„æ’ç‰ˆ
    text = standardize_math_symbols(text)
    
    return text


def standardize_math_symbols(text: str) -> str:
    r"""æ ‡å‡†åŒ–æ•°å­¦ç¬¦å·ï¼ˆè™šæ•°å•ä½/åœ†å‘¨ç‡/è‡ªç„¶åº•æ•°ç­‰ï¼‰

    ä¿®å¤ P2-001: å¤„ç† \text{æ•°å­—}ã€\text{æ•°å­—Ï€} ç­‰æ¨¡å¼
    ğŸ†• P1-001: æ·»åŠ æ•°å­¦å‡½æ•°å’Œæ•°é›†ç¬¦å·çš„æ ‡å‡†åŒ–
    """
    if not text:
        return text

    # ğŸ†• P1-001: æ•°å­¦å‡½æ•°æ›¿æ¢ (\text{sin} â†’ \sin)
    math_func_replacements = [
        (r'\\text\{\s*sin\s*\}', r'\\sin'),
        (r'\\text\{\s*cos\s*\}', r'\\cos'),
        (r'\\text\{\s*tan\s*\}', r'\\tan'),
        (r'\\text\{\s*cot\s*\}', r'\\cot'),
        (r'\\text\{\s*sec\s*\}', r'\\sec'),
        (r'\\text\{\s*csc\s*\}', r'\\csc'),
        (r'\\text\{\s*ln\s*\}', r'\\ln'),
        (r'\\text\{\s*log\s*\}', r'\\log'),
        (r'\\text\{\s*lg\s*\}', r'\\lg'),
        (r'\\text\{\s*lim\s*\}', r'\\lim'),
        (r'\\text\{\s*max\s*\}', r'\\max'),
        (r'\\text\{\s*min\s*\}', r'\\min'),
        (r'\\text\{\s*exp\s*\}', r'\\exp'),
        (r'\\text\{\s*arcsin\s*\}', r'\\arcsin'),
        (r'\\text\{\s*arccos\s*\}', r'\\arccos'),
        (r'\\text\{\s*arctan\s*\}', r'\\arctan'),
    ]
    
    for pattern, replacement in math_func_replacements:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # ğŸ†• P1-001: æ•°é›†ç¬¦å·æ›¿æ¢ (\text{N} â†’ \mathbb{N})
    number_set_replacements = [
        (r'\\text\{\s*N\s*\}', r'\\mathbb{N}'),
        (r'\\text\{\s*Z\s*\}', r'\\mathbb{Z}'),
        (r'\\text\{\s*Q\s*\}', r'\\mathbb{Q}'),
        (r'\\text\{\s*R\s*\}', r'\\mathbb{R}'),
        (r'\\text\{\s*C\s*\}', r'\\mathbb{C}'),
    ]
    
    for pattern, replacement in number_set_replacements:
        text = re.sub(pattern, replacement, text)

    # è™šæ•°å•ä½ - ä¿æŒ \text{i} æ ¼å¼ä¸èŒƒæœ¬ä¸€è‡´
    # æ³¨é‡Šæ‰ä»¥ä¸‹è½¬æ¢ï¼Œä¿ç•™åŸå§‹ \text{i} æ ¼å¼
    # text = re.sub(r'\\text\{\s*i\s*\}', r'\\mathrm{i}', text)
    # text = re.sub(r'\\text\{\s*-\s*i\s*\}', r'-\\mathrm{i}', text)

    # ğŸ†• P2-001: å¤„ç† \text{æ•°å­—Ï€} æˆ– \text{æ•°å­—\pi}ï¼ˆå¿…é¡»åœ¨ \text{æ•°å­—} ä¹‹å‰ï¼‰
    text = re.sub(r'\\text\{(\d+)Ï€\}', r'\1\\pi', text)
    text = re.sub(r'\\text\{(\d+)\\pi\}', r'\1\\pi', text)

    # ğŸ†• P2-001: å¤„ç† \text{Ï€æ•°å­—} æˆ– \text{\piæ•°å­—}
    text = re.sub(r'\\text\{Ï€(\d+)\}', r'\\pi\1', text)
    text = re.sub(r'\\text\{\\pi(\d+)\}', r'\\pi\1', text)

    # ğŸ†• P2-001: å¤„ç† \text{æ•°å­—}
    text = re.sub(r'\\text\{(\d+)\}', r'\1', text)

    # åœ†å‘¨ç‡
    text = re.sub(r'\\text\{\s*Ï€\s*\}', r'\\pi', text)
    text = re.sub(r'(?<!\\)Ï€', r'\\pi', text)

    # è‡ªç„¶å¯¹æ•°åº• eï¼šä»…åœ¨ä½œä¸ºæŒ‡æ•°åº•æ•°æ—¶æ›¿æ¢
    text = re.sub(r'\\text\{\s*e\s*\}(?=\s*[\^_])', r'\\mathrm{e}', text)

    return text


# DEPRECATED: å·²è¢« MathStateMachine æ›¿æ¢ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§æµ‹è¯•ï¼›ä¸»æµç¨‹ä¸å†è°ƒç”¨
def smart_inline_math(text: str) -> str:
    r"""æ™ºèƒ½è½¬æ¢è¡Œå†…å…¬å¼ï¼š$...$ -> \(...\)ï¼Œ$$...$$ -> \(...\)

    ğŸ†• v1.5 æ”¹è¿›ï¼šå½»åº•é¿å…åŒé‡åŒ…è£¹ï¼Œexamx ç»Ÿä¸€ä½¿ç”¨ \(...\)

    æ³¨æ„ï¼šæ‰€æœ‰ $$...$$ æ˜¾ç¤ºå…¬å¼éƒ½ä¼šè¢«è½¬æ¢ä¸ºè¡Œå†… \(...\) æ ¼å¼ï¼Œ
    è¿™æ˜¯ä¸ºäº†ä¸ examx åŒ…çš„å…¼å®¹æ€§ã€‚å¦‚æœéœ€è¦çœŸæ­£çš„æ˜¾ç¤ºå…¬å¼ï¼Œ
    åº”åœ¨åç»­æ‰‹åŠ¨è°ƒæ•´ä¸º \[...\] æ ¼å¼ã€‚
    """
    if not text:
        return text
    
    # æ­¥éª¤1: ä¿æŠ¤å·²æœ‰çš„è¡Œå†…å…¬å¼ \(...\)ï¼ˆé¿å…é‡å¤è½¬æ¢ï¼‰
    inline_math_blocks = []
    def save_inline(match):
        inline_math_blocks.append(match.group(0))
        return f"@@INLINEMATH{len(inline_math_blocks)-1}@@"
    text = re.sub(r'\\\((.+?)\\\)', save_inline, text, flags=re.DOTALL)
    
    # æ­¥éª¤2: ä¿æŠ¤å·²æœ‰çš„æ˜¾ç¤ºå…¬å¼ \[...\]ï¼ˆä¿æŒä¸å˜ï¼‰
    display_math_blocks = []
    def save_display(match):
        display_math_blocks.append(match.group(0))
        return f"@@DISPLAYMATH{len(display_math_blocks)-1}@@"
    text = re.sub(r'\\\[(.+?)\\\]', save_display, text, flags=re.DOTALL)
    
    # æ­¥éª¤3: ä¿æŠ¤TikZåæ ‡ $(A)$ æˆ– $(A)!0.5!(B)$ æˆ– $(A)+(1,2)$
    tikz_coords = []
    def save_tikz_coord(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)!0.5!(B)$' æˆ– '$(0,1)$'
        inner = block[2:-2]         # å»æ‰å¤–å±‚ '$(' å’Œ ')$'
        # ä»…å½“å†…éƒ¨åŒ…å« '!' æˆ– å¤§å†™å­—æ¯ æ—¶ï¼Œè®¤ä¸ºæ˜¯ TikZ åæ ‡è¡¨è¾¾å¼
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZCOORD{len(tikz_coords)-1}@@"
        else:
            # å¦åˆ™è®¤ä¸ºæ˜¯æ™®é€šæ•°å­¦åæ ‡/åŒºé—´ï¼ŒåŸæ ·è¿”å›
            return block
    # åŒ¹é… TikZ åæ ‡ï¼š$(...)$ å†…éƒ¨æ˜¯ç®€å•çš„åæ ‡è®¡ç®—è¡¨è¾¾å¼
    # åŒ…å«å­—æ¯ã€æ•°å­—ã€æ‹¬å·ã€åŠ å‡ä¹˜é™¤ã€ç‚¹ã€æ„Ÿå¹å·ã€å†’å·ç­‰ä½†ä¸åŒ…å«å¤æ‚æ•°å­¦
    text = re.sub(r'\$\([A-Za-z0-9!+\-*/\.\(\):,\s]+\)\$', save_tikz_coord, text)

    # æ­¥éª¤3.5: ğŸ†• v1.8 ä¿®å¤ \right.\ $$ è¾¹ç•Œé—®é¢˜
    # å°† \right.\ $$ è½¬æ¢ä¸º \right.\) ï¼ˆé—­åˆå½“å‰æ•°å­¦æ¨¡å¼ï¼‰
    text = re.sub(r'\\right\.\\\s+\$\$', r'\\right.\\) ', text)

    # æ­¥éª¤4: è½¬æ¢æ˜¾ç¤ºå…¬å¼ $$ ... $$ ä¸º \(...\)ï¼ˆexamx ç»Ÿä¸€é£æ ¼ï¼‰
    # æ³¨æ„ï¼šæ‰€æœ‰ $$...$$ éƒ½è½¬ä¸ºè¡Œå†…æ ¼å¼ï¼Œä¸ç”Ÿæˆ \[...\]
    text = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # æ­¥éª¤5: è½¬æ¢å• $ ... $ ä¸º \(...\)
    text = re.sub(r'(?<!\\)\$([^\$]+?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤6: å…œåº•æ£€æŸ¥ï¼Œæ¸…ç†æ®‹ç•™çš„å• $ï¼ˆå•è¡Œå†…ï¼Œé™åˆ¶200å­—ç¬¦ï¼‰
    text = re.sub(r'(?<!\\)\$([^\$\n]{1,200}?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤7: æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, block in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZCOORD{i}@@", block)
    for i, block in enumerate(display_math_blocks):
        text = text.replace(f"@@DISPLAYMATH{i}@@", block)
    for i, block in enumerate(inline_math_blocks):
        text = text.replace(f"@@INLINEMATH{i}@@", block)
    
    return text


# DEPRECATED: å·²è¢« MathStateMachine ç»Ÿä¸€å¤„ç†åŒé‡åŒ…è£¹
def fix_double_wrapped_math(text: str) -> str:
    r"""ä¿®æ­£åŒé‡åŒ…è£¹çš„æ•°å­¦å…¬å¼
    
    ğŸ†• v1.6 å¢å¼ºï¼šæ¸…ç†æ›´å¤šåµŒå¥—æ¨¡å¼
    ğŸ†• v1.5 æ–°å¢ï¼šæ¸…ç†å¯èƒ½æ®‹ç•™çš„åµŒå¥—æ ¼å¼
    ä¾‹å¦‚ï¼š$$\(...\)$$ â†’ \(...\)
    """
    if not text:
        return text
    
    # ä¿®æ­£ $$\(...\)$$ æˆ– $$\[...\]$$
    # æ³¨æ„ï¼š\\\( åŒ¹é…å­—é¢çš„ \(
    text = re.sub(r'\$\$\s*\\\((.+?)\\\)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\$\s*\\\[(.+?)\\\]\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ $\(...\)$ æˆ– $\[...\]$
    text = re.sub(r'\$\s*\\\((.+?)\\\)\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\s*\\\[(.+?)\\\]\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ä¸‰é‡åµŒå¥—ï¼ˆæç«¯æƒ…å†µï¼‰
    text = re.sub(r'\\\(\s*\\\((.+?)\\\)\s*\\\)', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† \because\(\) æˆ– \therefore\(\) çš„ç©ºåµŒå¥—
    # æ³¨æ„ï¼šæ›¿æ¢åä¿ç•™ç©ºæ ¼ï¼Œé¿å…ä¸åç»­å­—æ¯è¿æ¥
    text = re.sub(r'\\because\s*\\\(\\\)\s*', r'\\because ', text)
    text = re.sub(r'\\therefore\s*\\\(\\\)\s*', r'\\therefore ', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† \(\because\(\) æˆ– \(\therefore\(\) å½¢å¼
    text = re.sub(r'\\\(\\because\s*\\\(\\\)\s*', r'\\(\\because ', text)
    text = re.sub(r'\\\(\\therefore\s*\\\(\\\)\s*', r'\\(\\therefore ', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç†ç‹¬ç«‹çš„ç©ºæ‹¬å· \(\)ï¼ˆå¯èƒ½å‡ºç°åœ¨ä»»ä½•ä½ç½®ï¼‰
    text = re.sub(r'\\\(\s*\\\)', r'', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šä¿®æ­£ \(...\(\)...\) å½¢å¼çš„åµŒå¥—ï¼ˆç©ºå ä½ç¬¦ï¼‰
    # è¿­ä»£æ¸…ç†ï¼Œæœ€å¤š3æ¬¡
    for _ in range(3):
        before = text
        text = re.sub(r'\\\(([^)]*?)\\\(\\\)([^)]*?)\\\)', r'\\(\1\2\\)', text, flags=re.DOTALL)
        if text == before:
            break
    
    return text


def fix_array_boundaries(text: str) -> str:
    r"""ä¿®å¤ array ç¯å¢ƒçš„è¾¹ç•Œç¬¦é”™è¯¯
    
    ğŸ†• v1.6 P0 ä¿®å¤ï¼šä¿®æ­£ \right.\\) â†’ \right.\)
    """
    # ä¿®æ­£ \right. åçš„åŒåæ–œæ 
    text = re.sub(r'\\right\.\\\\\)', r'\\right.\\)', text)
    
    # ä¿®æ­£å…¶ä»–è¾¹ç•Œç¬¦
    text = re.sub(r'\\right\)\\\\\)', r'\\right)\\)', text)
    text = re.sub(r'\\right\]\\\\\)', r'\\right]\\)', text)
    text = re.sub(r'\\right\}\\\\\)', r'\\right}\\)', text)
    
    # åŒæ ·ä¿®æ­£ \left çš„æƒ…å†µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    text = re.sub(r'\\\\\(\\left', r'\\(\\left', text)
    
    return text


def clean_image_attributes(text: str) -> str:
    r"""ç»Ÿä¸€æ¸…ç† Markdown å›¾ç‰‡æ ‡è®°ä¸­çš„å±æ€§å—ï¼ˆå¢å¼ºç‰ˆ P2-001ï¼‰
    
    æ”¯æŒï¼š
    - å•è¡Œå±æ€§å—ï¼š{width="3in" height="2in"}
    - è·¨è¡Œå±æ€§å—ï¼š{width="3in"\nheight="2in"}
    - ç§‘å­¦è®¡æ•°æ³•å°ºå¯¸ï¼š{width="1.38e-2in"}
    - å­¤ç«‹çš„ width/height è¡Œ
    - æå°å›¾ç‰‡ç§»é™¤ï¼ˆOCR å™ªå£°ï¼‰
    """
    if not text:
        return text

    # ğŸ†• P1-004 ä¿®å¤ï¼šæ”¯æŒè·¨è¡Œå±æ€§å—ï¼ˆä½¿ç”¨ DOTALL æ ‡å¿—ï¼‰
    # åŒ¹é…åŒ…å«ç§‘å­¦è®¡æ•°æ³•çš„å°ºå¯¸å€¼ï¼Œå¦‚ 1.3888888888888888e-2in
    attr_pattern = re.compile(
        r'\{[^{}]*(?:width|height)\s*=\s*"[^"]*"[^{}]*\}',
        re.IGNORECASE | re.DOTALL
    )
    text = attr_pattern.sub('', text)

    # ğŸ†• P2-001: æ¸…ç†å­¤ç«‹çš„ width="..." / height="..." è¡Œ
    text = re.sub(r'^\s*(width|height)="[^"]*"\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
    
    # ğŸ†• P2-001: æ¸…ç†è·¨è¡Œçš„å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s*\n\s*height="[^"]*"\}', '', text, flags=re.MULTILINE)
    
    # ğŸ†• P2-001: æ¸…ç†å•è¡Œå®Œæ•´å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s+height="[^"]*"\}', '', text)
    
    # ğŸ†• P2-001: æ¸…ç†æ®‹ç•™çš„ height="..." å’Œ width="..."ï¼ˆå¸¦å¯èƒ½çš„å°¾éš }ï¼‰
    text = re.sub(r'height="[^"]*"[}]*', '', text)
    text = re.sub(r'width="[^"]*"[}]*', '', text)
    
    # ğŸ†• P2-001: ç§»é™¤æå°å›¾ç‰‡ï¼ˆå°ºå¯¸ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³• e-2 æˆ–æ›´å°ï¼Œå¯èƒ½æ˜¯ OCR å™ªå£°ï¼‰
    tiny_pattern = re.compile(
        r'!\[[^\]]*\]\([^)]+\)\s*\{[^}]*?(?:\d+\.?\d*e-[2-9]|\d+\.?\d*e-\d{2,})in[^}]*\}',
        re.IGNORECASE | re.DOTALL
    )
    text = tiny_pattern.sub('', text)
    
    return text


def remove_decorative_images(text: str) -> str:
    """ç§»é™¤æå°çš„è£…é¥°æ€§å›¾ç‰‡ï¼ˆé€šå¸¸æ˜¯ OCR å™ªå£°ï¼‰

    æ£€æµ‹å°ºå¯¸å°äº 0.1in çš„å›¾ç‰‡ï¼ŒåŒ…æ‹¬ç§‘å­¦è®¡æ•°æ³•æ ¼å¼å¦‚:
    - 1.3888888888888888e-2in (çº¦ 0.014in)
    - 1e-3in (0.001in)
    - 0.01in, 0.001in (å¸¸è§„å°æ•°æ ¼å¼)
    """
    if not text:
        return text

    # ğŸ†• P1-003 ä¿®å¤ï¼šåŒ¹é…ç§‘å­¦è®¡æ•°æ³•æ ¼å¼çš„æå°å°ºå¯¸ï¼ˆe-2, e-3 æˆ–æ›´å°ï¼‰
    # æ”¯æŒæ–‡ä»¶é¦–è¡Œã€è¡Œä¸­ã€è¡Œå°¾çš„å›¾ç‰‡æ ‡è®°
    tiny_sci_pattern = re.compile(
        r'!\[[^\]]*\]\([^)]+\)\{[^}]*?(?:\d+\.?\d*e-[2-9]|\d+\.?\d*e-\d{2,})in[^}]*\}',
        re.IGNORECASE | re.DOTALL,
    )
    text = tiny_sci_pattern.sub('', text)

    # ğŸ†• P1-003 ä¿®å¤ï¼šåŒ¹é…å¸¸è§„å°æ•°æ ¼å¼çš„æå°å°ºå¯¸ï¼ˆ0.0å¼€å¤´ï¼‰
    tiny_decimal_pattern = re.compile(
        r'!\[[^\]]*\]\([^)]+\)\{[^}]*?0\.0\d+in[^}]*\}',
        re.IGNORECASE | re.DOTALL,
    )
    text = tiny_decimal_pattern.sub('', text)

    return text


def clean_residual_image_attrs(text: str) -> str:
    r"""æ¸…ç†æ®‹ç•™çš„å›¾ç‰‡å±æ€§å—

    ğŸ†• v1.7 å¢å¼ºï¼šæ¸…ç†æ›´å¤š Markdown å›¾ç‰‡å±æ€§æ®‹ç•™
    ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† Pandoc ç”Ÿæˆçš„å›¾ç‰‡å±æ€§
    """
    if not text:
        return text

    text = clean_image_attributes(text)

    # æ¸…ç†å•ç‹¬æˆè¡Œçš„å±æ€§å—å¼€å§‹
    text = re.sub(r'^\s*\{width="[^"]*"\s*$', '', text, flags=re.MULTILINE)
    # æ¸…ç†å•ç‹¬æˆè¡Œçš„å±æ€§å—ç»“æŸ
    text = re.sub(r'^\s*height="[^"]*"\}\s*$', '', text, flags=re.MULTILINE)

    # æ¸…ç†è·¨è¡Œçš„å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s*\n\s*height="[^"]*"\}', '', text, flags=re.MULTILINE)

    # æ¸…ç†å•è¡Œå®Œæ•´å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s+height="[^"]*"\}', '', text)

    # ğŸ†• v1.7ï¼šæ¸…ç†æ®‹ç•™çš„ height="..." å’Œ width="..." ï¼ˆå¸¦å¯èƒ½çš„å°¾éš }ï¼‰
    text = re.sub(r'height="[^"]*"[}]*', '', text)
    text = re.sub(r'width="[^"]*"[}]*', '', text)

    return text


# DEPRECATED: çŠ¶æ€æœºåä¸å†éœ€è¦å˜é‡è‡ªåŠ¨åŒ…è£¹ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦åŒ…è£¹
def wrap_math_variables(text: str) -> str:
    """æ™ºèƒ½åŒ…è£¹æ•°å­¦å˜é‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    # ä¿æŠ¤å·²æœ‰çš„æ•°å­¦æ¨¡å¼
    protected = []
    def save_math(match):
        protected.append(match.group(0))
        return f"@@MATH{len(protected)-1}@@"
    
    text = re.sub(r'\\\(.*?\\\)', save_math, text, flags=re.DOTALL)
    text = re.sub(r'\\\[.*?\\\]', save_math, text, flags=re.DOTALL)
    
    # ä¿æŠ¤ TikZ åæ ‡
    tikz_coords = []
    def save_tikz(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)$' æˆ– '$(0,1)$'
        inner = block[2:-2]
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZ{len(tikz_coords)-1}@@"
        else:
            return block
    text = re.sub(r'\$\([\d\w\s,+\-*/\.]+\)\$', save_tikz, text)
    
    # è§„åˆ™1ï¼šå•å­—æ¯å˜é‡ + è¿ç®—ç¬¦/ä¸‹æ ‡/ä¸Šæ ‡
    text = re.sub(
        r'\b([a-zA-Z])(?=\s*[=+\-*/^<>]|_{|\^{)',
        r'\\(\1\\)',
        text
    )
    
    # è§„åˆ™2ï¼šæ•°å­¦å‡½æ•°å¿…é¡»æœ‰åæ–œæ 
    math_functions = [
        'sin', 'cos', 'tan', 'cot', 'sec', 'csc',
        'arcsin', 'arccos', 'arctan',
        'sinh', 'cosh', 'tanh',
        'log', 'ln', 'lg', 'exp',
        'lim', 'sup', 'inf',
        'max', 'min', 'det', 'dim', 'ker'
    ]
    for func in math_functions:
        text = re.sub(rf'(?<!\\)\b{func}\b(?!\w)', rf'\\{func}', text)
    
    # è§„åˆ™3ï¼šè™šæ•°å•ä½ iï¼ˆé¿å…è¯¯è½¬æ¢ç½—é©¬æ•°å­—ï¼‰
    # åªåœ¨æ˜ç¡®çš„æ•°å­¦ä¸Šä¸‹æ–‡ä¸­è½¬æ¢ï¼Œé¿å… (i), (ii) ç­‰ç½—é©¬æ•°å­—è¢«è½¬æ¢
    # åŒ¹é…ï¼šç‹¬ç«‹çš„ i åé¢è·Ÿç€æ•°å­¦è¿ç®—ç¬¦æˆ–ç»“æŸï¼Œä½†ä¸åœ¨æ‹¬å·å†…
    text = re.sub(r'(?<!\\)(?<!\()\bi\b(?=[^a-zA-Z\)])', r'\\mathrm{i}', text)
    
    # æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, coord in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZ{i}@@", coord)
    for i, math in enumerate(protected):
        text = text.replace(f"@@MATH{i}@@", math)
    
    return text


def _fix_array_left_braces(block: str) -> str:
    r"""ğŸ†• v1.8.9ï¼šåœ¨æ•°å­¦å—å†…éƒ¨ï¼Œä¸ºå…¸å‹çš„ array/cases æ–¹ç¨‹ç»„å°è¯•è¡¥å…¨ç¼ºå¤±çš„ \left\{ï¼ˆéå¸¸ä¿å®ˆï¼‰
    
    å¯åŠ¨æ¡ä»¶ï¼ˆå¿…é¡»å…¨éƒ¨æ»¡è¶³ï¼‰ï¼š
    1. block ä¸­åŒ…å« \begin{array} æˆ– \begin{cases}
    2. block ä¸­åŒ…å« \right.ï¼ˆå³ä¾§å·²æœ‰å³è¾¹ç•Œï¼‰
    3. block ä¸­ \left çš„ä¸ªæ•°å°‘äº \right
    
    è¡¥å…¨è§„åˆ™ï¼ˆä¿å®ˆå¯å‘å¼ï¼‰ï¼š
    - å¯¹æ¯ä¸ª \begin{array} / \begin{cases}ï¼š
      - æ£€æŸ¥å…¶å‰æ–¹ 50 ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡çª—å£
      - å¦‚æœçª—å£å†…æ²¡æœ‰ \left æˆ– \{ï¼Œåˆ™æ’å…¥ \left\{
      - å¦‚æœçª—å£å†…å·²æœ‰å·¦è¾¹ç•Œï¼Œåˆ™ä¸æ’å…¥
    
    é£é™©æ§åˆ¶ï¼š
    - å®å¯ä¸ä¿®ï¼Œä¸è¦è¯¯ä¼¤
    - åªåœ¨é«˜ç½®ä¿¡åº¦åœºæ™¯ä¸‹è¡¥å…¨
    - ä¿ç•™åŸæœ‰é™çº§é€»è¾‘ä½œä¸ºå…œåº•
    """
    if not block:
        return block
    
    # å¯åŠ¨æ¡ä»¶æ£€æŸ¥ï¼ˆæ³¨æ„ï¼šblock ä¸­çš„åæ–œæ æ˜¯å•ä¸ª \ï¼Œä¸æ˜¯åŒåæ–œæ ï¼‰
    has_array_or_cases = '\\begin{array}' in block or '\\begin{cases}' in block
    has_right_dot = '\\right.' in block
    
    if not has_array_or_cases or not has_right_dot:
        return block
    
    # ç»Ÿè®¡ left/right æ•°é‡ï¼Œåªæœ‰ right åå¤šæ—¶æ‰è€ƒè™‘è¡¥
    left_count = len(re.findall(r'\\left\b', block))
    right_count = len(re.findall(r'\\right\b', block))
    
    if left_count >= right_count:
        return block  # ä¸ç¼º leftï¼Œä¸éœ€è¦è¡¥
    
    # å¯¹ \begin{array} å’Œ \begin{cases} å°è¯•è¡¥å…¨
    # ä½¿ç”¨å›è°ƒå‡½æ•°æ£€æŸ¥ä¸Šä¸‹æ–‡å¹¶å†³å®šæ˜¯å¦æ’å…¥
    def _insert_left_if_needed(m: re.Match) -> str:
        start = m.start()
        # å‘å‰çœ‹ 50 ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡çª—å£
        prefix = block[:start]
        context = prefix[-50:] if len(prefix) > 50 else prefix
        
        # å¦‚æœä¸Šä¸‹æ–‡ä¸­å·²ç»æœ‰ \left æˆ–æ˜¾å¼çš„å¤§æ‹¬å· \{ï¼Œåˆ™ä¸æ’å…¥
        if '\\left' in context or '\\{' in context:
            return m.group(0)  # ä¿æŒåŸæ ·
        
        # æ»¡è¶³æ¡ä»¶ï¼šåœ¨ begin å‰æ’å…¥ \left\{
        return r'\left\{' + m.group(0)
    
    # å…ˆå¤„ç† \begin{array}
    block = re.sub(r'\\begin\{array\}', _insert_left_if_needed, block)
    
    # å†å¤„ç† \begin{cases}
    block = re.sub(r'\\begin\{cases\}', _insert_left_if_needed, block)
    
    return block


def _sanitize_math_block(block: str) -> str:
    """ä¿®æ­£æ•°å­¦å—å†…éƒ¨çš„ OCR é”™è¯¯
    
    ä¿®å¤ï¼š
    - \\left / \\right ä¸åŒ¹é…æ—¶é™çº§ä¸ºæ™®é€šæ‹¬å·
    - \\right.\\ ) ç­‰ç•¸å½¢ç»„åˆ
    """
    if not block:
        return block
    
    # ç»Ÿä¸€æ•°å­¦ç¯å¢ƒå†…çš„ä¸­æ–‡æ ‡ç‚¹ä¸ºè‹±æ–‡æ ‡ç‚¹
    block = (block
             .replace('ï¼Œ', ',')
             .replace('ï¼š', ':')
             .replace('ï¼›', ';')
             .replace('ã€‚', '.')
             .replace('ã€', ','))

    # æ›¿æ¢å¸¸è§çš„ Unicode ç¬¦å·ä¸º LaTeX å‘½ä»¤ï¼ˆé¿å…ç¼ºå­—å½¢ï¼‰
    block = block.replace('âˆµ', r'\\because').replace('âˆ´', r'\\therefore')

    # å°†ä¸Šä¸‹æ ‡ä¸­çš„ä¸­æ–‡åŒ…è£…ä¸º \text{...}
    # å½¢å¼ä¸€ï¼š_[{...ä¸­æ–‡...}] æˆ– ^[{...ä¸­æ–‡...}]
    def _wrap_cjk_in_braced_subsup(m: re.Match) -> str:
        lead = m.group(1)
        inner = m.group(2)
        if '\\text{' in inner:
            return f"{lead}{{{inner}}}"
        return f"{lead}{{\\text{{{inner}}}}}"
    block = re.sub(r'([_^])\{([^{}]*?[\u4e00-\u9fff]+[^{}]*?)\}', _wrap_cjk_in_braced_subsup, block)

    # å½¢å¼äºŒï¼šå•å­—ç¬¦ä¸Šä¸‹æ ‡ï¼š_æ°´ æˆ– ^é«˜
    block = re.sub(r'([_^])([\u4e00-\u9fff])', r'\1{\\text{\2}}', block)

    # æ•°å­¦å†…å¸¸è§ä¸­æ–‡è¿æ¥è¯ï¼Œæ›¿æ¢ä¸º \text{...}ï¼ˆä¿å®ˆé›†ï¼‰
    for w in ['ä¸”', 'æˆ–', 'åˆ™', 'å³', 'æ•…', 'æ‰€ä»¥', 'å› ä¸º']:
        block = re.sub(fr'(?<!\\text\{{){re.escape(w)}(?![^\{{]*\}})', rf'\\text{{{w}}}', block)
    
    # ğŸ†• v1.8.9ï¼šåœ¨ç»Ÿè®¡ left/right ä¹‹å‰ï¼Œå…ˆå°è¯•ä¿®å¤å…¸å‹ array/cases æ–¹ç¨‹ç»„
    # ä¸ºç¼ºå¤± \\left\\{ çš„æ–¹ç¨‹ç»„è¡¥å…¨å·¦å¤§æ‹¬å·ï¼Œé¿å…åç»­é™çº§å¤„ç†
    block = _fix_array_left_braces(block)
    
    # ç»Ÿè®¡ left/right æ•°é‡
    left_count = len(re.findall(r'\\left\b', block))
    right_count = len(re.findall(r'\\right\b', block))
    
    # ä¿®å¤ç•¸å½¢ \right.\ ) å’Œ \right.\\)
    block = re.sub(r'\\right\.\s*\\\s*\)', r'\\right.', block)
    # ä¿®å¤ \right.\\\) æ¨¡å¼ï¼ˆarrayç»“å°¾çš„å¸¸è§OCRé”™è¯¯ï¼‰
    block = re.sub(r'\\right\.\\\\+\)', r'\\right.', block)
    
    # å¦‚æœ left/right ä¸åŒ¹é…ï¼Œé™çº§ä¸ºæ™®é€šæ‹¬å·
    if left_count != right_count:
        block = re.sub(r'\\left\s*([\(\[\{])', r'\1', block)
        block = re.sub(r'\\right\s*([\)\]\}])', r'\1', block)
        block = re.sub(r'\\left\.', '', block)
        block = re.sub(r'\\right\.', '', block)
    
    return block


# DEPRECATED: çŠ¶æ€æœºå·²å¤„ç†æ•°å­¦å®šç•Œç¬¦ä¸ OCR è¾¹ç•Œï¼Œæ­¤å‡½æ•°ä»…ä¿ç•™å…¼å®¹æ€§
def sanitize_math(text: str) -> str:
    """æ‰«æå…¨æ–‡ï¼Œä»…ä¿®æ­£æ•°å­¦ç¯å¢ƒå†…çš„ OCR é”™è¯¯
    
    åªå¤„ç† \\(...\\) å’Œ \\[...\\] å†…éƒ¨çš„å†…å®¹ã€‚
    """
    if not text:
        return text
    
    result = []
    i = 0
    n = len(text)
    
    while i < n:
        # åŒ¹é… \(..\)
        if text.startswith(r"\(", i):
            j = text.find(r"\)", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\(" + inner + r"\)")
            i = j + 2
            continue
        
        # åŒ¹é… \[..\]
        if text.startswith(r"\[", i):
            j = text.find(r"\]", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\[" + inner + r"\]")
            i = j + 2
            continue
        
        result.append(text[i])
        i += 1
    
    return "".join(result)


def remove_blank_lines_in_macro_args(text: str) -> str:
    """åˆ é™¤å®å‚æ•°ä¸­çš„ç©ºè¡Œï¼ˆå¢å¼ºç‰ˆï¼‰"""
    macros = ['explain', 'topics', 'answer', 'difficulty', 'source']
    
    for macro in macros:
        pattern = rf'(\\{macro}\{{)([^{{}}]*(?:\{{[^{{}}]*\}}[^{{}}]*)*?)(\}})'
        
        def clean_arg(match):
            prefix = match.group(1)
            arg = match.group(2)
            suffix = match.group(3)
            
            # æ”¹è¿›1ï¼šåˆ é™¤è¿ç»­ç©ºè¡Œ
            arg = re.sub(r'\n\s*\n+', '\n', arg)
            
            # æ”¹è¿›2ï¼šåˆ é™¤æ®µé¦–æ®µå°¾ç©ºè¡Œ
            arg = arg.strip()
            
            # æ”¹è¿›3ï¼šæ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼ˆä¿ç•™ç¼©è¿›ï¼‰
            lines = arg.split('\n')
            lines = [line.rstrip() for line in lines]
            arg = '\n'.join(lines)
            
            return prefix + arg + suffix
        
        text = re.sub(pattern, clean_arg, text, flags=re.DOTALL)
    
    return text


def clean_question_environments(text: str) -> str:
    """æ¸…ç† question ç¯å¢ƒå†…éƒ¨çš„å¤šä½™ç©ºè¡Œï¼Œå¹¶æ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®"""
    pattern = r'(\\begin\{question\})(.*?)(\\end\{question\})'

    def clean_env(match):
        begin = match.group(1)
        content = match.group(2)
        end = match.group(3)

        # åˆ é™¤è¿ç»­çš„3ä¸ªä»¥ä¸Šæ¢è¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)

        # ğŸ†• v1.8: æ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®ï¼ˆç›´æ¥ä» \item å¼€å§‹ï¼‰
        # å»é™¤å‰å¯¼ç©ºç™½åæ£€æŸ¥æ˜¯å¦ä»¥ \item å¼€å¤´
        content_stripped = content.lstrip()
        if content_stripped.startswith('\\item'):
            # åœ¨ \begin{question} åæ’å…¥ TODO æ³¨é‡Š
            warning = '\n% âš ï¸ TODO: è¡¥å……é¢˜å¹² - æ­¤é¢˜ç›´æ¥ä» \\item å¼€å§‹ï¼Œè¯·åœ¨ä¸Šæ–¹æ·»åŠ é¢˜ç›®ä¸»å¹²æè¿°\n'
            content = warning + content

        return begin + content + end

    return re.sub(pattern, clean_env, text, flags=re.DOTALL)


def split_long_lines_in_explain(text: str, max_length: int = 800) -> str:
    """åœ¨ explain{} ä¸­è‡ªåŠ¨åˆ†å‰²è¶…é•¿è¡Œ"""
    pattern = r'(\\explain\{)([^{}]*(?:\{[^{}]*\}[^{}]*)*?)(\})'

    def split_content(match):
        prefix = match.group(1)
        content = match.group(2)
        suffix = match.group(3)

        lines = content.split('\n')
        new_lines = []

        for line in lines:
            if len(line) <= max_length:
                new_lines.append(line)
            else:
                # åœ¨æ ‡ç‚¹ååˆ†å‰²
                segments = re.split(r'([ï¼Œã€‚ï¼›ï¼ï¼Ÿ])', line)
                current = ""
                for seg in segments:
                    if len(current + seg) > max_length and current:
                        new_lines.append(current.rstrip())
                        current = seg
                    else:
                        current += seg
                if current:
                    new_lines.append(current.rstrip())

        return prefix + '\n'.join(new_lines) + suffix

    return re.sub(pattern, split_content, text, flags=re.DOTALL)


def fix_missing_items_in_enumerate(tex: str) -> str:
    """ğŸ†• ä»»åŠ¡1ï¼šåœ¨ enumerate ç¯å¢ƒä¸­è‡ªåŠ¨è¡¥å……ç¼ºå¤±çš„ \\item

    åŠŸèƒ½ï¼šæ‰«æ TeX æ–‡æœ¬ï¼Œæ£€æµ‹ \\begin{enumerate} åˆ° \\end{enumerate} ä¹‹é—´çš„å†…å®¹ï¼Œ
    åœ¨æšä¸¾ç¯å¢ƒå†…è‡ªåŠ¨ä¸ºéç©ºè¡Œï¼ˆéæ³¨é‡Šè¡Œã€é \\item è¡Œï¼‰æ·»åŠ  \\item å‰ç¼€ã€‚

    é€»è¾‘ï¼š
    - ç©ºè¡Œï¼šä¿ç•™
    - æ³¨é‡Šè¡Œï¼ˆä»¥ % å¼€å¤´ï¼‰ï¼šä¿ç•™
    - ä»¥ \\item å¼€å¤´çš„è¡Œï¼šä¿ç•™
    - å…¶ä»–éç©ºè¡Œï¼šåœ¨è¡Œé¦–è‡ªåŠ¨æ·»åŠ  \\itemï¼ˆä¿æŒåŸæœ‰ç¼©è¿›ï¼‰

    Args:
        tex: å®Œæ•´çš„ TeX æ–‡æœ¬

    Returns:
        ä¿®å¤åçš„ TeX æ–‡æœ¬
    """
    if not tex:
        return tex

    result = []
    i = 0
    lines = tex.split('\n')
    n = len(lines)

    while i < n:
        line = lines[i]

        # æ£€æµ‹ enumerate ç¯å¢ƒå¼€å§‹
        if r'\begin{enumerate}' in line:
            result.append(line)
            i += 1

            # å¤„ç† enumerate ç¯å¢ƒå†…çš„å†…å®¹
            depth = 1
            while i < n and depth > 0:
                current_line = lines[i]

                # æ£€æµ‹åµŒå¥—çš„ enumerate ç¯å¢ƒ
                if r'\begin{enumerate}' in current_line:
                    depth += 1
                    result.append(current_line)
                    i += 1
                    continue
                elif r'\end{enumerate}' in current_line:
                    depth -= 1
                    result.append(current_line)
                    i += 1
                    if depth == 0:
                        break
                    continue

                stripped = current_line.strip()

                # è§„åˆ™1ï¼šç©ºè¡Œ - ä¿ç•™
                if not stripped:
                    result.append(current_line)
                    i += 1
                    continue

                # è§„åˆ™2ï¼šæ³¨é‡Šè¡Œï¼ˆä»¥ % å¼€å¤´ï¼‰- ä¿ç•™
                if stripped.startswith('%'):
                    result.append(current_line)
                    i += 1
                    continue

                # è§„åˆ™3ï¼šå·²æœ‰ \item çš„è¡Œ - ä¿ç•™
                if stripped.startswith(r'\item'):
                    result.append(current_line)
                    i += 1
                    continue

                # è§„åˆ™4ï¼šå…¶ä»–éç©ºè¡Œ - æ·»åŠ  \item
                # ä¿æŒåŸæœ‰ç¼©è¿›
                leading_spaces = len(current_line) - len(current_line.lstrip())
                indent = current_line[:leading_spaces]
                content = current_line[leading_spaces:]
                result.append(f"{indent}\\item {content}")
                i += 1
        else:
            result.append(line)
            i += 1

    return '\n'.join(result)


def soft_wrap_paragraph(s: str, limit: int = 80) -> str:
    """ğŸ†• ä»»åŠ¡2ï¼šä¸ºé•¿æ®µè½åœ¨æ ‡ç‚¹å¤„æ·»åŠ è½¯æ¢è¡Œï¼Œä¾¿äº LaTeX æŠ¥é”™å®šä½

    åŠŸèƒ½ï¼šå¯¹äºè¶…è¿‡æŒ‡å®šé•¿åº¦çš„å­—ç¬¦ä¸²ï¼Œåœ¨åˆé€‚çš„æ ‡ç‚¹ä½ç½®æ’å…¥æ¢è¡Œç¬¦ï¼Œ
    ä½¿å¾—æ¯è¡Œé•¿åº¦ä¸è¶…è¿‡ limitï¼Œä¾¿äº LaTeX ç¼–è¯‘æ—¶å¿«é€Ÿå®šä½é”™è¯¯è¡Œã€‚

    é€»è¾‘ï¼š
    - å¦‚æœå­—ç¬¦ä¸²é•¿åº¦ < limitï¼Œç›´æ¥è¿”å›
    - å¦‚æœè¾ƒé•¿ï¼š
      - ä»å¤´æ‰«æï¼Œè®°å½•æœ€è¿‘çš„"å¯æ‹†åˆ†æ ‡ç‚¹"ä½ç½®ï¼ˆã€‚ï¼›ï¼Ÿï¼ï¼Œï¼‰
      - å½“å½“å‰è¡Œé•¿åº¦è¶…è¿‡ limit/2 æ—¶ï¼Œåœ¨æœ€è¿‘æ ‡ç‚¹åæ’å…¥æ¢è¡Œ
      - é¿å…åœ¨ LaTeX å‘½ä»¤å†…éƒ¨æ‹†è¡Œï¼ˆé‡åˆ° \\ å¼€å¤´çš„ token æ—¶ä¸æ‹†ï¼‰

    Args:
        s: è¾“å…¥å­—ç¬¦ä¸²
        limit: æ¯è¡Œæœ€å¤§é•¿åº¦é™åˆ¶ï¼ˆé»˜è®¤ 80ï¼‰

    Returns:
        æ·»åŠ è½¯æ¢è¡Œåçš„å­—ç¬¦ä¸²
    """
    if not s or len(s) < limit:
        return s

    # å¯æ‹†åˆ†çš„ä¸­æ–‡æ ‡ç‚¹
    breakable_puncts = set('ã€‚ï¼›ï¼Ÿï¼ï¼Œ')

    result = []
    current_line = []
    current_length = 0
    last_punct_pos = -1  # è®°å½•å½“å‰è¡Œä¸­æœ€è¿‘çš„æ ‡ç‚¹ä½ç½®

    i = 0
    while i < len(s):
        char = s[i]

        # æ£€æµ‹ LaTeX å‘½ä»¤ï¼ˆä»¥ \ å¼€å¤´ï¼‰
        if char == '\\' and i + 1 < len(s):
            # æ”¶é›†å®Œæ•´çš„ LaTeX å‘½ä»¤
            cmd_start = i
            i += 1
            # è·³è¿‡å‘½ä»¤åï¼ˆå­—æ¯ï¼‰
            while i < len(s) and s[i].isalpha():
                i += 1
            # è·³è¿‡å¯èƒ½çš„å‚æ•°ï¼ˆèŠ±æ‹¬å·ï¼‰
            if i < len(s) and s[i] == '{':
                brace_depth = 1
                i += 1
                while i < len(s) and brace_depth > 0:
                    if s[i] == '{':
                        brace_depth += 1
                    elif s[i] == '}':
                        brace_depth -= 1
                    i += 1

            # å°†æ•´ä¸ªå‘½ä»¤ä½œä¸ºä¸€ä¸ªå•å…ƒæ·»åŠ 
            cmd = s[cmd_start:i]
            current_line.append(cmd)
            current_length += len(cmd)
            continue

        # æ£€æµ‹æ¢è¡Œç¬¦ - ä¿ç•™åŸæœ‰æ¢è¡Œ
        if char == '\n':
            result.append(''.join(current_line))
            result.append('\n')
            current_line = []
            current_length = 0
            last_punct_pos = -1
            i += 1
            continue

        # æ™®é€šå­—ç¬¦
        current_line.append(char)
        current_length += 1

        # è®°å½•å¯æ‹†åˆ†æ ‡ç‚¹çš„ä½ç½®
        if char in breakable_puncts:
            last_punct_pos = len(current_line) - 1

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢è¡Œ
        if current_length > limit // 2 and last_punct_pos >= 0:
            # åœ¨æœ€è¿‘çš„æ ‡ç‚¹åæ¢è¡Œ
            before_break = ''.join(current_line[:last_punct_pos + 1])
            after_break = current_line[last_punct_pos + 1:]

            result.append(before_break)
            result.append('\n')

            current_line = after_break
            current_length = len(after_break)
            last_punct_pos = -1

        i += 1

    # æ·»åŠ å‰©ä½™å†…å®¹
    if current_line:
        result.append(''.join(current_line))

    return ''.join(result)


def remove_par_breaks_in_explain(text: str) -> str:
    r"""ç§»é™¤ \explain{...} ä¸­çš„ç©ºæ®µè½ï¼ˆæ”¹è¿›ç‰ˆï¼šæ­£ç¡®å¤„ç†åµŒå¥—æ‹¬å·ï¼‰

    ğŸ†• v1.8.2ï¼šå®Œå…¨é‡å†™ï¼Œä¿®å¤æ‹¬å·è®¡æ•°é”™è¯¯
    - æ­£ç¡®å¤„ç† \{ \} è½¬ä¹‰æ‹¬å·ï¼ˆä¸è®¡å…¥ depthï¼‰
    - æ­£ç¡®å¤„ç†åæ–œæ è½¬ä¹‰ï¼ˆ\\ åçš„å­—ç¬¦ä¸å¤„ç†ï¼‰
    - å°†ç©ºæ®µè½æ›¿æ¢ä¸º % æ³¨é‡Šè€Œé \parï¼ˆæ›´å®‰å…¨ï¼‰
    """
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    out = []
    i = 0
    n = len(text)

    while i < n:
        if text.startswith("\\explain{", i):
            out.append("\\explain{")
            i += len("\\explain{")
            depth = 1
            buf = []

            while i < n and depth > 0:
                # å¤„ç†åæ–œæ è½¬ä¹‰åºåˆ—
                if text[i] == '\\' and i + 1 < n:
                    next_char = text[i + 1]
                    # \{ å’Œ \} ä¸è®¡å…¥æ‹¬å·æ·±åº¦
                    if next_char in '{}':
                        buf.append(text[i:i+2])
                        i += 2
                        continue
                    # å…¶ä»–åæ–œæ åºåˆ—ï¼ˆå¦‚ \\, \left, \right ç­‰ï¼‰ç›´æ¥å¤åˆ¶
                    buf.append(text[i])
                    i += 1
                    continue

                # æ£€æµ‹ç©ºæ®µè½ï¼ˆè¿ç»­ä¸¤ä¸ªæ¢è¡Œï¼Œä¸­é—´åªæœ‰ç©ºç™½ï¼‰
                if text[i] == '\n':
                    j = i + 1
                    while j < n and text[j] in ' \t':
                        j += 1
                    if j < n and text[j] == '\n':
                        # ç©ºæ®µè½ï¼šæ›¿æ¢ä¸ºæ³¨é‡Šè¡Œ
                        buf.append('\n%\n')
                        i = j + 1
                        continue

                # æ™®é€šå¤§æ‹¬å·è®¡æ•°
                if text[i] == '{':
                    depth += 1
                    buf.append(text[i])
                    i += 1
                elif text[i] == '}':
                    depth -= 1
                    if depth == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„é—­æ‹¬å·
                        out.append(''.join(buf))
                        out.append('}')
                        i += 1
                        break
                    buf.append(text[i])
                    i += 1
                else:
                    buf.append(text[i])
                    i += 1

            # å¦‚æœå¾ªç¯ç»“æŸä½† depth > 0ï¼Œè¯´æ˜æ‹¬å·ä¸åŒ¹é…ï¼ˆä¿ç•™åŸå†…å®¹ï¼‰
            if depth > 0:
                out.append(''.join(buf))
        else:
            out.append(text[i])
            i += 1

    return ''.join(out)


def _is_likely_stem(first_item: str, all_lines: list, item_indices: list, section_type: str = "") -> bool:
    """ğŸ†• v1.8.5ï¼šåˆ¤æ–­ç¬¬ä¸€ä¸ª \\item æ˜¯å¦å¯èƒ½æ˜¯é¢˜å¹²ï¼ˆå¢å¼ºç‰ˆï¼‰

    å¯å‘å¼è§„åˆ™ï¼š
        1. é¢˜å‹åˆ¤æ–­ï¼šè§£ç­”é¢˜æ›´å¯èƒ½æœ‰é¢˜å¹²ï¼Œé€‰æ‹©é¢˜å¯èƒ½ç›´æ¥æ˜¯å°é—®
        2. é•¿åº¦æ£€æŸ¥ï¼šæ ¹æ®é¢˜å‹åŠ¨æ€è°ƒæ•´é˜ˆå€¼
        3. å…³é”®è¯æ£€æŸ¥ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«"å·²çŸ¥"ã€"è®¾"ã€"å¦‚å›¾"ç­‰é¢˜å¹²å…³é”®è¯
        4. å°é—®æ ‡è®°æ£€æŸ¥ï¼šç¬¬ä¸€è¡Œä¸åŒ…å«å¸¸è§å°é—®æ ‡è®°ï¼ˆâ‘ â‘¡â‘¢ã€(1)(2)ç­‰ï¼‰
        5. åç»­æ£€æŸ¥ï¼šåç»­ \\item åŒ…å«å°é—®æ ‡è®°

    Args:
        first_item: ç¬¬ä¸€ä¸ª \\item è¡Œçš„å†…å®¹
        all_lines: question ç¯å¢ƒå†…çš„æ‰€æœ‰è¡Œ
        item_indices: æ‰€æœ‰ \\item çš„è¡Œç´¢å¼•
        section_type: é¢˜å‹ï¼ˆå¦‚ "è§£ç­”é¢˜"ã€"å•é€‰é¢˜"ã€"å¤šé€‰é¢˜"ã€"å¡«ç©ºé¢˜"ï¼‰

    Returns:
        True å¦‚æœå¯èƒ½æ˜¯é¢˜å¹²ï¼ŒFalse å¦‚æœå¯èƒ½æ˜¯å°é—®
    """
    # æå–ç¬¬ä¸€ä¸ª \\item çš„çº¯æ–‡æœ¬å†…å®¹
    stem_text = re.sub(r'^(\s*)\\item\s*', '', first_item).strip()

    # è§„åˆ™1ï¼šé¢˜å‹åˆ¤æ–­ - åŠ¨æ€è°ƒæ•´é˜ˆå€¼å’Œå…³é”®è¯
    if section_type == "è§£ç­”é¢˜":
        # è§£ç­”é¢˜é€šå¸¸æœ‰é¢˜å¹²
        min_length = 15
        stem_keywords = ['å·²çŸ¥', 'è®¾', 'å¦‚å›¾', 'è¯æ˜', 'æ±‚', 'è®¡ç®—', 'è‹¥', 'å‡è®¾', 'åœ¨']
    elif section_type in ["å•é€‰é¢˜", "å¤šé€‰é¢˜"]:
        # é€‰æ‹©é¢˜å¯èƒ½ç›´æ¥æ˜¯å°é—®
        min_length = 30
        stem_keywords = ['å·²çŸ¥', 'è®¾', 'å¦‚å›¾', 'è‹¥', 'å‡è®¾', 'ä¸‹åˆ—', 'å…³äº', 'åœ¨']
    else:
        # å¡«ç©ºé¢˜æˆ–æœªçŸ¥ç±»å‹
        min_length = 20
        stem_keywords = ['å·²çŸ¥', 'è®¾', 'å¦‚å›¾', 'è‹¥', 'å‡è®¾', 'åœ¨']

    # è§„åˆ™2ï¼šé•¿åº¦æ£€æŸ¥ï¼ˆå»æ‰LaTeXå‘½ä»¤åï¼‰
    # å»æ‰æ•°å­¦æ¨¡å¼å’Œå¸¸è§LaTeXå‘½ä»¤æ¥ä¼°ç®—æ–‡æœ¬é•¿åº¦
    clean_text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', stem_text)
    clean_text = re.sub(r'\\[()\[\]]', '', clean_text)

    if len(clean_text) < min_length:
        # å¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯é¢˜å¹²
        return False

    # è§„åˆ™3ï¼šå…³é”®è¯æ£€æŸ¥
    has_stem_keyword = any(kw in stem_text for kw in stem_keywords)

    # è§„åˆ™4ï¼šæ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«å°é—®æ ‡è®°ï¼ˆæ’é™¤æ³•ï¼‰
    subq_markers = [
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # åœ†åœˆæ•°å­—
        r'\(\d+\)',            # (1) (2)
        r'^\d+[\.ã€]',         # 1. 1ã€
        r'^[â… -â…©][\.ã€]',      # â… . â…¡.
    ]

    has_subq_marker = False
    for pattern in subq_markers:
        if re.search(pattern, stem_text[:50]):  # åªæ£€æŸ¥å‰50ä¸ªå­—ç¬¦
            # ç¬¬ä¸€è¡Œæœ‰å°é—®æ ‡è®°ï¼Œå¯èƒ½ä¸æ˜¯é¢˜å¹²
            has_subq_marker = True
            break

    if has_subq_marker:
        return False

    # è§„åˆ™5ï¼šæ£€æŸ¥åç»­ \\item æ˜¯å¦åŒ…å«å°é—®æ ‡è®°
    # å¦‚æœåç»­æœ‰æ ‡è®°ï¼Œè¯´æ˜å½“å‰è¿™ä¸ªå¯èƒ½æ˜¯é¢˜å¹²
    next_items_have_markers = False
    if len(item_indices) >= 2:
        # æ£€æŸ¥ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ª \\item
        for idx in item_indices[1:min(3, len(item_indices))]:
            if idx < len(all_lines):
                next_item = all_lines[idx]
                for pattern in subq_markers:
                    if re.search(pattern, next_item):
                        # åç»­æœ‰å°é—®æ ‡è®°ï¼Œå½“å‰å¯èƒ½æ˜¯é¢˜å¹²
                        next_items_have_markers = True
                        break
                if next_items_have_markers:
                    break

    if next_items_have_markers:
        return True

    # è§„åˆ™6ï¼šç»¼åˆåˆ¤æ–­
    if section_type == "è§£ç­”é¢˜":
        # è§£ç­”é¢˜ï¼šæœ‰å…³é”®è¯æˆ–é•¿åº¦è¶³å¤Ÿ â†’ é¢˜å¹²
        return has_stem_keyword or len(clean_text) > 30
    else:
        # å…¶ä»–é¢˜å‹ï¼šå¿…é¡»æœ‰å…³é”®è¯ä¸”é•¿åº¦è¶³å¤Ÿ â†’ é¢˜å¹²
        return has_stem_keyword and len(clean_text) > min_length


def fix_merged_questions_structure(content: str) -> str:
    """ğŸ†• v1.8.4ï¼šä¿®å¤åˆå¹¶é¢˜ç›®çš„ç»“æ„é—®é¢˜ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    é—®é¢˜åœºæ™¯ï¼š
        å½“åŒä¸€é¢˜å·çš„å¤šä¸ªéƒ¨åˆ†è¢«åˆå¹¶åï¼Œæ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¾ç¤ºä¸º \\itemï¼Œ
        ä½†æ­£ç¡®ç»“æ„åº”è¯¥æ˜¯ï¼šç¬¬ä¸€éƒ¨åˆ†=é¢˜å¹²ï¼Œåç»­éƒ¨åˆ†=å°é—®
    
    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼ˆé”™è¯¯ï¼‰ï¼š
            \\begin{question}
            \\item ç”²ã€ä¹™ä¸¤äººç»„é˜Ÿå‚åŠ æŒ‘æˆ˜...  ï¼ˆåº”è¯¥æ˜¯é¢˜å¹²ï¼‰
            \\item å·²çŸ¥ç”²å…ˆä¸Šåœº...              ï¼ˆè¿™æ‰æ˜¯å°é—®1ï¼‰
            \\item å¦‚æœnå…³éƒ½æŒ‘æˆ˜æˆåŠŸ...         ï¼ˆè¿™æ˜¯å°é—®2ï¼‰
            \\end{question}
        
        è¾“å‡ºï¼ˆæ­£ç¡®ï¼‰ï¼š
            \\begin{question}
            ç”²ã€ä¹™ä¸¤äººç»„é˜Ÿå‚åŠ æŒ‘æˆ˜...  ï¼ˆé¢˜å¹²ï¼‰
            
            \\begin{enumerate}[label=(\\arabic*)]
            \\item å·²çŸ¥ç”²å…ˆä¸Šåœº...      ï¼ˆå°é—®1ï¼‰
            \\item å¦‚æœnå…³éƒ½æŒ‘æˆ˜æˆåŠŸ... ï¼ˆå°é—®2ï¼‰
            \\end{enumerate}
            \\end{question}
    
    ğŸ†• v1.8.4 å¢å¼ºæ£€æµ‹é€»è¾‘ï¼š
        1. æ‰¾åˆ° \\begin{question} åç¬¬ä¸€ä¸ª \\item
        2. æ£€æŸ¥ç¬¬ä¸€ä¸ª \\item æ˜¯å¦ä¸ºé¢˜å¹²ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰ï¼š
           - å­—æ•°è¾ƒå¤šï¼ˆ>20å­—ç¬¦ï¼‰ä¸”ä¸åŒ…å«å°é—®æ ‡è®°ï¼ˆâ‘ â‘¡â‘¢ã€(1)(2)ç­‰ï¼‰
           - åç»­æœ‰å…¶ä»– \\item ä¸”åŒ…å«å°é—®æ ‡è®°
        3. å¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œå°†ç¬¬ä¸€ä¸ª \\item æå–ä¸ºé¢˜å¹²ï¼Œå…¶ä½™åŒ…è£¹åœ¨ enumerate ä¸­
    
    Args:
        content: å®Œæ•´çš„ LaTeX å†…å®¹
    
    Returns:
        ä¿®å¤åçš„ LaTeX å†…å®¹
    """
    lines = content.split('\n')
    result = []
    i = 0
    
    while i < len(lines):
        line = lines[i]
        
        # æ£€æµ‹ question ç¯å¢ƒå¼€å§‹
        if r'\begin{question}' in line:
            result.append(line)
            i += 1
            
            # æ”¶é›† question ç¯å¢ƒå†…çš„æ‰€æœ‰è¡Œ
            question_lines = []
            question_start = i
            depth = 1
            
            while i < len(lines) and depth > 0:
                current_line = lines[i]
                if r'\begin{question}' in current_line:
                    depth += 1
                elif r'\end{question}' in current_line:
                    depth -= 1
                    if depth == 0:
                        break
                question_lines.append(current_line)
                i += 1
            
            # åˆ†æ question å†…å®¹
            item_indices = []
            for idx, qline in enumerate(question_lines):
                if r'\item' in qline and not qline.strip().startswith('%'):
                    item_indices.append(idx)
            
            # å¦‚æœæœ‰å¤šä¸ª \itemï¼Œéœ€è¦ä¿®å¤ç»“æ„
            if len(item_indices) >= 2:
                # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å« enumerateï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
                has_enumerate = any(r'\begin{enumerate}' in qline for qline in question_lines)
                
                if not has_enumerate:
                    # æå–ç¬¬ä¸€ä¸ª \item ä½œä¸ºé¢˜å¹²
                    first_item_idx = item_indices[0]
                    stem_line = question_lines[first_item_idx]

                    # ğŸ†• v1.8.5ï¼šæ¨æ–­é¢˜å‹ï¼ˆä»å‰é¢çš„ \section å‘½ä»¤ï¼‰
                    section_type = ""
                    # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„ \section å‘½ä»¤
                    for prev_line in reversed(result[-50:]):  # æ£€æŸ¥å‰50è¡Œ
                        if r'\section{' in prev_line:
                            # æå– section åç§°
                            match = re.search(r'\\section\{([^}]+)\}', prev_line)
                            if match:
                                section_type = match.group(1)
                                break

                    # ğŸ†• v1.8.5ï¼šå¢å¼ºé¢˜å¹²è¯†åˆ« - æ£€æŸ¥ç¬¬ä¸€ä¸ª \item æ˜¯å¦çœŸçš„æ˜¯é¢˜å¹²
                    is_likely_stem = _is_likely_stem(stem_line, question_lines, item_indices, section_type)
                    
                    # å¦‚æœç¬¬ä¸€ä¸ª \item ä¸åƒé¢˜å¹²ï¼ˆä¾‹å¦‚ç›´æ¥æ˜¯å°é—®ï¼‰ï¼Œè·³è¿‡ä¿®å¤
                    if not is_likely_stem:
                        result.extend(question_lines)
                        if i < len(lines):
                            result.append(lines[i])
                            i += 1
                        continue
                    
                    # å»æ‰ \item å‰ç¼€å¾—åˆ°é¢˜å¹²
                    stem_content = re.sub(r'^(\s*)\\item\s*', r'\1', stem_line)
                    
                    # æ„å»ºæ–°çš„ question å†…å®¹
                    new_question_lines = []
                    
                    # æ·»åŠ é¢˜å¹²ä¹‹å‰çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                    new_question_lines.extend(question_lines[:first_item_idx])
                    
                    # æ·»åŠ é¢˜å¹²
                    new_question_lines.append(stem_content)
                    new_question_lines.append('')  # ç©ºè¡Œåˆ†éš”
                    
                    # æ·»åŠ  enumerate ç¯å¢ƒåŒ…è£¹å‰©ä½™çš„ \item
                    new_question_lines.append(r'\begin{enumerate}[label=(\arabic*)]')
                    
                    # æ·»åŠ å‰©ä½™çš„ \itemï¼ˆä»ç¬¬äºŒä¸ª \item å¼€å§‹ï¼‰
                    new_question_lines.extend(question_lines[first_item_idx + 1:])
                    
                    # æ·»åŠ  enumerate ç»“æŸæ ‡è®°
                    new_question_lines.append(r'\end{enumerate}')
                    
                    result.extend(new_question_lines)
                else:
                    # å·²æœ‰ enumerateï¼Œä¿æŒåŸæ ·
                    result.extend(question_lines)
            else:
                # åªæœ‰ 0 æˆ– 1 ä¸ª \itemï¼Œä¿æŒåŸæ ·
                result.extend(question_lines)
            
            # æ·»åŠ  \end{question}
            if i < len(lines):
                result.append(lines[i])
                i += 1
        else:
            result.append(line)
            i += 1
    
    return '\n'.join(result)


def fix_broken_set_definitions(text: str) -> str:
    r"""ä¿®å¤è¢« $$ æˆªæ–­çš„é›†åˆå®šä¹‰ (P0-001)
    
    æ£€æµ‹æ¨¡å¼ï¼š\right.\ $$ä¸­æ–‡$$\left. \
    æ›¿æ¢ä¸ºï¼š\right.\text{ä¸­æ–‡}\left.
    
    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š\right.\ $$æ˜¯è´¨æ•°$$\left. \
        è¾“å‡ºï¼š\right.\text{ æ˜¯è´¨æ•° }\left.
    """
    import re
    
    if not text:
        return text
    
    # æ¨¡å¼1: \right.\ $$ä¸­æ–‡$$\left. \ï¼ˆé›†åˆæ¡ä»¶è¢«æˆªæ–­ï¼‰
    pattern1 = re.compile(
        r'(\\right\.)\s*\\\s*\$\$([^$]+)\$\$\s*\\left\.\s*\\',
        re.DOTALL
    )
    text = pattern1.sub(r'\1\\text{\2}\\left.', text)
    
    # æ¨¡å¼2: \right.\ $$ä¸­æ–‡ï¼ˆè¡Œå°¾æˆªæ–­ï¼‰
    # ğŸ”§ v1.9.5ï¼šå¢åŠ é™åˆ¶æ¡ä»¶ï¼Œé¿å…åŒ¹é… \right.\ $$ï¼Œ\n è¿™ç§æ ‡ç‚¹åæ¢è¡Œçš„æ¨¡å¼
    # è¦æ±‚ä¸­æ–‡å†…å®¹è‡³å°‘åŒ…å«ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œè€Œä¸ä»…ä»…æ˜¯æ ‡ç‚¹
    pattern2 = re.compile(
        r'(\\right\.)\s*\\\s*\$\$([\u4e00-\u9fff][^$]*)$',
        re.MULTILINE
    )
    text = pattern2.sub(r'\1\\text{\2}', text)
    
    # æ¨¡å¼3: $$æˆ–$$\left. \ï¼ˆ"æˆ–"å­—è¢«åˆ†ç¦»ï¼‰
    pattern3 = re.compile(
        r'\$\$(æˆ–|ä¸”|å’Œ|å³)\$\$\\left\.\s*\\',
        re.DOTALL
    )
    text = pattern3.sub(r'\\text{ \1 }\\left.', text)
    
    return text


def fix_ocr_specific_errors(text: str) -> str:
    r"""ä¿®å¤ OCR ç‰¹æœ‰çš„è¯†åˆ«é”™è¯¯ (P1-005)
    
    å¤„ç†ï¼š
    1. ç§»é™¤ \boxed{}ï¼Œä¿ç•™å†…å®¹
    2. æ¸…ç†è¿ç»­ç©ºæ ¼è½¬ä¹‰ \  \  \ 
    3. ä¿®å¤ \left| ä¸º \midï¼ˆåœ¨é›†åˆå®šä¹‰ä¸­ï¼‰
    
    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼šB = \left\{ \boxed{x} - 3 < x < 1 \right\}
        è¾“å‡ºï¼šB = \left\{ x \mid -3 < x < 1 \right\}
    """
    import re
    
    if not text:
        return text
    
    # 1. ç§»é™¤ \boxed{}ï¼Œä¿ç•™å†…å®¹
    text = re.sub(r'\\boxed\{([^}]*)\}', r'\1', text)
    
    # 2. æ¸…ç†è¿ç»­ç©ºæ ¼è½¬ä¹‰ \  \  \ 
    text = re.sub(r'(\\ ){2,}', r' ', text)
    
    # 3. ä¿®å¤ \left| ä¸º \midï¼ˆåœ¨é›†åˆå®šä¹‰ä¸­ï¼‰
    # åŒ¹é… \left\{ ... \left| ... \right. ... \right\}
    def fix_set_bar(match):
        content = match.group(0)
        # å°†é›†åˆæ¡ä»¶ä¸­çš„ \left| æ›¿æ¢ä¸º \mid
        content = re.sub(r'\\left\|', r'\\mid ', content)
        # ç§»é™¤å¯¹åº”çš„ \right.ï¼ˆå¦‚æœæœ‰ï¼‰
        content = re.sub(r'\\mid\s*([^}]*?)\\right\.', r'\\mid \1', content)
        return content
    
    text = re.sub(
        r'\\left\\{[^}]*?\\left\|[^}]*?\\right\\}',
        fix_set_bar,
        text,
        flags=re.DOTALL
    )
    
    return text


def fix_right_boundary_errors(text: str) -> str:
    """ä¿®å¤ \\right. è¾¹ç•Œé”™è¯¯ - å¢å¼ºç‰ˆ
    
    å¤„ç†ä»¥ä¸‹ç•¸å½¢æ¨¡å¼ï¼š
    1. \\right.\\ $$ â†’ \\right.\\)  (åæ–œæ +ç©ºæ ¼+åŒç¾å…ƒ)
    2. \\right.\\\\ $$ â†’ \\right.\\)  (åŒåæ–œæ +åŒç¾å…ƒ)
    3. \\right. $$ â†’ \\right.\\)  (ç©ºæ ¼+åŒç¾å…ƒ)
    4. \\right.ä¸­æ–‡ â†’ \\right.\\)ä¸­æ–‡  (ç›´æ¥è·Ÿä¸­æ–‡)
    """
    import re
    
    if not text:
        return text
    
    # æ¨¡å¼1: \right.\ $$ (åæ–œæ +ç©ºæ ¼+åŒç¾å…ƒ) - æœ€å¸¸è§çš„OCRé”™è¯¯
    text = re.sub(r'\\right\.\\\s\$\$', r'\\right.\\)', text)
    
    # æ¨¡å¼2: \right.\\ $$ (åŒåæ–œæ +å¯é€‰ç©ºæ ¼+åŒç¾å…ƒ)
    text = re.sub(r'\\right\.\\\\\s*\$\$', r'\\right.\\)', text)
    
    # æ¨¡å¼3: \right. $$ (ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼+åŒç¾å…ƒ)
    text = re.sub(r'\\right\.\s+\$\$', r'\\right.\\)', text)
    
    # æ¨¡å¼4: \right.$$ (ç›´æ¥è·ŸåŒç¾å…ƒï¼Œæ— ç©ºæ ¼)
    text = re.sub(r'\\right\.\$\$', r'\\right.\\)', text)
    
    # æ¨¡å¼5: \right. åç›´æ¥è·Ÿä¸­æ–‡æ ‡ç‚¹ï¼ˆç¼ºå°‘ \)ï¼‰
    text = re.sub(r'(\\right\.)\s*([ï¼Œã€‚ï¼›ï¼šã€ï¼ï¼Ÿ])', r'\1\\)\2', text)
    
    # æ¨¡å¼6: \right. åç›´æ¥è·Ÿä¸­æ–‡æ–‡å­—ï¼ˆç¼ºå°‘ \)ï¼‰
    text = re.sub(r'(\\right\.)\s*([\u4e00-\u9fa5])', r'\1\\)\2', text)
    
    return text


def fix_reversed_delimiters(text: str) -> str:
    r"""ä¿®å¤åå‘å®šç•Œç¬¦ - ä½¿ç”¨æ ˆç®—æ³•ï¼ˆè·¨è¡Œå¤„ç†ï¼‰
    
    æ£€æµ‹æ²¡æœ‰åŒ¹é…çš„ \) å¹¶åˆ é™¤å®ƒä»¬ã€‚
    
    ğŸ†• v1.9.4: æ”¹ä¸ºå…¨æ–‡è·¨è¡Œå¤„ç†ï¼Œè€Œéé€è¡Œå¤„ç†ï¼Œä»¥æ­£ç¡®å¤„ç†å¤šè¡Œæ•°å­¦å—å¦‚ï¼š
        è”ç«‹\(\left\{ \begin{array}{r}
        x = my + \frac{3}{2} \\
        y^{2} = 6x
        \end{array} \right.\)
    
    é€è¡Œå¤„ç†ä¼šé”™è¯¯åœ°åœ¨ç¬¬ä¸€è¡Œæœ«å°¾æ·»åŠ  \)ï¼Œå› ä¸ºè¯¥è¡Œçš„ \( åœ¨åç»­è¡Œæ‰é—­åˆã€‚
    """
    import re
    
    if not text:
        return text
    
    # å…¨æ–‡å¤„ç†ï¼šä½¿ç”¨æ ˆæ£€æµ‹ä¸åŒ¹é…çš„å®šç•Œç¬¦
    stack = []  # å­˜å‚¨ \( çš„ä½ç½®
    unmatched_close = []  # å­˜å‚¨æ²¡æœ‰åŒ¹é…çš„ \) çš„ä½ç½®
    
    # æ‰¾åˆ°æ‰€æœ‰å®šç•Œç¬¦ï¼ˆæ’é™¤æ³¨é‡Šè¡Œä¸­çš„ï¼‰
    lines = text.split('\n')
    comment_ranges = []  # è®°å½•æ³¨é‡Šè¡Œçš„å­—ç¬¦èŒƒå›´
    pos = 0
    for line in lines:
        if line.strip().startswith('%'):
            comment_ranges.append((pos, pos + len(line)))
        pos += len(line) + 1  # +1 for \n
    
    def is_in_comment(position):
        for start, end in comment_ranges:
            if start <= position < end:
                return True
        return False
    
    for m in re.finditer(r'\\\(|\\\)', text):
        if is_in_comment(m.start()):
            continue
        delim = m.group(0)
        pos = m.start()
        
        if delim == r'\(':
            stack.append(pos)
        else:  # \)
            if stack:
                stack.pop()  # æ‰¾åˆ°åŒ¹é…
            else:
                unmatched_close.append(pos)  # æ²¡æœ‰åŒ¹é…çš„ \)
    
    # å¦‚æœå­˜åœ¨ä¸åŒ¹é…çš„å®šç•Œç¬¦ï¼Œéœ€è¦ä¿®å¤
    if not unmatched_close and not stack:
        return text  # å·²ç»å¹³è¡¡ï¼Œæ— éœ€ä¿®æ”¹
    
    # åˆ é™¤æ²¡æœ‰åŒ¹é…çš„ \)ï¼ˆä»åå¾€å‰åˆ é™¤ä»¥ä¿æŒä½ç½®æ­£ç¡®ï¼‰
    result = text
    if unmatched_close:
        text_chars = list(result)
        for pos in reversed(unmatched_close):
            # åˆ é™¤ \) (ä¸¤ä¸ªå­—ç¬¦)
            if pos + 1 < len(text_chars):
                del text_chars[pos:pos+2]
        result = ''.join(text_chars)
    
    # æ³¨æ„ï¼šä¸å†ä¸ºæœªåŒ¹é…çš„ \( åœ¨è¡Œå°¾æ·»åŠ  \)
    # è¿™æ˜¯å› ä¸ºå¤šè¡Œæ•°å­¦å—çš„ \( å¯èƒ½åœ¨åç»­è¡Œæ‰é—­åˆï¼Œé€è¡Œæ·»åŠ ä¼šé€ æˆé”™è¯¯
    # å¦‚æœçœŸçš„æœ‰æœªé—­åˆçš„ \(ï¼Œåº”è¯¥ç”±å…¶ä»–åå¤„ç†å‡½æ•°æˆ–æ‰‹åŠ¨ä¿®å¤
    # ï¼ˆåªè®°å½•è­¦å‘Šï¼Œä¸è‡ªåŠ¨æ·»åŠ ï¼‰
    if stack:
        # å¯ä»¥é€‰æ‹©æ‰“å°è­¦å‘Šï¼Œä½†ä¸è‡ªåŠ¨ä¿®å¤
        pass
    
    return result


def balance_array_and_cases_env(text: str) -> str:
    """ğŸ†• v1.8.6ï¼šåå¤„ç† - åˆ é™¤æ˜æ˜¾å¤šä½™çš„ \\end{array}/\\end{cases}

    åªåœ¨æ²¡æœ‰åŒ¹é… \\begin æ—¶ä¸¢å¼ƒ \\endï¼Œä¸è‡ªåŠ¨ç”Ÿæˆæ–°çš„ \\beginã€‚
    ä½¿ç”¨æ ˆåŒ¹é…ç®—æ³•ï¼Œç¡®ä¿ array/cases ç¯å¢ƒå¹³è¡¡ã€‚

    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š\\end{array} \\right.\\)ï¼Œåˆ™ï¼ˆæ— å¯¹åº”çš„ \\begin{array}ï¼‰
        è¾“å‡ºï¼š\\right.\\)ï¼Œåˆ™ï¼ˆä¸¢å¼ƒå¤šä½™çš„ \\end{array}ï¼‰
    """
    if not text:
        return text

    pattern = re.compile(r'\\(begin|end)\{(array|cases)\}')
    out_parts = []
    stack = []
    last = 0

    for m in pattern.finditer(text):
        out_parts.append(text[last:m.start()])
        kind, env = m.group(1), m.group(2)
        token = m.group(0)

        if kind == 'begin':
            stack.append(env)
            out_parts.append(token)
        else:  # end
            if stack and env in stack:
                # ä»æ ˆå°¾æ‰¾åŒ¹é…çš„ begin
                idx = len(stack) - 1 - stack[::-1].index(env)
                stack.pop(idx)
                out_parts.append(token)
            else:
                # æ²¡æœ‰åŒ¹é…çš„ beginï¼Œè¯´æ˜æ˜¯å¤šä½™çš„ \end{env}ï¼Œç›´æ¥ä¸¢å¼ƒ
                # å¯ä»¥å¢åŠ æ—¥å¿—ï¼Œå¦‚æœé¡¹ç›®ä¸­æœ‰ logger
                print(f"âš ï¸  [balance_array_and_cases_env] Drop unmatched {token} at pos {m.start()}")
                pass

        last = m.end()

    out_parts.append(text[last:])
    return ''.join(out_parts)


def fix_nested_subquestions(text: str) -> str:
    r"""ğŸ†• v1.9.6ï¼šä¿®å¤åµŒå¥—å­é¢˜å·æ ¼å¼
    
    é—®é¢˜æ¨¡å¼ï¼š
    - \item (i)xxx â†’ éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸º (i)(ii) æ˜¯ç¬¬äºŒçº§å­é¢˜
    - ç›®å‰ä¿å®ˆå¤„ç†ï¼šåªæ¸…ç† \item åç´§è·Ÿ (i)/(ii) çš„æƒ…å†µ
    
    ä¾‹å¦‚ï¼š
    - \item (i)æ±‚è§’çš„å¤§å° â†’ \item[(i)] æ±‚è§’çš„å¤§å°
    """
    import re
    
    # åŒ¹é… \item åç´§è·Ÿ (i)/(ii)/(iii) ç­‰
    # æ›¿æ¢ä¸º \item[(i)] æ ¼å¼
    pattern = r'\\item\s+\(([ivxIVX]+)\)'
    text = re.sub(pattern, r'\\item[(\1)]', text)
    
    # åŒæ ·å¤„ç†å…¨è§’æ‹¬å·
    pattern_cn = r'\\item\s+ï¼ˆ([ivxIVX]+)ï¼‰'
    text = re.sub(pattern_cn, r'\\item[(\1)]', text)
    
    return text


def fix_spurious_items_in_enumerate(text: str) -> str:
    r"""ğŸ†• v1.9.6ï¼šåˆå¹¶ enumerate ä¸­é”™è¯¯çš„å¤šä½™ \item
    
    é—®é¢˜æ¨¡å¼ï¼š
    åœ¨ enumerate ç¯å¢ƒä¸­ï¼Œå¦‚æœä¸€ä¸ªå­é—®é¢˜è·¨å¤šè¡Œï¼Œæ¯è¡Œå¯èƒ½éƒ½è¢«é”™è¯¯åœ°åŠ ä¸Š \itemã€‚
    ä¾‹å¦‚ï¼š
      \item è‹¥è§’å¹³åˆ†çº¿äº¤ACäºç‚¹Dï¼Œ
      \item ä¸”AD = 2DCï¼Œ
      \item æ±‚BDï¼
    
    åº”è¯¥åˆå¹¶ä¸ºï¼š
      \item è‹¥è§’å¹³åˆ†çº¿äº¤ACäºç‚¹Dï¼Œä¸”AD = 2DCï¼Œæ±‚BDï¼
    
    ä¿å®ˆç­–ç•¥ï¼š
    - åªåˆå¹¶é‚£äº›ä¸ä»¥å°é—®ç¼–å·ï¼ˆå¦‚ "â‘ â‘¡" æˆ– "(1)(2)"ï¼‰å¼€å¤´çš„ \item
    - å¦‚æœ \item å†…å®¹ä»¥ "æ±‚"ã€"è¯æ˜"ã€"è®¾" ç­‰åŠ¨è¯å¼€å¤´ï¼Œä¿ç•™ä¸ºç‹¬ç«‹ \item
    """
    import re
    
    lines = text.split('\n')
    result = []
    i = 0
    n = len(lines)
    
    # ç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯å­é—®é¢˜å¼€å¤´çš„æ¨¡å¼
    subq_start_patterns = [
        r'^\\item\s*[\(ï¼ˆ][1-9ivxIVX]+[\)ï¼‰]',  # (1), (i), ï¼ˆ1ï¼‰, ï¼ˆiï¼‰
        r'^\\item\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # â‘ â‘¡â‘¢...
        r'^\\item\s*\[[^\]]+\]',  # \item[(i)]
        r'^\\item\s*(æ±‚è¯|è¯æ˜|æ±‚|è®¾|è§£)',  # ä»¥åŠ¨è¯å¼€å¤´
    ]
    
    def is_subq_start(line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å­é—®é¢˜å¼€å¤´"""
        for pattern in subq_start_patterns:
            if re.match(pattern, line.strip()):
                return True
        return False
    
    in_enumerate = False
    enumerate_depth = 0
    pending_item = None  # å¾…åˆå¹¶çš„ \item è¡Œ
    
    while i < n:
        line = lines[i]
        stripped = line.strip()
        
        # æ£€æµ‹ enumerate ç¯å¢ƒ
        if r'\begin{enumerate}' in line:
            if pending_item:
                result.append(pending_item)
                pending_item = None
            result.append(line)
            in_enumerate = True
            enumerate_depth += 1
            i += 1
            continue
        
        if r'\end{enumerate}' in line:
            if pending_item:
                result.append(pending_item)
                pending_item = None
            result.append(line)
            enumerate_depth -= 1
            if enumerate_depth == 0:
                in_enumerate = False
            i += 1
            continue
        
        # å¦‚æœä¸åœ¨ enumerate ä¸­ï¼Œç›´æ¥è¾“å‡º
        if not in_enumerate:
            result.append(line)
            i += 1
            continue
        
        # åœ¨ enumerate ä¸­
        if stripped.startswith(r'\item'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯å­é—®é¢˜å¼€å¤´
            if is_subq_start(stripped):
                # è¿™æ˜¯ä¸€ä¸ªæ–°çš„å­é—®é¢˜ï¼Œå…ˆè¾“å‡ºä¹‹å‰çš„ pending
                if pending_item:
                    result.append(pending_item)
                pending_item = line
            else:
                # ä¸æ˜¯å­é—®é¢˜å¼€å¤´ï¼Œå¯èƒ½éœ€è¦åˆå¹¶
                if pending_item:
                    # æå– \item åçš„å†…å®¹
                    item_content = re.sub(r'^\\item\s*', '', stripped)
                    # åˆå¹¶åˆ° pending_item
                    pending_item = pending_item.rstrip() + item_content
                else:
                    # æ²¡æœ‰ pendingï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ª item
                    pending_item = line
        else:
            # é \item è¡Œ
            if pending_item:
                result.append(pending_item)
                pending_item = None
            result.append(line)
        
        i += 1
    
    # è¾“å‡ºæœ€åçš„ pending
    if pending_item:
        result.append(pending_item)
    
    return '\n'.join(result)


def fix_keep_questions_together(text: str) -> str:
    r"""ğŸ†• v1.9.7ï¼šå°½é‡ä¸åˆ†é¡µï¼ˆä¿å®ˆï¼‰

    âš ï¸ å·²ç¦ç”¨ï¼šsamepage ç¯å¢ƒä¸èƒ½åœ¨ question ç¯å¢ƒå†…éƒ¨ä½¿ç”¨ï¼Œä¼šå¯¼è‡´åµŒå¥—é”™è¯¯ã€‚
    éœ€è¦åœ¨ examx.sty ä¸­é€šè¿‡å…¶ä»–æ–¹å¼å®ç°ï¼ˆå¦‚ needspace æˆ– samepage åœ¨ question ç¯å¢ƒå®šä¹‰ä¸­ï¼‰ã€‚
    
    åŸè®¾è®¡ï¼šåœ¨æ¯ä¸ª `question` ç¯å¢ƒçš„ä¸»ä½“å‰åæ·»åŠ  `samepage` ç¯å¢ƒåŒ…è£…
    é—®é¢˜ï¼šquestion ç¯å¢ƒæœ‰ç‰¹æ®Šç»“æ„ï¼Œå†…éƒ¨æ’å…¥ samepage ä¼šå¯¼è‡´ LaTeX åµŒå¥—é”™è¯¯
    """
    # æš‚æ—¶ç¦ç”¨ï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬
    return text


def fix_trig_function_spacing(text: str) -> str:
    r"""ğŸ†• v1.9.6ï¼šä¿®å¤ä¸‰è§’å‡½æ•°å’Œå¯¹æ•°å‡½æ•°åç¼ºå°‘ç©ºæ ¼çš„é—®é¢˜
    
    é—®é¢˜æ¨¡å¼ï¼š
    - \sinx â†’ \sin x
    - \cosB â†’ \cos B
    - \lnt â†’ \ln t
    
    ä¿å®ˆå¤„ç†ï¼šåªä¿®å¤åé¢ç´§è·Ÿå•ä¸ªå­—æ¯/å˜é‡çš„æƒ…å†µ
    """
    import re
    
    # å®šä¹‰éœ€è¦å¤„ç†çš„å‡½æ•°å
    trig_funcs = ['sin', 'cos', 'tan', 'cot', 'sec', 'csc', 'arcsin', 'arccos', 'arctan',
                  'sinh', 'cosh', 'tanh', 'ln', 'log', 'lg', 'exp']
    
    for func in trig_funcs:
        # åŒ¹é… \func åç´§è·Ÿå­—æ¯ï¼ˆé { æˆ–ç©ºæ ¼çš„æƒ…å†µï¼‰
        # ä¾‹å¦‚ \sinx â†’ \sin x, \cosB â†’ \cos B
        pattern = rf'\\{func}([A-Za-z])(?![a-zA-Z])'
        text = re.sub(pattern, rf'\\{func} \1', text)
    
    return text


def fix_undefined_symbols(text: str) -> str:
    r"""ğŸ†• v1.9.6ï¼šæ›¿æ¢å¯èƒ½æœªå®šä¹‰çš„æ•°å­¦ç¬¦å·
    
    å·²çŸ¥é—®é¢˜ï¼š
    - \bigtriangleup â†’ \triangle (amssymb ä¸­æœ‰å®šä¹‰)
    """
    import re
    
    # \bigtriangleup æ›¿æ¢ä¸º \triangle
    text = re.sub(r'\\bigtriangleup\b', r'\\triangle', text)
    
    return text


def fix_markdown_bold_residue(text: str) -> str:
    r"""ğŸ†• v1.9.7ï¼šæ¸…ç† Markdown ç²—ä½“æ®‹ç•™
    
    é—®é¢˜æ¥æºï¼š
    - Word æ–‡æ¡£ä¸­æŸäº›æ ‡ç‚¹è¢«åŠ ç²—ï¼ŒPandoc è½¬æ¢ä¸º **ï¼Œ** ç­‰
    - é¢„å¤„ç†å¯èƒ½æ²¡æœ‰å®Œå…¨æ¸…ç†å¹²å‡€
    
    ä¿å®ˆç­–ç•¥ï¼š
    - åªå¤„ç†"çº¯æ ‡ç‚¹æˆ–çŸ­æ–‡æœ¬+æ ‡ç‚¹è¢«ç²—ä½“åŒ…è£¹"çš„æƒ…å†µ
    - ä¸å¤„ç†æ­£å¸¸çš„ç²—ä½“æ–‡æœ¬
    
    ä¾‹å¦‚ï¼š
    - **ï¼Œ** â†’ ï¼Œ
    - **ï¼Œå¾—è¯.** â†’ ï¼Œå¾—è¯.
    - **ã€‚** â†’ ã€‚
    """
    import re
    
    # æ¨¡å¼1ï¼šçº¯æ ‡ç‚¹è¢«ç²—ä½“åŒ…è£¹ **ï¼Œ** **ã€‚** **ï¼›** ç­‰
    text = re.sub(r'\*\*([ï¼Œã€‚ï¼›ã€ï¼šï¼ï¼Ÿ,.;:!?])\*\*', r'\1', text)
    
    # æ¨¡å¼2ï¼šæ ‡ç‚¹å¼€å¤´+çŸ­æ–‡æœ¬+æ ‡ç‚¹ç»“å°¾è¢«ç²—ä½“åŒ…è£¹
    # ä¾‹å¦‚ï¼š**ï¼Œå¾—è¯.** â†’ ï¼Œå¾—è¯.
    text = re.sub(r'\*\*([ï¼Œã€‚ï¼›ã€ï¼š,.;:][^\*]{0,10}[.ï¼ã€‚])\*\*', r'\1', text)
    
    return text


def fix_specific_reversed_pairs(text: str) -> str:
    r"""ğŸ†• v1.8.7ï¼šæçª„è‡ªåŠ¨ä¿®å¤ç‰¹å®šåå‘æ•°å­¦å®šç•Œç¬¦æ¨¡å¼

    ä»…é’ˆå¯¹ç²¾ç¡®åŒ¹é…çš„å·²çŸ¥é”™è¯¯æ¨¡å¼ï¼š
    - æ¨¡å¼ A: æ±‚ç‚¹\)X_{2}\(æ‰€æœ‰å¯èƒ½çš„åæ ‡ â†’ æ±‚ç‚¹\(X_{2}\)æ‰€æœ‰å¯èƒ½çš„åæ ‡
    - æ¨¡å¼ B: å…¶ä¸­\)x_{i} â†’ å…¶ä¸­ x_{i}ï¼ˆåˆ é™¤ä¸åŒ¹é…çš„ \)ï¼‰

    å®‰å…¨æ€§ï¼šåªé’ˆå¯¹ç²¾ç¡®åŒ¹é…çš„æ¨¡å¼ï¼Œä¸å½±å“å…¶ä»–å†…å®¹
    """
    if not text:
        return text

    # æ¨¡å¼ A: æ±‚ç‚¹\)X_{2}\(æ‰€æœ‰å¯èƒ½çš„åæ ‡ â†’ æ±‚ç‚¹\(X_{2}\)æ‰€æœ‰å¯èƒ½çš„åæ ‡
    # ç²¾ç¡®åŒ¹é…ï¼š\) + å­—æ¯/æ•°å­—/ä¸‹åˆ’çº¿ + \( â†’ \( + å­—æ¯/æ•°å­—/ä¸‹åˆ’çº¿ + \)
    pattern_a = re.compile(r'\\\)([A-Za-z0-9_{}]+)\\\(')
    text = pattern_a.sub(r'\(\1\)', text)

    # æ¨¡å¼ B: å…¶ä¸­\)x_{i} â†’ å…¶ä¸­ x_{i}ï¼ˆåˆ é™¤ä¸åŒ¹é…çš„ \)ï¼‰
    # ç²¾ç¡®åŒ¹é…ï¼š\) + ç©ºæ ¼ + å­—æ¯/æ•°å­—ï¼ˆè¡Œå°¾æˆ–åç»­æ—  \(ï¼‰
    pattern_b = re.compile(r'\\\)\s+([a-z][a-z_0-9{}]*(?![^\n]*\\\())')
    text = pattern_b.sub(r' \1', text)

    return text


def fix_simple_reversed_inline_pairs(text: str) -> str:
    r"""ğŸ†• v1.8.8 / v1.9.3ï¼šæåº¦ä¿å®ˆçš„åå‘å®šç•Œç¬¦è‡ªåŠ¨ä¿®å¤

    åªä¿®å¤çœŸæ­£çš„åå‘å®šç•Œç¬¦ï¼šå³ \) ä¹‹å‰æ²¡æœ‰åŒ¹é…çš„ \(ï¼Œä¸” \( ä¹‹åæ²¡æœ‰åŒ¹é…çš„ \)ã€‚

    v1.9.3 ä¿®å¤ï¼šä¸å†é”™è¯¯åœ°åˆå¹¶ä¸¤ä¸ªç‹¬ç«‹çš„æ­£ç¡®æ•°å­¦å—ï¼Œä¾‹å¦‚ï¼š
    - æ­£ç¡®ä¿ç•™ï¼š\(AP\bot AB\)ï¼Œ\(AP\bot AD\) ï¼ˆä¸¤ä¸ªç‹¬ç«‹å—ï¼Œä¸åº”ä¿®æ”¹ï¼‰
    - ä»…ä¿®å¤çœŸæ­£åå‘çš„ï¼šæ±‚ç‚¹\) X_2 \(æ‰€æœ‰å¯èƒ½ â†’ æ±‚ç‚¹\( X_2 \)æ‰€æœ‰å¯èƒ½

    å®‰å…¨æ€§ï¼šä½¿ç”¨å®šç•Œç¬¦å¹³è¡¡æ£€æŸ¥ï¼Œç¡®ä¿åªä¿®å¤çœŸæ­£æ‚¬ç©ºçš„å®šç•Œç¬¦å¯¹
    """
    if not text:
        return text

    import re

    # é€è¡Œå¤„ç†ï¼Œé¿å…è·¨è¡ŒåŒ¹é…å¸¦æ¥çš„å¤æ‚æ€§
    lines = text.split('\n')
    fixed_lines = []

    for line in lines:
        # è·³è¿‡æ³¨é‡Šè¡Œ
        if line.strip().startswith('%'):
            fixed_lines.append(line)
            continue

        # ğŸ†• v1.9.5ï¼šè·³è¿‡å¤šè¡Œæ•°å­¦å—çš„ä¸­é—´è¡Œ
        # å¦‚æœè¡ŒåŒ…å« \begin{array/cases æˆ– \end{array/cases}ï¼Œè¯´æ˜æ˜¯å¤šè¡Œå—çš„ä¸€éƒ¨åˆ†
        # è¿™äº›è¡Œçš„å®šç•Œç¬¦å¯èƒ½æ˜¯è·¨è¡Œé…å¯¹çš„ï¼Œä¸åº”è¯¥æŒ‰å•è¡Œå¤„ç†
        if re.search(r'\\begin\{(array|cases|matrix|pmatrix|bmatrix|vmatrix)', line) or \
           re.search(r'\\end\{(array|cases|matrix|pmatrix|bmatrix|vmatrix)', line):
            fixed_lines.append(line)
            continue

        # æ‰¾åˆ°æ‰€æœ‰å®šç•Œç¬¦çš„ä½ç½®
        delimiters = []
        for m in re.finditer(r'\\\(|\\\)', line):
            delimiters.append((m.start(), m.group(0)))

        if len(delimiters) < 2:
            fixed_lines.append(line)
            continue

        # ä½¿ç”¨æ ˆç®—æ³•æ‰¾åˆ°çœŸæ­£æ‚¬ç©ºçš„ \) å’Œ \(
        stack = []  # å­˜å‚¨æœªåŒ¹é… \( çš„ç´¢å¼•
        unmatched_close_indices = []  # å­˜å‚¨æ‚¬ç©º \) åœ¨ delimiters ä¸­çš„ç´¢å¼•
        unmatched_open_indices = []  # å­˜å‚¨æ‚¬ç©º \( åœ¨ delimiters ä¸­çš„ç´¢å¼•

        for i, (pos, delim) in enumerate(delimiters):
            if delim == r'\(':
                stack.append(i)
            else:  # \)
                if stack:
                    stack.pop()  # åŒ¹é…æˆåŠŸ
                else:
                    unmatched_close_indices.append(i)  # æ‚¬ç©ºçš„ \)

        # æ ˆä¸­å‰©ä½™çš„æ˜¯æ‚¬ç©ºçš„ \(
        unmatched_open_indices = stack

        # åªæœ‰å½“å­˜åœ¨æ‚¬ç©ºçš„ \) ä¸”ç´§éšå…¶åæœ‰æ‚¬ç©ºçš„ \( æ—¶ï¼Œæ‰è€ƒè™‘ä¿®å¤
        # æ‰¾åˆ° (æ‚¬ç©º\), æ‚¬ç©º\() é…å¯¹
        pairs_to_fix = []
        for close_idx in unmatched_close_indices:
            # æ‰¾ç´§éšå…¶åçš„æ‚¬ç©º \(
            for open_idx in unmatched_open_indices:
                if open_idx > close_idx:
                    # æ£€æŸ¥ä¸­é—´æ˜¯å¦åªæœ‰ç®€å•å†…å®¹ï¼ˆæ ‡ç‚¹ã€ç©ºç™½ã€ç®€å•å­—æ¯æ•°å­—ï¼‰
                    close_pos = delimiters[close_idx][0]
                    open_pos = delimiters[open_idx][0]
                    middle = line[close_pos + 2:open_pos]  # è·³è¿‡ \) çš„ä¸¤ä¸ªå­—ç¬¦

                    # å…è®¸ç©ºç™½ã€æ ‡ç‚¹ã€ç®€å•å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿ï¼ˆç±»ä¼¼å˜é‡åï¼‰
                    # ä½†ä¸å…è®¸å¤æ‚çš„ LaTeX å‘½ä»¤æˆ–åµŒå¥—å®šç•Œç¬¦
                    if re.fullmatch(r'[\s.,ï¼Œã€‚ï¼›;:ï¼šã€!?ï¼ï¼Ÿ"""\'\'ã€Šã€‹ï¼ˆï¼‰()â€¦â€”\-A-Za-z0-9_{}]*', middle or ''):
                        pairs_to_fix.append((close_idx, open_idx, middle))
                        break  # æ¯ä¸ªæ‚¬ç©º \) åªé…å¯¹ä¸€ä¸ªæ‚¬ç©º \(

        # ä»åå¾€å‰ä¿®å¤ï¼ˆä¿æŒä½ç½®æ­£ç¡®ï¼‰
        line_chars = list(line)
        for close_idx, open_idx, middle in reversed(pairs_to_fix):
            close_pos = delimiters[close_idx][0]
            open_pos = delimiters[open_idx][0]
            # æ›¿æ¢ \) ä¸º \(
            line_chars[close_pos:close_pos + 2] = list(r'\(')
            # æ›¿æ¢ \( ä¸º \)ï¼ˆæ³¨æ„ï¼šç”±äºå‰é¢çš„æ›¿æ¢ï¼Œé•¿åº¦ä¸å˜ï¼‰
            line_chars[open_pos:open_pos + 2] = list(r'\)')

        fixed_lines.append(''.join(line_chars))

    return '\n'.join(fixed_lines)


def collect_reversed_math_samples(text: str, slug: str = "") -> None:
    r"""ğŸ†• v1.8.8ï¼šæ£€æµ‹å¹¶è®°å½•åå‘æ•°å­¦å®šç•Œç¬¦æ¡ˆä¾‹ï¼ˆåªè®°å½•ï¼Œä¸ä¿®æ”¹ï¼‰

    æœç´¢ \)...\( å’Œ \]...\[ ç±»å‹çš„åå‘å®šç•Œç¬¦ï¼Œè®°å½•åˆ° issue æ—¥å¿—ã€‚

    Args:
        text: å®Œæ•´çš„ TeX æ–‡æœ¬
        slug: è¯•å· slugï¼ˆç”¨äºæ—¥å¿—æ–‡ä»¶åï¼‰
    """
    if not text or not slug:
        return

    import re

    lines = text.splitlines()
    reversed_cases = []

    for line_num, line in enumerate(lines, start=1):
        # åªè€ƒè™‘æ³¨é‡Šå‰çš„éƒ¨åˆ†
        content = line.split('%', 1)[0]

        # æ£€æµ‹åå‘çš„è¡Œå†…æ•°å­¦å®šç•Œç¬¦ \)...\(
        inline_pattern = re.compile(r'\\\)([^%]*?)\\\(')
        inline_matches = list(inline_pattern.finditer(content))
        for match in inline_matches:
            middle = match.group(1)
            # æˆªæ–­è¡Œå†…å®¹ç”¨äºæ—¥å¿—æ˜¾ç¤º
            line_display = line[:100] + '...' if len(line) > 100 else line
            reversed_cases.append(
                f"Line {line_num}: Found reversed inline math \\)...\\("
                f"\n  Middle content: '{middle}'"
                f"\n  Line: {line_display}"
            )

        # æ£€æµ‹åå‘çš„æ˜¾ç¤ºæ•°å­¦å®šç•Œç¬¦ \]...\[
        display_pattern = re.compile(r'\\\]([^%]*?)\\\[')
        display_matches = list(display_pattern.finditer(content))
        for match in display_matches:
            middle = match.group(1)
            line_display = line[:100] + '...' if len(line) > 100 else line
            reversed_cases.append(
                f"Line {line_num}: Found reversed display math \\]...\\["
                f"\n  Middle content: '{middle}'"
                f"\n  Line: {line_display}"
            )

    # å¦‚æœæ‰¾åˆ°åå‘å®šç•Œç¬¦ï¼Œè®°å½•åˆ°æ—¥å¿—
    if reversed_cases:
        from pathlib import Path
        debug_dir = Path("word_to_tex/output/debug")
        debug_dir.mkdir(parents=True, exist_ok=True)
        log_file = debug_dir / f"{slug}_reversed_delimiters.log"

        with log_file.open("w", encoding="utf-8") as f:
            f.write(f"# Reversed Math Delimiters Detection Log for {slug}\n")
            f.write(f"# Total cases found: {len(reversed_cases)}\n")
            f.write(f"# Generated: {Path(__file__).name}\n")
            f.write("\n")

            for i, case in enumerate(reversed_cases, start=1):
                f.write(f"{'='*80}\n")
                f.write(f"Case #{i}:\n")
                f.write(case + "\n\n")

        print(f"âš ï¸  Found {len(reversed_cases)} reversed math delimiter cases, logged to {log_file}")


def validate_and_fix_image_todo_blocks(text: str) -> str:
    """ğŸ†• v1.8.5ï¼šéªŒè¯å¹¶ä¿®å¤ IMAGE_TODO å—æ ¼å¼é”™è¯¯

    æ£€æŸ¥å¹¶ä¿®å¤ï¼š
    1. IMAGE_TODO_END åçš„å¤šä½™èŠ±æ‹¬å·
    2. IMAGE_TODO_START å‚æ•°æ ¼å¼é”™è¯¯
    3. ç¼ºå¤±çš„å¿…éœ€å‚æ•°

    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š% IMAGE_TODO_END id=xxx{
        è¾“å‡ºï¼š% IMAGE_TODO_END id=xxx
    """
    if not text:
        return text

    issues = []

    # ä¿®å¤1ï¼šIMAGE_TODO_END åçš„å¤šä½™å­—ç¬¦ï¼ˆèŠ±æ‹¬å·æˆ–å…¶ä»–ï¼‰
    # åŒ¹é…ï¼š% IMAGE_TODO_END id=xxx{ æˆ– % IMAGE_TODO_END id=xxx {
    pattern = r'(% IMAGE_TODO_END id=[a-zA-Z0-9_-]+)\s*\{[^}]*\}'
    matches = list(re.finditer(pattern, text))
    for match in matches:
        line_num = text[:match.start()].count('\n') + 1
        issues.append(f"Line {line_num}: IMAGE_TODO_END has extra brace")

    # æ‰§è¡Œä¿®å¤
    text = re.sub(pattern, r'\1', text)

    # ä¿®å¤2ï¼šIMAGE_TODO_END åçš„å•ä¸ªèŠ±æ‹¬å·ï¼ˆæ— é…å¯¹ï¼‰
    text = re.sub(
        r'(% IMAGE_TODO_END id=[a-zA-Z0-9_-]+)\s*\{',
        r'\1',
        text
    )

    # ä¿®å¤3ï¼šIMAGE_TODO_START è¡Œæœ«çš„å¤šä½™å­—ç¬¦
    text = re.sub(
        r'(% IMAGE_TODO_START[^\n]+)\s*\{[^}]*\}',
        r'\1',
        text
    )

    # ä¿®å¤4ï¼šIMAGE_TODO_END ä¸æ­£æ–‡åŒå¤„ä¸€è¡Œï¼Œè‡ªåŠ¨æ‹†åˆ†
    def _split_image_end(match: re.Match) -> str:
        trailing = match.group(2)
        if not trailing.strip():
            return match.group(1)
        return f"{match.group(1)}\n{trailing.lstrip()}"

    text = re.sub(
        r'(% IMAGE_TODO_END id=[^\n]+)([^\n]+)',
        _split_image_end,
        text
    )

    if issues:
        print(f"âš ï¸  ä¿®å¤äº† {len(issues)} ä¸ª IMAGE_TODO æ ¼å¼é”™è¯¯ï¼š")
        for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {issue}")

    return text


def balance_left_right_delimiters(text: str) -> str:
    r"""å¹³è¡¡ \left/\right å®šç•Œç¬¦ï¼Œå­¤ç«‹é¡¹é™çº§ä¸ºæ™®é€šæ‹¬å·"""
    if not text or ('\\left' not in text and '\\right' not in text):
        return text

    pattern = re.compile(r'\\left\s*(?:\\[a-zA-Z]+|\\.|.)|\\right\s*(?:\\[a-zA-Z]+|\\.|.)')
    parts: List[str] = []
    stack: List[int] = []
    last = 0

    def _downgrade_left(token: str) -> str:
        remainder = token[len('\\left'):].lstrip()
        return '' if remainder.startswith('.') else remainder

    def _downgrade_right(token: str) -> str:
        remainder = token[len('\\right'):].lstrip()
        return '' if remainder.startswith('.') else remainder

    for match in pattern.finditer(text):
        parts.append(text[last:match.start()])
        token = match.group(0)

        if token.startswith('\\left'):
            parts.append(token)
            stack.append(len(parts) - 1)
        else:
            if stack:
                stack.pop()
                parts.append(token)
            else:
                parts.append(_downgrade_right(token))
        last = match.end()

    parts.append(text[last:])

    for idx in stack:
        parts[idx] = _downgrade_left(parts[idx])

    return ''.join(parts)


def cleanup_remaining_image_markers(text: str) -> str:
    """ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    
    ğŸ†• v1.6.2ï¼šå¢å¼ºå†…è”å…¬å¼å¤„ç†
    - ç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡ â†’ TikZå ä½ç¬¦å—ï¼ˆå¤§å›¾ï¼‰
    - å†…è”å›¾ç‰‡ï¼ˆå…¬å¼ï¼‰â†’ ç®€å•æ–‡æœ¬å ä½ç¬¦ [å…¬å¼:filename]
    
    å°†æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…åœ¨ PDF ä¸­æ˜¾ç¤º
    ä¸ºåŸå§‹è·¯å¾„æ–‡æœ¬ã€‚æ”¯æŒï¼š
      - ![@@@id](path){...}
      - ![](path){...}
    """
    if not text:
        return text
    
    def _make_tikz_placeholder(label: str) -> str:
        """åˆ›å»º TikZ å ä½ç¬¦å—ï¼ˆç”¨äºç‹¬ç«‹å›¾ç‰‡ï¼‰"""
        label = label.strip() or "image"
        return (
            "\n\\begin{center}\n"
            "\\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]\n"
            f"  \\node[draw, minimum width=6cm, minimum height=4cm] {{å›¾ç•¥ï¼ˆå›¾ ID: {label}ï¼‰}};\n"
            "\\end{tikzpicture}\n"
            "\\end{center}\n"
        )
    
    def _make_inline_placeholder(label: str) -> str:
        """åˆ›å»ºå†…è”å ä½ç¬¦ï¼ˆç”¨äºå…¬å¼å›¾ç‰‡ï¼‰"""
        label = label.strip() or "formula"
        # ä½¿ç”¨ç®€å•çš„æ–‡æœ¬å ä½ç¬¦ï¼Œå¯ä»¥åœ¨åç»­è¢«è¯†åˆ«å’Œæ›¿æ¢
        return f"[å…¬å¼:{label}]"
    
    def is_standalone_line(match_obj: re.Match, full_text: str) -> bool:
        """åˆ¤æ–­åŒ¹é…æ˜¯å¦ä¸ºç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡"""
        # è·å–åŒ¹é…å‰åçš„æ–‡æœ¬
        start = match_obj.start()
        end = match_obj.end()
        
        # å‘å‰æŸ¥æ‰¾åˆ°è¡Œé¦–
        line_start = start
        while line_start > 0 and full_text[line_start - 1] not in '\n':
            line_start -= 1
        
        # å‘åæŸ¥æ‰¾åˆ°è¡Œå°¾
        line_end = end
        while line_end < len(full_text) and full_text[line_end] not in '\n':
            line_end += 1
        
        # æ£€æŸ¥è¡Œå†…å®¹ï¼šå»é™¤ç©ºç™½åæ˜¯å¦åªæœ‰è¿™ä¸ªå›¾ç‰‡æ ‡è®°
        line_content = full_text[line_start:line_end].strip()
        match_content = match_obj.group(0).strip()
        
        return line_content == match_content
    
    # å¤„ç†å¸¦IDçš„å›¾ç‰‡æ ‡è®°
    def repl_with_id(m: re.Match) -> str:
        img_id = m.group(1)
        basename = os.path.basename(img_id) if img_id else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(basename)
        else:
            return _make_inline_placeholder(basename)
    
    import os  # ç¡®ä¿å¯¼å…¥
    text = IMAGE_PATTERN_WITH_ID.sub(repl_with_id, text)
    
    # å¤„ç†æ— IDçš„å›¾ç‰‡æ ‡è®°
    def repl_no_id(m: re.Match) -> str:
        path = m.group(1).strip()
        basename = os.path.basename(path)
        label = basename if basename else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(label)
        else:
            return _make_inline_placeholder(label)
    
    text = IMAGE_PATTERN_NO_ID.sub(repl_no_id, text)
    
    return text


def cleanup_guxuan_in_macros(text: str) -> str:
    """ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™
    
    é’ˆå¯¹ \\topics{...} å’Œ \\explain{...} ç­‰å®å‚æ•°å†…çš„"æ•…é€‰ï¼šX"è¿›è¡Œæ¸…ç†ã€‚
    
    Args:
        text: LaTeX æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text or 'æ•…é€‰' not in text:
        return text
    
    # å®šä¹‰è¦æ¸…ç†çš„å®åˆ—è¡¨
    macros = ['topics', 'explain', 'keywords', 'analysis']
    
    for macro_name in macros:
        # åŒ¹é… \macro{content}ï¼Œä½¿ç”¨é€’å½’åŒ¹é…åµŒå¥—å¤§æ‹¬å·
        # ç”±äºPython reä¸æ”¯æŒé€’å½’ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…+æ‰‹å·¥è§£æ
        pattern = rf'\\{macro_name}\{{'
        
        pos = 0
        result_parts = []
        
        while True:
            start_idx = text.find(pattern, pos)
            if start_idx == -1:
                result_parts.append(text[pos:])
                break
            
            # æ·»åŠ å‰é¢çš„æ–‡æœ¬
            result_parts.append(text[pos:start_idx])
            
            # æ‰‹å·¥è§£æåµŒå¥—å¤§æ‹¬å·
            brace_count = 0
            content_start = start_idx + len(pattern)
            i = content_start
            
            while i < len(text):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    if brace_count == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·
                        content = text[content_start:i]
                        
                        # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰"
                        # 1. æ¸…ç†è¡Œæœ«çš„"æ•…é€‰ï¼šX"ï¼ˆå«å„ç§æ ‡ç‚¹ç»„åˆå’Œå¯èƒ½çš„åç»­æ–‡æœ¬ï¼‰
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 2. æ¸…ç†å•ç‹¬ä¸€è¡Œçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 3. æ¸…ç†æ¢è¡Œç¬¦åçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*(?=\n|$)', '', content)
                        # 4. æ¸…ç†ä»»æ„ä½ç½®çš„"æ•…é€‰ï¼šX"ï¼ˆæ›´æ¿€è¿›ï¼‰
                        content = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*', '', content)
                        # 5. æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*[.ã€‚]?\s*', '', content)
                        
                        result_parts.append(rf'\{macro_name}{{{content}}}')
                        pos = i + 1
                        break
                    else:
                        brace_count -= 1
                elif text[i] == '\\' and i + 1 < len(text):
                    # è·³è¿‡è½¬ä¹‰å­—ç¬¦
                    i += 1
                i += 1
            else:
                # æ²¡æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·ï¼Œä¿ç•™åŸæ–‡
                result_parts.append(text[start_idx:])
                break
        
        text = ''.join(result_parts)
    
    return text


def fix_tabular_environments(text: str) -> str:
    r"""ğŸ†• v1.9.1ï¼šä¿®å¤ tabular ç¯å¢ƒç¼ºå¤±åˆ—æ ¼å¼ï¼ˆP1ï¼‰

    æ£€æµ‹å¹¶ä¿®å¤ \begin{tabular} ç¼ºå°‘åˆ—æ ¼å¼å‚æ•°çš„é—®é¢˜
    ä¾‹å¦‚ï¼š\begin{tabularend{center} â†’ \begin{tabular}{|c|c|}...\end{center}

    Args:
        text: LaTeX æ–‡æœ¬

    Returns:
        ä¿®å¤åçš„æ–‡æœ¬
    """
    if not text or '\\begin{tabular}' not in text:
        return text

    import re

    # æ£€æµ‹ä¸å®Œæ•´çš„ tabularï¼ˆåé¢æ²¡æœ‰ç´§è·Ÿ {åˆ—æ ¼å¼}ï¼‰
    pattern = re.compile(r'\\begin\{tabular\}(?!\s*\{)')

    def fix_tabular(match):
        # è·å–åŒ¹é…ä½ç½®
        start_pos = match.end()

        # æŸ¥æ‰¾åç»­å†…å®¹ï¼Œå°è¯•æ¨æ–­åˆ—æ•°
        # å‘åæŸ¥æ‰¾æœ€å¤š500ä¸ªå­—ç¬¦
        remaining = text[start_pos:start_pos+500]

        # å°è¯•æ‰¾åˆ°ç¬¬ä¸€è¡Œå†…å®¹ï¼ˆåˆ° \\ æˆ–æ¢è¡Œï¼‰
        first_row_match = re.search(r'([^\n\\]+?)(?:\\\\|\n)', remaining)
        if first_row_match:
            first_row = first_row_match.group(1)
            # ç»Ÿè®¡ & çš„æ•°é‡æ¥æ¨æ–­åˆ—æ•°
            col_count = first_row.count('&') + 1
        else:
            # é»˜è®¤2åˆ—
            col_count = 2

        # ç”Ÿæˆé»˜è®¤çš„åˆ—æ ¼å¼ï¼ˆå±…ä¸­å¯¹é½ï¼Œå¸¦ç«–çº¿ï¼‰
        col_format = '|' + 'c|' * col_count

        return match.group(0) + '{' + col_format + '}'

    return pattern.sub(fix_tabular, text)


def convert_markdown_table_to_latex(text: str) -> str:
    """å°† Markdown è¡¨æ ¼è½¬æ¢ä¸º LaTeX tabular"""
    table_pattern = r'(\|[^\n]+\|\n)+\|[-:\s|]+\|\n(\|[^\n]+\|\n)+'

    def convert_one_table(match):
        table_text = match.group(0)
        lines = [line.strip() for line in table_text.split('\n') if line.strip()]

        data_lines = [line for line in lines if not re.match(r'^\|[-:\s|]+\|$', line)]

        if not data_lines:
            return table_text

        rows = []
        for line in data_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            rows.append(cells)

        if not rows:
            return table_text

        ncols = len(rows[0])
        latex = "\\begin{center}\n"
        latex += f"\\begin{{tabular}}{{{'c' * ncols}}}\n"
        latex += "\\hline\n"

        header = rows[0]
        latex += " & ".join(escape_latex_special(cell, False) for cell in header)
        latex += " \\\\\n\\hline\n"

        for row in rows[1:]:
            latex += " & ".join(escape_latex_special(cell, False) for cell in row)
            latex += " \\\\\n"

        latex += "\\hline\n\\end{tabular}\n\\end{center}"
        return latex

    return re.sub(table_pattern, convert_one_table, text)


def convert_ascii_table_blocks(text: str) -> str:
    """å°†ç”±æ¨ªçº¿ + ç©ºæ ¼å¯¹é½ç»„æˆçš„ ASCII è¡¨æ ¼è½¬æ¢ä¸º tabular"""
    if not text:
        return text

    lines = text.splitlines()
    result: List[str] = []
    i = 0

    def _is_rule(line: str) -> bool:
        stripped = line.strip()
        if not stripped:
            return False
        return all(ch in {'-', ' '} for ch in stripped) and stripped.count('-') >= 6

    def _convert_block(block: List[str]) -> Optional[str]:
        inner = [ln.rstrip() for ln in block[1:-1]]
        rows = [ln.strip() for ln in inner if ln.strip() and not _is_rule(ln)]
        if len(rows) < 2:
            return None

        split_rows = [re.split(r'\s{2,}', row) for row in rows]
        col_count = max(len(r) for r in split_rows)
        if col_count < 2:
            return None

        def _pad(row: List[str]) -> List[str]:
            padded = [cell.strip() for cell in row]
            while len(padded) < col_count:
                padded.append('')
            return padded[:col_count]

        latex_lines = ["\\begin{center}", f"\\begin{{tabular}}{{{'c' * col_count}}}", "\\hline"]

        header = _pad(split_rows[0])
        latex_lines.append(" & ".join(escape_latex_special(cell, False) for cell in header) + r" \\")
        latex_lines.append("\\hline")

        for row in split_rows[1:]:
            cells = _pad(row)
            latex_lines.append(" & ".join(escape_latex_special(cell, False) for cell in cells) + r" \\")

        latex_lines.append("\\hline")
        latex_lines.append("\\end{tabular}")
        latex_lines.append("\\end{center}")
        return "\n".join(latex_lines)

    while i < len(lines):
        if _is_rule(lines[i]):
            j = i + 1
            while j < len(lines) and not _is_rule(lines[j]):
                j += 1
            if j < len(lines):
                block = lines[i:j + 1]
                converted = _convert_block(block)
                if converted:
                    result.append(converted)
                    i = j + 1
                    continue
        result.append(lines[i])
        i += 1

    return "\n".join(result)


def normalize_fullwidth_brackets(text: str) -> str:
    """ğŸ†• v1.6.3ï¼šç»Ÿä¸€å…¨è§’æ‹¬å·ä¸ºåŠè§’

    æ³¨æ„ï¼šä¸è¦æ›¿æ¢ç”¨äº meta æ ‡è®°çš„ã€ã€‘
    """
    pairs = {
        "ï¼ˆ": "(",
        "ï¼‰": ")",
        "ï½›": "{",
        "ï½": "}",
        # ä¸æ›¿æ¢ ï¼»ï¼½ï¼Œé¿å…å½±å“æŸäº› Markdown è¯­æ³•
    }
    for fw, hw in pairs.items():
        text = text.replace(fw, hw)
    return text


def clean_markdown(text: str) -> str:
    """æ¸…ç† markdown åƒåœ¾

    ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    ğŸ†• v1.6.3ï¼šå¢å¼ºå…¨è§’æ‹¬å·ç»Ÿä¸€
    """
    # ğŸ†• v1.6.3ï¼šé¦–å…ˆç»Ÿä¸€å…¨è§’æ‹¬å·
    text = normalize_fullwidth_brackets(text)

    text = re.sub(
        r"<br><span class='markdown-page-line'>.*?</span><br><br>",
        "\n", text, flags=re.S,
    )
    text = re.sub(
        r"<span id='page\d+' class='markdown-page-text'>\[.*?\]</span>",
        "", text,
    )

    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text)

    # é¢„æ¸…ç†è£…é¥°æ€§å›¾ç‰‡åŠå…¶å±æ€§
    text = remove_decorative_images(text)
    text = clean_image_attributes(text)

    # ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    # ä¿æŠ¤å·²æœ‰çš„LaTeXå‘½ä»¤
    protected = []
    def save_latex_cmd(match):
        protected.append(match.group(0))
        return f"@@LATEXCMD{len(protected)-1}@@"
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', save_latex_cmd, text)

    # ç»Ÿä¸€æ‹¬å·ï¼ˆå…¨è§’â†’åŠè§’ï¼‰- å·²åœ¨ normalize_fullwidth_brackets ä¸­å¤„ç†
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    # ç»Ÿä¸€å¼•å·ï¼ˆå¼¯å¼•å·â†’ç›´å¼•å·ï¼‰
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")

    # æ¢å¤LaTeXå‘½ä»¤
    for i, cmd in enumerate(protected):
        text = text.replace(f"@@LATEXCMD{i}@@", cmd)

    # æ¸…ç†ä»£ç å—æ ‡è®°
    text = re.sub(r'```[a-z]*\n?', '', text)
    text = re.sub(r'```', '', text)

    # è½¬æ¢è¡¨æ ¼
    text = convert_ascii_table_blocks(text)
    if '|' in text and '---' in text:
        text = convert_markdown_table_to_latex(text)

    # å¤„ç†ä¸‹åˆ’çº¿
    text = text.replace(r'\_', '@@ESCAPED_UNDERSCORE@@')
    text = re.sub(r'(?<!\\)_(?![{_])', r'\\_', text)
    text = text.replace('@@ESCAPED_UNDERSCORE@@', r'\_')

    return text.strip()


# ==================== é¢˜ç›®è§£æå‡½æ•° ====================

def split_sections(text: str) -> List[Tuple[str, str]]:
    """æ‹†åˆ†ç« èŠ‚ï¼ˆæ”¯æŒ markdown æ ‡é¢˜å’ŒåŠ ç²—æ ¼å¼ï¼‰
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. Markdown æ ‡é¢˜ï¼š# ä¸€ã€å•é€‰é¢˜
    2. åŠ ç²—æ ¼å¼ï¼š**ä¸€ã€å•é€‰é¢˜**
    """
    lines = text.splitlines()
    sections = []
    current_title = None
    current_lines = []

    for line in lines:
        stripped = line.strip()
        # ä¼˜å…ˆåŒ¹é… markdown æ ‡é¢˜æ ¼å¼
        m = re.match(
            r"^#+\s*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)",
            stripped,
        )
        # å¦‚æœä¸åŒ¹é…ï¼Œå°è¯•åŒ¹é…åŠ ç²—æ ¼å¼ **ç« èŠ‚æ ‡é¢˜**
        if not m:
            m = re.match(
                r"^\*\*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)\*\*",
                stripped,
            )
        
        if m:
            if current_title is not None:
                sections.append((current_title, "\n".join(current_lines).strip()))
                current_lines = []
            current_title = m.group(1)
        else:
            if current_title is not None:
                current_lines.append(line)

    if current_title is not None and current_lines:
        sections.append((current_title, "\n".join(current_lines).strip()))

    return sections


def split_questions(section_body: str) -> List[str]:
    """æ‹†åˆ†é¢˜ç›®ï¼ˆæ™ºèƒ½åˆå¹¶ç›¸åŒé¢˜å·ï¼‰
    
    ğŸ†• v1.8.5ï¼šæ”¹è¿›é¢˜ç›®æ‹†åˆ†é€»è¾‘ï¼Œé¿å…å°†è§£ç­”é¢˜çš„å°é—®è¯¯è¯†åˆ«ä¸ºæ–°é¢˜
    - åªåœ¨é¢˜å·è¿ç»­é€’å¢æ—¶æ‰æ‹†åˆ†æ–°é¢˜
    - ç›¸åŒé¢˜å·æˆ–é¢˜å·ä¸è¿ç»­çš„è¡Œä¸ä¼šè¢«æ‹†åˆ†ï¼ˆå¯èƒ½æ˜¯å°é—®ï¼‰
    
    ä¿®å¤ï¼šè¿ç»­ç›¸åŒé¢˜å·çš„å†…å®¹ä¼šåˆå¹¶åˆ°ä¸€ä¸ªé¢˜ç›®ä¸­
    ä¾‹å¦‚ï¼š17. é¢˜å¹²  17. (1)...  17. (2)... â†’ åˆå¹¶ä¸ºä¸€é¢˜
    """
    lines = section_body.splitlines()
    blocks = []
    current = []
    last_question_num = 0  # è®°å½•ä¸Šä¸€é¢˜çš„é¢˜å·ï¼Œåˆå§‹ä¸º0

    def flush():
        if current:
            blocks.append("\n".join(current).strip())
            current.clear()

    for line in lines:
        stripped = line.strip()
        # åŒ¹é…é¢˜å·ï¼š1. æˆ– 1ï¼ æˆ– 1ã€
        match = re.match(r"^(\d+)[\.ï¼ã€]\s*", stripped)
        if match:
            num = int(match.group(1))
            # åªæœ‰åœ¨é¢˜å·è¿ç»­é€’å¢æ—¶æ‰è®¤ä¸ºæ˜¯æ–°é¢˜
            # æˆ–è€…æ˜¯ç¬¬ä¸€é¢˜ï¼ˆlast_question_num == 0ï¼‰
            if last_question_num == 0 or num == last_question_num + 1:
                flush()
                last_question_num = num
                current.append(line)
            else:
                # é¢˜å·ä¸è¿ç»­ï¼ˆåŒ…æ‹¬ç›¸åŒé¢˜å·ï¼‰ï¼Œå¯èƒ½æ˜¯å°é—®æ ‡å·ï¼Œä¸æ‹†åˆ†
                current.append(line)
        else:
            current.append(line)

    flush()
    return blocks


def extract_context_around_image(text: str, img_match_start: int, img_match_end: int,
                                  context_len: int = 50) -> Tuple[str, str]:
    """æå–å›¾ç‰‡å‰åçš„ä¸Šä¸‹æ–‡æ–‡æœ¬

    Args:
        text: å®Œæ•´æ–‡æœ¬
        img_match_start: å›¾ç‰‡åŒ¹é…çš„èµ·å§‹ä½ç½®
        img_match_end: å›¾ç‰‡åŒ¹é…çš„ç»“æŸä½ç½®
        context_len: ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰

    Returns:
        (context_before, context_after) å…ƒç»„
    """
    # æå–å‰æ–‡
    before_start = max(0, img_match_start - context_len)
    context_before = text[before_start:img_match_start].strip()
    # æ¸…ç†æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    context_before = ' '.join(context_before.split())

    # æå–åæ–‡
    after_end = min(len(text), img_match_end + context_len)
    context_after = text[img_match_end:after_end].strip()
    context_after = ' '.join(context_after.split())

    return context_before, context_after


def extract_meta_and_images(block: str, question_index: int = 0, slug: str = "") -> Tuple[str, Dict, List, List]:
    r"""æå–å…ƒä¿¡æ¯ã€å›¾ç‰‡ä¸é™„ä»¶ï¼ˆçŠ¶æ€æœºé‡æ„ï¼šé˜²æ­¢è·¨é¢˜ç´¯ç§¯ï¼‰

    ğŸ†• v1.9: æ–°å¢é™„ä»¶è¯†åˆ«ä¸æå–
    ğŸ†• æ–°å¢å‚æ•°ï¼šquestion_index å’Œ slug ç”¨äºç”Ÿæˆå›¾ç‰‡ ID

    ç›®æ ‡ï¼šé¿å…ä¸Šä¸€é¢˜çš„å¤šè¡Œã€è¯¦è§£ã€‘/ã€åˆ†æã€‘é”™è¯¯åå¹¶ä¸‹ä¸€é¢˜é¢˜å¹²ã€‚
    å…³é”®è¾¹ç•Œï¼š
      - æ–°çš„ meta å¼€å§‹ï¼ˆç­”æ¡ˆ/éš¾åº¦/çŸ¥è¯†ç‚¹/è¯¦è§£/åˆ†æï¼‰
      - é¢˜å·å¼€å§‹ï¼š^\s*>?\s*(?:\d+[\.ï¼ã€]\s+|ï¼ˆ\d+ï¼‰\s+|\d+\)\s+)
      - ç« èŠ‚æ ‡é¢˜ï¼š^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$
      - ç©ºè¡Œ + lookahead ä¸ºé¢˜å·æ—¶ï¼Œä½œä¸ºå®‰å…¨è¾¹ç•Œï¼ˆè‹¥ä¸Šä¸€è¡Œåƒç¯å¢ƒç»­è¡Œåˆ™è·³è¿‡è¯¥ç©ºè¡Œè¾¹ç•Œï¼‰
      - å¼•è¿°ç©ºè¡Œ ^>\s*$ å¿½ç•¥
      - ğŸ†• é™„ä»¶æ ‡è®°ï¼š^é™„[:ï¼š]ã€^é™„è¡¨ã€^å‚è€ƒæ•°æ®è¡¨

    Returns:
        (content, meta, images, attachments) å››å…ƒç»„
        attachments: List[Dict] åŒ…å« kind, lines å­—æ®µ
    """
    # è§„èŒƒåŒ–å¹¶åˆ‡åˆ†è¡Œ
    lines = block.splitlines()

    # ç»“æœå®¹å™¨
    meta = {k: "" for k in META_PATTERNS}
    # ğŸ†• ä¿®å¤ï¼šanalysis å•ç‹¬å­˜åœ¨ï¼Œåç»­ä¼šè¢«ä¸¢å¼ƒ
    meta_alias_map = {
        "analysis": "analysis",  # analysis å•ç‹¬å­˜åœ¨ï¼Œåé¢ä¼šè¢«ä¸¢å¼ƒ
        "explain": "explain",
        "answer": "answer",
        "difficulty": "difficulty",
        "topics": "topics",
    }

    content_lines: List[str] = []
    images: List[Dict] = []
    attachments: List[Dict] = []  # ğŸ†• é™„ä»¶åˆ—è¡¨

    # ç¼–è¯‘è¾¹ç•Œæ­£åˆ™ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šé¢˜å·æ ¼å¼å’Œç« èŠ‚æ ‡é¢˜ï¼‰
    question_start_perm = re.compile(r"^\s*>?\s*(?:\d{1,3}[\.ï¼ã€]\s+|ï¼ˆ\d{1,3}ï¼‰\s+|\d{1,3}\)\s+)")
    section_header = re.compile(r"^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$")
    quote_blank = re.compile(r"^>\s*$")
    env_cont_hint = re.compile(r"(\\\\\s*$)|\\begin\{|\\left|\\right")

    # ğŸ†• v1.9: é™„ä»¶æ ‡è®°æ­£åˆ™
    attachment_start = re.compile(r"^(é™„[:ï¼š]|é™„è¡¨|å‚è€ƒæ•°æ®è¡¨)")
    markdown_table_line = re.compile(r"^\s*\|.*\|.*$")  # Markdown è¡¨æ ¼è¡Œ
    box_drawing_chars = re.compile(r"[â”‚â”€â”Œâ”â””â”˜â”¼â”œâ”¤â”¬â”´]")  # Box-drawing å­—ç¬¦

    # ğŸ†• ä¿®å¤ï¼šå°† META_PATTERNS ç¼–è¯‘ï¼Œåˆ†ç¦» analysis å’Œ explain
    meta_starts = [
        ("answer", re.compile(r"^ã€\s*ç­”æ¡ˆ\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("difficulty", re.compile(r"^ã€\s*éš¾åº¦\s*ã€‘[:ï¼š]?\s*([\d.]+).*")),
        ("topics", re.compile(r"^ã€\s*(çŸ¥è¯†ç‚¹|è€ƒç‚¹)\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("analysis", re.compile(r"^ã€\s*åˆ†æ\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("explain", re.compile(r"^ã€\s*è¯¦è§£\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("diangjing", re.compile(r"^ã€\s*ç‚¹ç›\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("dianjing_alt", re.compile(r"^ã€\s*ç‚¹è¯„\s*ã€‘[:ï¼š]?\s*(.*)$")),
    ]

    # çŠ¶æ€
    state = "NORMAL"  # or "IN_META" or "IN_ATTACHMENT"
    current_meta_key: Optional[str] = None
    current_meta_lines: List[str] = []

    # ğŸ†• v1.9: é™„ä»¶çŠ¶æ€
    current_attachment_lines: List[str] = []
    current_attachment_kind: Optional[str] = None  # "table", "text", "figure"

    def flush_meta():
        nonlocal current_meta_key, current_meta_lines
        if current_meta_key is None:
            return
        # å½’ä¸€åŒ–åˆ°åˆ«åé”®
        key = meta_alias_map.get(current_meta_key, current_meta_key)
        # ğŸ†• ä¿®å¤ï¼šé‡åˆ° analysis/diangjing/dianjing_alt æ—¶ç›´æ¥ä¸¢å¼ƒ
        if key in ("analysis", "diangjing", "dianjing_alt"):
            # è¯´æ˜è¿™æ˜¯ã€åˆ†æã€‘/ã€ç‚¹ç›ã€‘/ã€ç‚¹è¯„ã€‘æ®µï¼Œç›´æ¥èˆå¼ƒï¼Œä¸å†™å…¥ meta å­—å…¸
            current_meta_key = None
            current_meta_lines = []
            return
        # åˆå¹¶æ¸…ç†
        text = "\n".join(current_meta_lines)
        # å»æ‰å¯èƒ½æ®‹ç•™çš„æ ‡ç­¾å‰ç¼€
        text = re.sub(r"^ã€?(?:ç­”æ¡ˆ|éš¾åº¦|çŸ¥è¯†ç‚¹|è€ƒç‚¹|è¯¦è§£|åˆ†æ)ã€‘?[:ï¼š]?\s*", "", text)
        # å¯¹äº explain å­—æ®µï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼ˆä¸æŠ˜å ç©ºè¡Œï¼‰ï¼Œè®©åç»­ remove_par_breaks_in_explain å¤„ç†
        # å…¶ä»–å­—æ®µå‹ç¼©ç©ºè¡Œ
        if key != "explain":
            text = re.sub(r"\n\s*\n+", "\n", text)
        # åˆå¹¶ï¼šè‹¥å·²æœ‰ explainï¼Œåˆ™è¿½åŠ ä¸€è¡Œ
        if key == "explain" and meta.get("explain"):
            meta["explain"] = (meta["explain"] + "\n" + text.strip()).strip()
        else:
            meta[key] = text.strip()
        # é‡ç½®
        current_meta_key = None
        current_meta_lines = []

    def flush_attachment():
        """ğŸ†• v1.9: åˆ·æ–°é™„ä»¶ç¼“å†²åŒº"""
        nonlocal current_attachment_lines, current_attachment_kind
        if not current_attachment_lines or current_attachment_kind is None:
            current_attachment_lines = []
            current_attachment_kind = None
            return

        # æ·»åŠ åˆ°é™„ä»¶åˆ—è¡¨
        attachments.append({
            "kind": current_attachment_kind,
            "lines": current_attachment_lines.copy()
        })

        # é‡ç½®
        current_attachment_lines = []
        current_attachment_kind = None

    def is_question_start(s: str) -> bool:
        return bool(question_start_perm.match(s))

    def is_section_header(s: str) -> bool:
        return bool(section_header.match(s))

    def image_match(s: str):
        # ä¼˜å…ˆåŒ¹é…å¸¦IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_WITH_ID.search(s)
        if m:
            return ('with_id', m)
        # ç„¶ååŒ¹é…æ— IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_NO_ID.search(s)
        if m:
            return ('no_id', m)
        # æœ€åå°è¯•æ—§ç‰ˆç®€å•æ ¼å¼
        m = IMAGE_PATTERN.search(s)
        if m:
            return ('simple', m)
        return None

    # æŸ¥æ‰¾ä¸Šä¸€æ¡éç©ºè¡Œï¼ˆç”¨äºç¯å¢ƒç»­è¡Œåˆ¤æ–­ï¼‰
    def find_prev_nonempty(idx: int) -> Optional[str]:
        j = idx - 1
        while j >= 0:
            if lines[j].strip():
                return lines[j]
            j -= 1
        return None

    # æŸ¥æ‰¾ä¸‹ä¸€æ¡éç©ºè¡Œï¼ˆç”¨äº blank+lookahead åˆ¤æ–­ï¼‰
    def find_next_nonempty(idx: int) -> Optional[str]:
        j = idx + 1
        while j < len(lines):
            if lines[j].strip():
                return lines[j]
            j += 1
        return None

    i = 0
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # ğŸ†• v1.6.2ï¼šå›¾ç‰‡è¡Œè¯†åˆ«å¢å¼º - åŒºåˆ†ç‹¬ç«‹å›¾ç‰‡å— vs å†…è”å…¬å¼å›¾ç‰‡
        # åªæœ‰å½“å›¾ç‰‡"ç‹¬å ä¸€è¡Œ"ä¸”æ˜¯å®Œæ•´åŒ¹é…æ—¶ï¼Œæ‰æå–ä¸ºå›¾ç‰‡å—
        # å†…è”å›¾ç‰‡ï¼ˆå¦‚ "å·²çŸ¥é›†åˆ![](image2.wmf)ï¼Œåˆ™..."ï¼‰ä¿ç•™åœ¨æ–‡æœ¬ä¸­
        # ğŸ†• Prompt 3: ç»Ÿä¸€å¤„ç†æ‰€æœ‰å›¾ç‰‡ï¼ˆç‹¬ç«‹å’Œå†…è”ï¼‰
        # æ£€æŸ¥æ•´è¡Œæ˜¯å¦åŒ…å«å›¾ç‰‡æ ‡è®°
        img_result = image_match(line)  # æ³¨æ„ï¼šä½¿ç”¨å®Œæ•´è¡Œè€Œéstripped
        if img_result:
            img_type, m_img = img_result
            # æ£€æŸ¥æ˜¯å¦ä¸ºç‹¬ç«‹å›¾ç‰‡è¡Œï¼šæ•´è¡Œåªæœ‰ä¸€ä¸ªå›¾ç‰‡æ ‡è®°
            is_standalone = (m_img.group(0).strip() == stripped)

            # ğŸ†• ç”Ÿæˆå›¾ç‰‡ ID å’Œæå–ä¸Šä¸‹æ–‡
            img_counter = len(images) + 1
            generated_id = f"{slug}-Q{question_index}-img{img_counter}" if slug else f"Q{question_index}-img{img_counter}"

            # æå–ä¸Šä¸‹æ–‡
            full_text = "\n".join(lines)
            img_start = full_text.find(m_img.group(0))
            img_end = img_start + len(m_img.group(0))
            context_before, context_after = extract_context_around_image(full_text, img_start, img_end)

            if is_standalone:
                # ç‹¬ç«‹å›¾ç‰‡å—ï¼šæå–åˆ°imagesåˆ—è¡¨
                if img_type == 'with_id':
                    # ![@@@id](path){...}
                    img_id = m_img.group(1)
                    path = m_img.group(2).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                elif img_type == 'no_id':
                    # ![](path){...}
                    path = m_img.group(1).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                else:
                    # ç®€å•æ ¼å¼: ![](images/...)
                    path = m_img.group(1)
                    width = int(m_img.group(2)) if m_img.group(2) else 60
                    images.append({
                        "path": path,
                        "width": width,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                i += 1
                continue
            else:
                # å†…è”å›¾ç‰‡ï¼šæ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œè®°å½•åˆ°imagesåˆ—è¡¨
                if img_type == 'with_id':
                    img_id = m_img.group(1)
                    path = m_img.group(2).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                elif img_type == 'no_id':
                    path = m_img.group(1).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                else:
                    path = m_img.group(1)
                    width = int(m_img.group(2)) if m_img.group(2) else 60
                    images.append({
                        "path": path,
                        "width": width,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })

                # æ›¿æ¢å›¾ç‰‡æ ‡è®°ä¸ºå ä½ç¬¦ï¼ˆä½¿ç”¨æ–°çš„ ID æ ¼å¼ï¼‰
                line = line.replace(m_img.group(0), f"<<IMAGE_INLINE:{generated_id}>>")
                # ç»§ç»­å¤„ç†è¯¥è¡Œï¼ˆfallthroughï¼‰

        # å¼•è¿°ç©ºè¡Œï¼šä¸¢å¼ƒ
        if quote_blank.match(stripped):
            i += 1
            continue

        if state == "NORMAL":
            # ğŸ†• v1.9: æ£€æµ‹é™„ä»¶å¼€å§‹
            if attachment_start.match(stripped):
                # è¿›å…¥é™„ä»¶çŠ¶æ€
                state = "IN_ATTACHMENT"
                current_attachment_lines = [line]
                # åˆ¤æ–­é™„ä»¶ç±»å‹ï¼ˆåˆæ­¥ï¼‰
                if "è¡¨" in stripped or markdown_table_line.match(stripped):
                    current_attachment_kind = "table"
                elif box_drawing_chars.search(stripped):
                    current_attachment_kind = "table"
                else:
                    current_attachment_kind = "text"
                i += 1
                continue

            # æ–°çš„ meta å¼€å§‹ï¼Ÿ
            started = False
            for key, pat in meta_starts:
                m = pat.match(stripped)
                if m:
                    state = "IN_META"
                    current_meta_key = key
                    seed = m.group(m.lastindex or 1) if m.groups() else ""
                    current_meta_lines = [seed.strip()] if seed.strip() else []
                    started = True
                    break
            if started:
                i += 1
                continue

            # æ™®é€šå†…å®¹
            content_lines.append(line)
            i += 1
            continue

        elif state == "IN_META":
            # state == IN_META
            # 1) æ–° meta å¼€å§‹ -> åˆ·æ–°å¹¶åˆ‡æ¢
            started = False
            for key, pat in meta_starts:
                m = pat.match(stripped)
                if m:
                    flush_meta()
                    state = "IN_META"
                    current_meta_key = key
                    seed = m.group(m.lastindex or 1) if m.groups() else ""
                    current_meta_lines = [seed.strip()] if seed.strip() else []
                    started = True
                    break
            if started:
                i += 1
                continue

            # 2) ç¡®è®¤é¢˜å·æˆ–ç« èŠ‚è¾¹ç•Œ -> ç»“æŸ metaï¼Œä¿ç•™è¯¥è¡Œç»™é¢˜å¹²
            if is_question_start(stripped) or is_section_header(stripped):
                flush_meta()
                state = "NORMAL"
                content_lines.append(line)
                i += 1
                continue

            # 3) ç©ºè¡Œ + lookahead ä¸ºé¢˜å· -> å®‰å…¨åœ°ç»“æŸ meta
            if stripped == "":
                next_ne = find_next_nonempty(i)
                if next_ne and is_question_start(next_ne.strip()):
                    prev_ne = find_prev_nonempty(i)
                    # è‹¥ä¸Šä¸€éç©ºè¡Œçœ‹èµ·æ¥æ˜¯ç¯å¢ƒç»­è¡Œï¼Œåˆ™ä¸è¦åœ¨æ­¤ç©ºè¡Œåˆ‡æ–­
                    if prev_ne and env_cont_hint.search(prev_ne):
                        # ç»§ç»­æŠŠç©ºè¡Œä¹Ÿå¹¶å…¥ metaï¼ˆä¿æŒåŸæ ·ï¼‰
                        current_meta_lines.append(line)
                        i += 1
                        continue
                    # å¦åˆ™åˆ‡æ–­ metaï¼ˆä¸æ¶ˆè´¹ç©ºè¡Œï¼‰
                    flush_meta()
                    state = "NORMAL"
                    i += 1  # è·³è¿‡è¯¥ç©ºè¡Œï¼Œä¸‹ä¸€è½®çœ‹åˆ°é¢˜å·è¡Œä¼šè¿›å…¥ NORMAL æµç¨‹
                    continue

            # 4) ç»§ç»­ç´¯ç§¯ meta å†…å®¹
            current_meta_lines.append(line)
            i += 1
            continue

        elif state == "IN_ATTACHMENT":
            # é™„ä»¶çŠ¶æ€å¤„ç†
            # 1) æ–° meta å¼€å§‹ -> åˆ·æ–°é™„ä»¶å¹¶åˆ‡æ¢åˆ° meta
            started = False
            for key, pat in meta_starts:
                m = pat.match(stripped)
                if m:
                    flush_attachment()
                    state = "IN_META"
                    current_meta_key = key
                    seed = m.group(m.lastindex or 1) if m.groups() else ""
                    current_meta_lines = [seed.strip()] if seed.strip() else []
                    started = True
                    break
            if started:
                i += 1
                continue

            # 2) ç¡®è®¤é¢˜å·æˆ–ç« èŠ‚è¾¹ç•Œ -> ç»“æŸé™„ä»¶ï¼Œä¿ç•™è¯¥è¡Œç»™é¢˜å¹²
            if is_question_start(stripped) or is_section_header(stripped):
                flush_attachment()
                state = "NORMAL"
                content_lines.append(line)
                i += 1
                continue

            # 3) ç©ºè¡Œ - å¯èƒ½ç»“æŸé™„ä»¶
            if stripped == "":
                next_ne = find_next_nonempty(i)
                # å¦‚æœä¸‹ä¸€è¡Œæ˜¯é¢˜å·ã€metaæ ‡è®°æˆ–ç« èŠ‚æ ‡é¢˜ï¼Œåˆ™ç»“æŸé™„ä»¶
                if next_ne and (is_question_start(next_ne.strip()) or
                               is_section_header(next_ne.strip()) or
                               any(pat.match(next_ne.strip()) for _, pat in meta_starts)):
                    flush_attachment()
                    state = "NORMAL"
                    i += 1
                    continue
                # å¦åˆ™ç»§ç»­ç´¯ç§¯ï¼ˆå¯èƒ½æ˜¯é™„ä»¶å†…çš„ç©ºè¡Œï¼‰
                current_attachment_lines.append(line)
                i += 1
                continue

            # 4) ç»§ç»­ç´¯ç§¯é™„ä»¶å†…å®¹
            # åŠ¨æ€æ›´æ–°é™„ä»¶ç±»å‹
            if markdown_table_line.match(stripped):
                current_attachment_kind = "table"
            elif box_drawing_chars.search(stripped):
                current_attachment_kind = "table"

            current_attachment_lines.append(line)
            i += 1
            continue

    # å¾ªç¯ç»“æŸï¼Œè‹¥è¿˜åœ¨ meta æˆ– attachment çŠ¶æ€åˆ™åˆ·æ–°
    if state == "IN_META":
        flush_meta()
    elif state == "IN_ATTACHMENT":
        flush_attachment()

    content = "\n".join(content_lines).strip()
    return content, meta, images, attachments


def parse_question_structure(content: str) -> Dict:
    """æ™ºèƒ½è¯†åˆ«é¢˜ç›®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    è§£æé¢˜å¹²ã€é€‰é¡¹ã€è§£æä¸‰éƒ¨åˆ†ï¼Œé¿å…å°†è§£ææ–‡æœ¬æ··å…¥é€‰é¡¹
    """
    lines = content.splitlines()
    
    structure = {
        'stem_lines': [],
        'choices': [],
        'analysis_lines': [],
        'in_choice': False,
        'in_analysis': False,
        'current_choice': '',
        'skip_analysis_block': False,
    }
    
    choice_pattern = re.compile(r'^([A-D])[\.ï¼ã€]\s*(.*)$')
    analysis_marker = re.compile(r'^ã€?\s*åˆ†æ\s*ã€‘[:ï¼š]?')
    explain_marker = re.compile(r'^ã€?\s*è¯¦è§£\s*ã€‘[:ï¼š]?\s*(.*)$')
    
    for line in lines:
        stripped = line.strip()
        normalized = re.sub(r'^>+\s*', '', stripped)

        if structure['skip_analysis_block']:
            if normalized.startswith('ã€'):
                structure['skip_analysis_block'] = False
            else:
                continue

        # ğŸ†• ä¿®å¤ï¼šåªåœ¨é‡åˆ°ã€è¯¦è§£ã€‘æ—¶è¿›å…¥è§£ææ¨¡å¼ï¼Œé‡åˆ°ã€åˆ†æã€‘æ—¶è·³è¿‡
        if analysis_marker.match(normalized):
            structure['in_choice'] = False
            structure['in_analysis'] = False
            structure['skip_analysis_block'] = True
            continue

        explain_match = explain_marker.match(normalized)
        if explain_match:
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
                structure['current_choice'] = ''
            structure['in_choice'] = False
            structure['in_analysis'] = True
            remainder = explain_match.group(1).strip()
            if remainder:
                structure['analysis_lines'].append(remainder)
            continue

        # åŒ¹é…é€‰é¡¹æ ‡è®° (A. B. C. D.)
        m = choice_pattern.match(normalized)
        if m:
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
            
            structure['current_choice'] = m.group(2)
            structure['in_choice'] = True
            structure['in_analysis'] = False
            continue
        
        # æ ¹æ®å½“å‰çŠ¶æ€åˆ†é…è¡Œ
        if structure['in_analysis']:
            structure['analysis_lines'].append(line)
        elif structure['in_choice']:
            structure['current_choice'] += ' ' + normalized
        else:
            structure['stem_lines'].append(line)
    
    if structure['current_choice']:
        structure['choices'].append(structure['current_choice'].strip())
    
    return structure


def split_inline_choice_line(line: str) -> List[str]:
    """å°†å•è¡Œå¤šé€‰é¡¹ï¼ˆå« $$ æ•°å­¦å…¬å¼ï¼‰æ‹†æˆç‹¬ç«‹å­—ç¬¦ä¸²

    ä¿®å¤ P0-002: ä½¿ç”¨ä¿æŠ¤-åˆ†å‰²-æ¢å¤ç­–ç•¥ï¼Œé¿å…æ•°å­¦å…¬å¼å¹²æ‰°é€‰é¡¹åˆ†å‰²
    """
    text = re.sub(r'^>+\s*', '', line.strip())
    if not text:
        return []

    # æ­¥éª¤1: ä¿æŠ¤æ•°å­¦å…¬å¼
    math_blocks = []
    def save_math(match):
        math_blocks.append(match.group(0))
        return f'@@MATH{len(math_blocks)-1}@@'

    # ä¿æŠ¤æ‰€æœ‰æ•°å­¦æ¨¡å¼ï¼š$$...$$, $...$, \(...\), \[...\]
    protected = re.sub(r'\$\$[^$]+\$\$|\$[^$]+\$|\\[()\[].*?\\[)\]]', save_math, text, flags=re.DOTALL)

    # æ­¥éª¤2: ä½¿ç”¨ finditer æ‰¾åˆ°æ‰€æœ‰é€‰é¡¹æ ‡è®°åŠå…¶ä½ç½®
    option_pattern = re.compile(r'([A-D][ï¼.])\s*')
    matches = list(option_pattern.finditer(protected))

    if not matches:
        return []

    # æ­¥éª¤3: æå–æ¯ä¸ªé€‰é¡¹çš„å†…å®¹
    segments: List[str] = []
    for i, match in enumerate(matches):
        option_marker = match.group(1)
        start = match.end()

        # ç¡®å®šå†…å®¹ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªé€‰é¡¹æ ‡è®°çš„å¼€å§‹ï¼Œæˆ–å­—ç¬¦ä¸²æœ«å°¾ï¼‰
        if i + 1 < len(matches):
            end = matches[i + 1].start()
        else:
            end = len(protected)

        # æå–é€‰é¡¹å†…å®¹
        content = protected[start:end].strip()

        # æ¢å¤æ•°å­¦å…¬å¼
        for j, block in enumerate(math_blocks):
            content = content.replace(f'@@MATH{j}@@', block)

        segments.append(f'{option_marker} {content}')

    return segments


def expand_inline_choices(content: str) -> str:
    """å±•å¼€å•è¡Œ/å¤šè¡Œå¼•è¿°é€‰é¡¹å¹¶å»é™¤'>'å‰ç¼€"""
    output_lines: List[str] = []
    pending_block: List[str] = []

    def flush_pending():
        nonlocal pending_block
        if not pending_block:
            return

        normalized = " ".join(re.sub(r'^>+\s*', '', ln).strip() for ln in pending_block if ln.strip())
        marker_count = len(re.findall(r'[A-D][ï¼\.\ã€]', normalized))
        if marker_count >= 2:
            expanded = split_inline_choice_line(normalized)
            if expanded:
                output_lines.extend(expanded)
            else:
                output_lines.extend(pending_block)
        elif marker_count == 1:
            expanded = split_inline_choice_line(normalized)
            if expanded:
                output_lines.extend(expanded)
            else:
                output_lines.extend(pending_block)
        else:
            output_lines.extend(pending_block)
        pending_block = []

    for line in content.splitlines():
        stripped = line.strip()
        if stripped.startswith('>'):
            pending_block.append(line)
            continue

        flush_pending()
        output_lines.append(line)

    flush_pending()
    return '\n'.join(output_lines)


def convert_choices(content: str) -> Tuple[str, List[str], str]:
    """æ‹†åˆ†é¢˜å¹²ã€é€‰é¡¹ã€è§£æï¼ˆå¢å¼ºç‰ˆï¼‰
    
    ğŸ†• v1.4 æ”¹è¿›ï¼šå…ˆå±•å¼€å•è¡Œé€‰é¡¹å†è§£æ
    """
    # ğŸ†• å…ˆå±•å¼€å¯èƒ½çš„å•è¡Œé€‰é¡¹
    content = expand_inline_choices(content)
    
    structure = parse_question_structure(content)
    
    stem = '\n'.join(structure['stem_lines']).strip()
    stem = re.sub(r"^\s*\d+[\.ï¼ã€]\s*", "", stem)
    
    # æå–çš„è§£æå†…å®¹
    analysis = '\n'.join(structure['analysis_lines']).strip()
    
    return stem, structure['choices'], analysis


def handle_subquestions(content: str) -> str:
    r"""å¤„ç†è§£ç­”é¢˜çš„å°é¢˜ç¼–å·

    ğŸ†• v1.7ï¼šç»Ÿä¸€å°é—®ç¼–å·æ ¼å¼ï¼Œä¸æ·»åŠ  \mathrm
    ğŸ†• v1.9ï¼šä¿ç•™é¢˜å¹²å‰å¯¼æ–‡æœ¬å¹¶è‡ªåŠ¨åŒ…è£¹ enumerate
    """
    if not re.search(r'\(\d+\)', content):
        return content

    parts = re.split(r'\((\d+)\)', content)
    if len(parts) < 3:
        return content

    prefix = parts[0].strip()
    subquestions = []
    for i in range(1, len(parts), 2):
        if i + 1 >= len(parts):
            break
        num = parts[i]
        body = parts[i + 1].strip()
        if body:
            subquestions.append((num, body))

    if len(subquestions) < 2:
        return content

    result_lines: List[str] = []
    if prefix:
        result_lines.append(prefix)

    result_lines.append(r"\begin{enumerate}[label=(\arabic*)]")
    for _, content_text in subquestions:
        result_lines.append(f"  \\item {content_text}")
    result_lines.append(r"\end{enumerate}")

    return '\n'.join(result_lines)


def ensure_choices_environment(lines: List[str], has_options: bool) -> List[str]:
    r"""å¦‚æœå­˜åœ¨ \item ä½†ç¼ºå°‘ choices ç¯å¢ƒï¼Œåˆ™è‡ªåŠ¨è¡¥å……"""
    if not has_options:
        return lines

    has_begin = any(r"\begin{choices}" in line for line in lines)
    has_end = any(r"\end{choices}" in line for line in lines)
    item_indices = [
        idx for idx, line in enumerate(lines)
        if re.match(r'\s*\\item\b', line)
    ]

    if item_indices and not has_begin:
        insert_at = item_indices[0]
        lines.insert(insert_at, r"\begin{choices}")
        item_indices = [idx + 1 if idx >= insert_at else idx for idx in item_indices]
        has_begin = True

    if item_indices and has_begin and not has_end:
        insert_at = item_indices[-1] + 1
        lines.insert(insert_at, r"\end{choices}")

    return lines


def _smart_replace_because_therefore(text: str) -> str:
    """æ™ºèƒ½æ›¿æ¢ âˆµ/âˆ´ ç¬¦å·ä¸º LaTeX å‘½ä»¤
    
    ğŸ†• v1.9.6ï¼šæ ¹æ®ç¬¦å·ä½ç½®å†³å®šæ˜¯å¦éœ€è¦åŒ…è£¹åœ¨æ•°å­¦æ¨¡å¼å†…
    
    è§„åˆ™ï¼š
    1. å¦‚æœ âˆµ/âˆ´ åœ¨ $$...$$ å†…éƒ¨ï¼Œç›´æ¥æ›¿æ¢ä¸º \\because/\\therefore
    2. å¦‚æœ âˆµ/âˆ´ åœ¨ $$...$$ å¤–éƒ¨ï¼Œæ›¿æ¢ä¸º \\(\\because\\)/\\(\\therefore\\)
    
    åˆ¤æ–­æ–¹æ³•ï¼šè®¡ç®—ç¬¦å·å‰çš„ $$ æ•°é‡ï¼Œå¥‡æ•° = åœ¨æ•°å­¦å†…ï¼Œå¶æ•° = åœ¨æ•°å­¦å¤–
    """
    if not text:
        return text
    
    def replace_symbol(symbol: str, latex_cmd: str, text: str) -> str:
        if symbol not in text:
            return text
        
        result = []
        last_pos = 0
        
        for i, char in enumerate(text):
            if char == symbol:
                # è®¡ç®—æ­¤ä½ç½®å‰ $$ çš„æ•°é‡
                before = text[:i]
                dollar_count = before.count('$$')
                
                # å¥‡æ•° = åœ¨æ•°å­¦æ¨¡å¼å†…ï¼Œå¶æ•° = åœ¨æ•°å­¦æ¨¡å¼å¤–
                in_math = dollar_count % 2 == 1
                
                result.append(text[last_pos:i])
                
                if in_math:
                    # åœ¨æ•°å­¦æ¨¡å¼å†…ï¼Œç›´æ¥æ›¿æ¢
                    result.append(f'\\{latex_cmd} ')
                else:
                    # åœ¨æ•°å­¦æ¨¡å¼å¤–ï¼Œéœ€è¦åŒ…è£¹
                    result.append(f'\\(\\{latex_cmd}\\) ')
                
                last_pos = i + 1
        
        result.append(text[last_pos:])
        return ''.join(result)
    
    text = replace_symbol('âˆµ', 'because', text)
    text = replace_symbol('âˆ´', 'therefore', text)
    
    return text


# DEPRECATED: çŠ¶æ€æœºå·²é¿å…è¿™äº›è¡Œå†…å¼‚å¸¸ï¼Œä¿ç•™å…œåº•æµ‹è¯•ä½¿ç”¨
def fix_inline_math_glitches(text: str) -> str:
    """ğŸ†• ä¿®å¤è¡Œå†…æ•°å­¦çš„å„ç§å¼‚å¸¸æ¨¡å¼

    ä¿®å¤ï¼š
    - ç©ºçš„ $$
    - $$$x$ â†’ $x$
    - \therefore$$ â†’ \therefore
    - \because$$ â†’ \because
    """
    if not text:
        return text

    # 1. å»æ‰å®Œå…¨ç©ºçš„ $$
    text = re.sub(r'\$\s*\$', '', text)

    # 2. ä¿®å¤ $$$x$ â†’ $x$
    text = re.sub(r'\$\s*\$\s*(\\\()', r'\1', text)

    # 3. ç‰¹ä¾‹ï¼š\therefore$$ â†’ \therefore
    text = re.sub(r'(\\therefore)\s*\$\s*\$', r'\1', text)

    # 4. ç‰¹ä¾‹ï¼š\because$$ â†’ \because
    text = re.sub(r'(\\because)\s*\$\s*\$', r'\1', text)

    return text


def process_text_for_latex(text: str, is_math_heavy: bool = False) -> str:
    r"""ç»Ÿä¸€å…¥å£ï¼šé¢˜å¹²/é€‰é¡¹/è§£ææ–‡æœ¬çš„ LaTeX å¤„ç†ï¼ˆçŠ¶æ€æœºç‰ˆï¼‰

    é‡æ„ç›®æ ‡ï¼š
    1. ä¿ç•™åŸæœ‰â€œéæ•°å­¦â€æ¸…ç†ä¸è½¬ä¹‰é€»è¾‘ï¼ˆæ•…é€‰/ã€è¯¦è§£ã€‘/OCR è¾¹ç•Œä¿®å¤ç­‰ï¼‰
    2. ç”¨ MathStateMachine å®Œå…¨æ›¿æ¢æ—§çš„ smart_inline_math / sanitize_math ç­‰æ­£åˆ™ç®¡çº¿
    3. æ•°å­¦å®šç•Œç¬¦ç»Ÿä¸€ï¼š$...$ / $$...$$ â†’ \(...\)ï¼Œä¿æŒå·²æœ‰ \(...\) ä¸é‡å¤åŒ…è£¹
    4. åœ¨çŠ¶æ€æœºå¤„ç†ååšè½»é‡å…œåº•æ¸…ç†ï¼ˆç©ºæ•°å­¦å—ã€å›¾ç‰‡å±æ€§æ®‹ç•™ç­‰ï¼‰
    """
    if not text:
        return text

    # ---------- 1. å‰ç½®ï¼šçº¯æ–‡æœ¬/éæ•°å­¦å±‚é¢æ¸…ç†ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰ ----------
    text = re.sub(r'\*\s*(\$[^$]+\$)\s*\*', r'\1', text)  # *$x$* â†’ $x$
    text = re.sub(r'\*([A-Za-z0-9])\*', r'\\emph{\1}', text)  # *x* â†’ \emph{x}

    # "æ•…é€‰" / "æ•…ç­”æ¡ˆä¸º" ç³»åˆ—æ¸…ç†
    text = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    text = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', text)
    text = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', text)
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'[ï¼Œ,]?\s*æ•…é€‰[:ï¼š]\s*[ABCD]+[ã€‚ï¼.]*\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', text)

    # OCR è¾¹ç•Œç•¸å½¢é¢„å¤„ç†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    text = re.sub(r'\\\\right\.\s*\\\\\\\)', r'\\\\right.', text)
    text = re.sub(r'\\\\right\.\\\\\+\)', r'\\\\right.', text)

    # Unicode ç¬¦å·æ›¿æ¢ï¼ˆæ™ºèƒ½å¤„ç† âˆµ/âˆ´ï¼‰
    # ğŸ†• v1.9.6ï¼šæ ¹æ®ç¬¦å·ä½ç½®å†³å®šæ˜¯å¦éœ€è¦åŒ…è£¹åœ¨æ•°å­¦æ¨¡å¼å†…
    if 'âˆµ' in text or 'âˆ´' in text:
        text = _smart_replace_because_therefore(text)

    # éæ•°å­¦æ¨¡å¼ä¸‹çš„ LaTeX ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
    if not is_math_heavy:
        text = escape_latex_special(text, in_math_mode=False)

    # ---------- 2. æ•°å­¦æ¨¡å¼ç»Ÿä¸€ï¼šçŠ¶æ€æœºå¤„ç† ----------
    global math_sm
    text = math_sm.process(text)

    # ---------- 2.5. æ ‡å‡†åŒ–æ•°å­¦ç¬¦å·ï¼ˆåœ¨çŠ¶æ€æœºä¹‹åï¼‰ ----------
    text = standardize_math_symbols(text)

    # ---------- 3. è½»é‡åå¤„ç†ï¼šå¸¸è§ç©ºå—/æ®‹ç•™ä¿®å¤ ----------
    text = fix_common_issues_v2(text)

    # ---------- 3.5. ä¿®å¤æœªé—­åˆçš„æ•°å­¦æ¨¡å¼ ----------
    text = fix_unclosed_math_mode(text)

    return text


def fix_unclosed_math_mode(text: str) -> str:
    """ä¿®å¤æœªé—­åˆçš„æ•°å­¦æ¨¡å¼ï¼ˆå¦‚ \\(text\\)text}ï¼‰

    ä¿®å¤æ¨¡å¼ï¼š
    1. \\)text} â†’ \\)textï¼ˆåˆ é™¤å¤šä½™çš„}ï¼‰
    2. \\(textæœªé—­åˆ â†’ \\(text\\)
    """
    if not text:
        return text

    # æ¨¡å¼1: \\)åé¢è·Ÿç€æ–‡æœ¬å’Œ}ï¼Œåˆ é™¤å¤šä½™çš„}
    # ä¾‹å¦‚: \\)ç›¸äº¤ä½†ä¸è¿‡åœ†å¿ƒ} â†’ \\)ç›¸äº¤ä½†ä¸è¿‡åœ†å¿ƒ
    text = re.sub(r'(\\\))([^}]*?)\}(\s*\\end\{)', r'\1\2\3', text)

    return text


def fix_common_issues_v2(text: str) -> str:
    r"""çŠ¶æ€æœºåçš„å…œåº•çº¯æ–‡æœ¬ä¿®å¤ï¼ˆåªå¤„ç†ä¸æ”¹å˜æ•°å­¦è¯­ä¹‰çš„æ®‹ç•™ï¼‰

    åŒ…å«ï¼š
    - ç©ºçš„è¡Œå†…/æ˜¾ç¤ºæ•°å­¦å— \(\) / \[\] åˆ é™¤
    - \because\(\) / \therefore\(\) æ¸…ç†ä¸ºçº¯å‘½ä»¤
    - OCR äº§ç”Ÿçš„æ•°ç»„è¾¹ç•Œç•¸å½¢ï¼ˆ\right.\\) â†’ \right.\)ï¼‰
    - å›¾ç‰‡æ®‹ä½™å±æ€§æ¸…ç†ï¼ˆåˆ©ç”¨åŸ clean_residual_image_attrsï¼‰
    - å»é™¤å­¤ç«‹çš„é‡å¤æ˜¾ç¤ºå…¬å¼å®šç•Œç¬¦ï¼ˆçŠ¶æ€æœºå·²è§„èŒƒï¼Œå…œåº•é˜²å¾¡ï¼‰
    """
    if not text:
        return text
    # ç©ºæ•°å­¦å—
    text = re.sub(r'\\\(\s*\\\)', '', text)
    text = re.sub(r'\\\[\s*\\\]', '', text)
    # æ¸…ç† \because\(\) / \therefore\(\)
    text = re.sub(r'\\because\s*\\\(\\\)', r'\\because ', text)
    text = re.sub(r'\\therefore\s*\\\(\\\)', r'\\therefore ', text)
    # æ•°ç»„/åˆ†æ®µç­‰ç¯å¢ƒè¾¹ç•Œç•¸å½¢ï¼ˆä¸ complete ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    text = text.replace(r'\right.\\)', r'\right.\)')
    text = text.replace(r'\right)\\)', r'\right)\)')
    # å›¾ç‰‡å±æ€§æ®‹ç•™ï¼ˆå¤ç”¨å·²æœ‰å‡½æ•°ï¼‰
    text = clean_residual_image_attrs(text)
    # ç§»é™¤ä»»ä½•æ®‹ç•™çš„è£¸ $$ï¼ˆçŠ¶æ€æœºåç†è®ºä¸Šä¸ä¼šå‡ºç°ï¼‰
    text = text.replace('$$', '')

    # æ¸…ç†å¤–å±‚å¤šä½™ç¾å…ƒ: $\(x\)$ â†’ \(x\)
    text = re.sub(r'\$(\\\([^$]+?\\\))\$', r'\1', text)
    # æ¸…ç† $\because$ â†’ \because ï¼ˆä»¥åŠ \thereforeï¼‰
    text = re.sub(r'\$(\\because)\$', r'\1', text)
    text = re.sub(r'\$(\\therefore)\$', r'\1', text)
    # æ¸…ç†ç®€å•å˜é‡å½¢å¼ $x$ è‹¥å•å­—ç¬¦ä¸”ä¸åœ¨å·²æœ‰æ•°å­¦ï¼ˆä¿å®ˆï¼šä»…å­—æ¯/æ•°å­—ï¼‰â†’ \(x\)
    def _wrap_simple_var(m: re.Match) -> str:
        var = m.group(1)
        return f'\\({var}\\)'
    text = re.sub(r'(?<!\\)\$([a-zA-Z0-9])\$', _wrap_simple_var, text)
    # å†æ¬¡ç§»é™¤å¯èƒ½äº§ç”Ÿçš„ç©ºæ•°å­¦å— \(\)
    text = re.sub(r'\\\(\s*\\\)', '', text)
    # å»é™¤é—ç•™çš„å­¤ç«‹å•ç¾å…ƒï¼ˆä¸åœ¨é…å¯¹å†…ï¼‰ï¼šåˆ é™¤
    # åŒ¹é…å•ç‹¬ä¸€è¡ŒåªåŒ…å« $ æˆ–è¡Œé¦–/è¡Œæœ«çš„å•ç¾å…ƒ
    text = re.sub(r'(^|\s)(\$)(?=\s|$)', lambda m: m.group(1), text)
    
    # ğŸ”§ v1.9.3: ç§»é™¤ v1.8.6 çš„é”™è¯¯åˆå¹¶é€»è¾‘
    # åŸç­–ç•¥é”™è¯¯åœ°å°† \(D\)å‡åœ¨çƒ\(O\) åˆå¹¶æˆ \(Då‡åœ¨çƒO\)
    # æ­£ç¡®è¡Œä¸ºæ˜¯ä¿æŒå„ä¸ªç‹¬ç«‹å˜é‡çš„æ•°å­¦æ¨¡å¼åˆ†éš”
    # 
    # å·²åˆ é™¤çš„é”™è¯¯ä»£ç ï¼š
    # - ç­–ç•¥1: text = re.sub(r'\\\)([\u4e00-\u9fa5]{1,3})\\\(', r'\1', text)
    # - ç­–ç•¥2: text = re.sub(r'(\\right\.)\\\)...', r'\1\2', text)
    # - ç­–ç•¥3: é‡å¤çš„ç­–ç•¥1
    
    return text


INLINE_MATH_PATTERN = re.compile(r'\\\((.+?)\\\)')
TRAILING_MATH_PUNCT = set('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?ã€;:')
CJK_CHAR_RE = re.compile(
    r'['
    r'\u3400-\u4dbf'
    r'\u4e00-\u9fff'
    r'\uf900-\ufaff'
    r']'
)
CJK_PUNCT_CHARS = set('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€œâ€â€˜â€™ï¼ˆï¼‰ã€Šã€‹ã€ã€‘â€”â€¦â€¦Â·ã€')
MATH_BLACKLIST_TOKENS = [
    '=', '+', '-', '^', '_',
    '\\frac', '\\sum', '\\int', '\\times', '\\div',
    '\\sqrt', '\\log', '\\sin', '\\cos', '\\tan',
]


def _is_cjk_char(ch: str) -> bool:
    return bool(CJK_CHAR_RE.match(ch)) or ch in CJK_PUNCT_CHARS


def _split_trailing_punct(segment: str) -> Tuple[str, str]:
    idx = len(segment)
    while idx > 0 and segment[idx - 1].isspace():
        idx -= 1
    punct_end = idx
    while idx > 0 and segment[idx - 1] in TRAILING_MATH_PUNCT:
        idx -= 1
    core = segment[:idx].rstrip()
    trailing = segment[idx:punct_end]
    return core, trailing


def _should_unwrap_inline_math(content: str) -> bool:
    tokenized = content.strip()
    if not tokenized:
        return True

    filtered = ''.join(ch for ch in tokenized if not ch.isspace())
    if not filtered:
        return True

    cjk_chars = sum(1 for ch in filtered if _is_cjk_char(ch))
    ratio = cjk_chars / len(filtered)
    if ratio <= 0.7:
        return False

    lowered = filtered.lower()
    for token in MATH_BLACKLIST_TOKENS:
        if token in lowered:
            return False

    return True


def postprocess_inline_math(line: str) -> str:
    """æ¸…æ´—å†…è”æ•°å­¦ç¯å¢ƒï¼Œç§»é™¤æ ‡ç‚¹å¹¶æ‹†æ‰çº¯ä¸­æ–‡"""
    if r'\(' not in line:
        return line

    def _replace(match: re.Match) -> str:
        inner = match.group(1)
        core, trailing = _split_trailing_punct(inner)
        normalized = core.strip()

        if not normalized:
            return trailing

        if _should_unwrap_inline_math(normalized):
            return normalized + trailing

        return f"\\({normalized}\\){trailing}"

    return INLINE_MATH_PATTERN.sub(_replace, line)


def validate_brace_balance(tex: str) -> List[str]:
    """ğŸ†• v1.8.6ï¼šå…¨å±€èŠ±æ‹¬å·æ£€æŸ¥ - å¿½ç•¥ \\{ \\} å’Œæ³¨é‡Šï¼Œåªç»Ÿè®¡è£¸ { }

    è¿”å›å½¢å¦‚ï¼š
    - "Line 555: extra '}' (brace balance went negative)"
    - "Global brace imbalance at EOF: balance=..."
    """
    issues: List[str] = []
    balance = 0

    for lineno, raw_line in enumerate(tex.splitlines(), start=1):
        # å»æ‰æ³¨é‡Š
        line = raw_line.split('%', 1)[0]
        # å»æ‰è½¬ä¹‰çš„ \{ \}
        line_wo_esc = re.sub(r'\\[{}]', '', line)

        for ch in line_wo_esc:
            if ch == '{':
                balance += 1
            elif ch == '}':
                balance -= 1
                if balance < 0:
                    issues.append(f"Line {lineno}: extra '}}' (brace balance went negative)")
                    balance = 0

    if balance != 0:
        issues.append(f"Global brace imbalance at EOF: balance={balance}")

    return issues


def validate_math_integrity(tex: str) -> List[str]:
    r"""åˆ†ææœ€ç»ˆ TeX æ•°å­¦å®Œæ•´æ€§é—®é¢˜å¹¶è¿”å›è­¦å‘Šåˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰

    æ£€æŸ¥é¡¹ï¼š
    - è¡Œå†…æ•°å­¦å®šç•Œç¬¦æ•°é‡å·®å¼‚ï¼ˆopens vs closesï¼‰- ğŸ†• v1.8.7ï¼šå¿½ç•¥æ³¨é‡Šä¸­çš„å®šç•Œç¬¦
    - è£¸éœ²ç¾å…ƒç¬¦å·
    - åŒé‡åŒ…è£¹æ®‹ç•™
    - å³è¾¹ç•Œç•¸å½¢ï¼ˆ\right. $$ ç­‰ï¼‰
    - ç©ºæ•°å­¦å—
    - ğŸ†• æˆªæ–­/æœªé—­åˆçš„æ•°å­¦ç‰‡æ®µï¼ˆæ”¶é›†å‰è‹¥å¹²æ ·æœ¬ï¼‰
      å…¸å‹æ¥æºï¼šå›¾ç‰‡å ä½ç¬¦æˆ– explain åˆå¹¶æ—¶è·¨è¡Œè¢«æˆªæ–­ï¼Œå¯¼è‡´ç¼ºå¤± \)
    - ğŸ†• v1.8.7ï¼šæ£€æµ‹ \) åœ¨ \( å‰é¢çš„åå‘æ¨¡å¼
    """
    issues: List[str] = []
    tex_no_comments_lines: List[str] = []
    for raw_line in tex.splitlines():
        tex_no_comments_lines.append(raw_line.split('%', 1)[0])
    tex_no_comments = "\n".join(tex_no_comments_lines)

    # ğŸ†• v1.8.7ï¼šç»Ÿè®¡æ—¶å¿½ç•¥æ³¨é‡Šä¸­çš„å®šç•Œç¬¦
    opens = 0
    closes = 0
    left_total = 0
    right_total = 0
    left_right_samples: List[str] = []
    reversed_pairs: List[Tuple[int, str]] = []  # (line_num, line_content)

    for lineno, code_part in enumerate(tex_no_comments_lines, start=1):
        line_opens = code_part.count('\\(')
        line_closes = code_part.count('\\)')
        line_left = code_part.count('\\left')
        line_right = code_part.count('\\right')
        opens += line_opens
        closes += line_closes
        left_total += line_left
        right_total += line_right

        if line_left != line_right and (line_left or line_right):
            snippet = code_part.strip()
            if len(snippet) > 80:
                snippet = snippet[:77] + '...'
            left_right_samples.append(
                f"Line {lineno}: \\left={line_left}, \\right={line_right} â†’ {snippet}"
            )

        if line_opens >= 1 and line_closes >= 1:
            idx_open = code_part.find(r'\(')
            idx_close = code_part.find(r'\)')
            if idx_close < idx_open:
                display_line = code_part.strip()
                if len(display_line) > 80:
                    display_line = display_line[:77] + '...'
                reversed_pairs.append((lineno, display_line))

    if opens != closes:
        issues.append(f"Math delimiter imbalance: opens={opens} closes={closes} diff={opens - closes}")
    if left_total != right_total:
        issues.append(f"\\left/\\right imbalance: left={left_total}, right={right_total}")
        if left_right_samples:
            issues.extend(left_right_samples[:5])

    stray = len(re.findall(r'(?<!\\)\$', tex_no_comments))
    if stray:
        issues.append(f"Stray dollar signs detected: {stray}")

    double_wrapped = (
        len(re.findall(r'\$\s*\\\(.*?\\\)\s*\$', tex_no_comments, flags=re.DOTALL)) +
        len(re.findall(r'\$\$\s*\\\(.*?\\\)\s*\$\$', tex_no_comments, flags=re.DOTALL))
    )
    if double_wrapped:
        issues.append(f"Double-wrapped math segments: {double_wrapped}")

    right_glitch = (
        len(re.findall(r'\\right\.\s*\$\$', tex_no_comments)) +
        len(re.findall(r'\\right\.\\\\\)', tex_no_comments))
    )
    if right_glitch:
        issues.append(f"Right boundary glitches: {right_glitch}")

    empty_math = (
        len(re.findall(r'\\\(\s*\\\)', tex_no_comments)) +
        len(re.findall(r'\\\[\s*\\\]', tex_no_comments))
    )
    if empty_math:
        issues.append(f"Empty math blocks: {empty_math}")

    unmatched_open_positions: List[int] = []
    unmatched_close_positions: List[int] = []

    token_iter = list(re.finditer(r'(\\\(|\\\))', tex_no_comments))
    stack: List[int] = []
    for m in token_iter:
        tok = m.group(0)
        pos = m.start()
        if tok == '\\(':
            stack.append(pos)
        else:
            if stack:
                stack.pop()
            else:
                unmatched_close_positions.append(pos)
    unmatched_open_positions.extend(stack)

    def _sample_at(pos: int, direction: str = 'forward', span: int = 140) -> str:
        if direction == 'forward':
            raw = tex_no_comments[pos:pos+span]
        else:
            start = max(0, pos-span)
            raw = tex_no_comments[start:pos+10]
        end_delim = raw.find('\\)')
        if end_delim != -1:
            raw = raw[:end_delim+2]
        raw = re.sub(r'\s+', ' ', raw).strip()
        return raw

    def _get_line_number(pos: int) -> int:
        return tex_no_comments[:pos].count('\n') + 1

    def _has_priority_keywords(sample: str) -> bool:
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦åŒ…å«ä¼˜å…ˆå…³é”®è¯ï¼ˆ\\right.ã€arrayã€casesã€é¢˜å·æ ‡è®°ç­‰ï¼‰"""
        priority_patterns = [
            r'\\right\.',
            r'\\begin\{array\}',
            r'\\end\{array\}',
            r'\\begin\{cases\}',
            r'\\end\{cases\}',
            r'\(\d+\)',  # (1) (2) ç­‰å°é—®æ ‡è®°
            r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # åœ†åœˆæ•°å­—
        ]
        return any(re.search(pat, sample) for pat in priority_patterns)

    # ğŸ†• v1.8.6ï¼šè¿›ä¸€æ­¥ç”„åˆ«"ç–‘ä¼¼æˆªæ–­"ï¼Œä¼˜å…ˆè¾“å‡ºåŒ…å«å…³é”®è¯çš„æ ·æœ¬
    truncated_open_samples: List[str] = []
    priority_open_samples: List[str] = []

    for p in unmatched_open_positions:
        segment = tex_no_comments[p:p+300]
        if '\\)' not in segment:  # æ˜æ˜¾æ²¡æœ‰é—­åˆ
            sample = _sample_at(p, 'forward')
            line_num = _get_line_number(p)
            sample_with_line = f"Line {line_num}: {sample}"

            if _has_priority_keywords(sample):
                priority_open_samples.append(sample_with_line)
            else:
                truncated_open_samples.append(sample_with_line)
        else:
            # å¯èƒ½é—­æ‹¬å·è¿œåœ¨è¶…è¿‡ 120 ä¹‹åï¼Œä¹Ÿè®¤ä¸ºå¯ç–‘
            close_rel = segment.find('\\)')
            if close_rel > 120:
                sample = _sample_at(p, 'forward')
                line_num = _get_line_number(p)
                sample_with_line = f"Line {line_num}: {sample}"

                if _has_priority_keywords(sample):
                    priority_open_samples.append(sample_with_line)
                else:
                    truncated_open_samples.append(sample_with_line)

        # é™åˆ¶æ€»æ•°
        if len(priority_open_samples) + len(truncated_open_samples) >= 10:
            break

    # ä¼˜å…ˆå±•ç¤ºåŒ…å«å…³é”®è¯çš„æ ·æœ¬ï¼Œç„¶åæ˜¯æ™®é€šæ ·æœ¬
    final_open_samples = priority_open_samples[:5] + truncated_open_samples[:max(0, 5 - len(priority_open_samples))]

    truncated_close_samples: List[str] = []
    priority_close_samples: List[str] = []

    for p in unmatched_close_positions[:10]:
        sample = _sample_at(p, 'backward')
        line_num = _get_line_number(p)
        sample_with_line = f"Line {line_num}: {sample}"

        if _has_priority_keywords(sample):
            priority_close_samples.append(sample_with_line)
        else:
            truncated_close_samples.append(sample_with_line)

    final_close_samples = priority_close_samples[:5] + truncated_close_samples[:max(0, 5 - len(priority_close_samples))]

    if final_open_samples:
        issues.append(
            "Unmatched opens (samples): " +
            '; '.join(final_open_samples)
        )
    if final_close_samples:
        issues.append(
            "Unmatched closes (samples): " +
            '; '.join(final_close_samples)
        )

    # é’ˆå¯¹å›¾ç‰‡å ä½ç¬¦é™„è¿‘çš„æˆªæ–­ï¼š\( ... IMAGE_TODO_START æœªé—­åˆ
    image_trunc = re.findall(r'\\\([^\\)]{0,200}?% IMAGE_TODO_START', tex_no_comments)
    if image_trunc:
        issues.append(f"Potential image-adjacent truncated math segments: {len(image_trunc)}")

    # ğŸ†• v1.8.7ï¼šæŠ¥å‘Šåå‘æ¨¡å¼ï¼ˆ\) åœ¨ \( å‰é¢ï¼‰
    if reversed_pairs:
        issues.append(f"Reversed inline math pairs detected: {len(reversed_pairs)} lines")
        for lineno, line_content in reversed_pairs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            issues.append(f"  Line {lineno}: {line_content}")

    return issues


def generate_image_todo_block(img: Dict, stem_text: str = "", is_inline: bool = False) -> str:
    """ç”Ÿæˆæ–°æ ¼å¼çš„ IMAGE_TODO å ä½å—

    ğŸ†• v1.7ï¼šIMAGE_TODO å—åä¸æ·»åŠ é¢å¤–ç©ºè¡Œ

    Args:
        img: å›¾ç‰‡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« id, path, width, inline, question_index, sub_index
        stem_text: é¢˜å¹²æ–‡æœ¬ï¼Œç”¨äºæå–ä¸Šä¸‹æ–‡
        is_inline: æ˜¯å¦ä¸ºå†…è”å›¾ç‰‡

    Returns:
        æ ¼å¼åŒ–çš„ IMAGE_TODO å ä½å—
    """
    img_id = img.get('id', 'unknown')
    path = img.get('path', '')
    width = img.get('width', 60)
    inline = 'true' if img.get('inline', False) else 'false'
    q_idx = img.get('question_index', 0)
    sub_idx = img.get('sub_index', 1)

    # æå–ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼šå–å›¾ç‰‡å‰åå„50ä¸ªå­—ç¬¦ï¼‰
    # æ¸…ç† context å†…å®¹ï¼šå»é™¤ LaTeX å‘½ä»¤ï¼Œé™åˆ¶é•¿åº¦ï¼Œæ£€æŸ¥æ‹¬å·å¹³è¡¡
    def clean_context(text: str, max_len: int = 80) -> str:
        r"""æ¸…ç† CONTEXT æ³¨é‡Šå†…å®¹ï¼ˆå¢å¼ºç‰ˆ v1.9.1ï¼‰

        ğŸ†• v1.9.1ï¼š
        - å¢åŠ æœ€å¤§é•¿åº¦åˆ° 80 å­—ç¬¦ï¼ˆæ ¹æ®æŠ¥å‘Šå»ºè®®ï¼‰
        - æ›´å¥½åœ°å¤„ç† LaTeX ç¯å¢ƒå‘½ä»¤
        - å»é™¤ LaTeX ç¯å¢ƒå‘½ä»¤ï¼ˆ\begin{...}ã€\end{...}ï¼‰
        - å»é™¤ LaTeX å‘½ä»¤ï¼ˆ\xxx{...}ï¼‰
        - å»é™¤æ•°å­¦å®šç•Œç¬¦ \(...\) å’Œ \[...\]
        - æˆªæ–­åˆ°æœ€å¤š max_len å­—ç¬¦
        - æ£€æŸ¥æ‹¬å·å¹³è¡¡ï¼Œå¦‚æœä¸å¹³è¡¡åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not text:
            return ""

        # ğŸ†• v1.9.1ï¼šæ›´æ¿€è¿›åœ°å»é™¤ LaTeX ç¯å¢ƒå‘½ä»¤
        # åŒ¹é… \begin{...} æˆ– \end{...}ï¼Œå¹¶åˆ é™¤æ•´ä¸ªå‘½ä»¤
        text = re.sub(r'\\begin\{[^}]+\}', '[ENV_START]', text)
        text = re.sub(r'\\end\{[^}]+\}', '[ENV_END]', text)

        # å»é™¤ LaTeX å‘½ä»¤ï¼ˆ\xxx{...}ï¼‰
        text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', text)

        # å»é™¤æ•°å­¦å®šç•Œç¬¦
        text = re.sub(r'\\\(|\\\)|\\\[|\\\]', '', text)

        # å»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦
        if '\n' in text:
            text = text.split('\n')[0]

        # ğŸ†• v1.9.1ï¼šæˆªæ–­åˆ°æœ€å¤š max_len å­—ç¬¦ï¼ˆé»˜è®¤ 80ï¼‰
        if len(text) > max_len:
            text = text[:max_len] + '...'

        # æ£€æŸ¥æ‹¬å·å¹³è¡¡
        open_count = text.count('{')
        close_count = text.count('}')
        if open_count != close_count:
            # æ‹¬å·ä¸å¹³è¡¡ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²é¿å…ç¼–è¯‘é”™è¯¯
            return ""

        return text

    context_before = clean_context(img.get('context_before', '').strip())
    context_after = clean_context(img.get('context_after', '').strip())

    # ğŸ†• v1.7ï¼šæ„å»ºå ä½å—ï¼ŒIMAGE_TODO_END åä¸æ·»åŠ é¢å¤–çš„ \n
    # ğŸ†• v1.8.4ï¼šè½¬ä¹‰è·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆä¸‹åˆ’çº¿ç­‰ï¼‰
    path_escaped = path.replace('_', '\\_') if path else ''
    
    if is_inline:
        # å†…è”å›¾ç‰‡ï¼šä¸ä½¿ç”¨ center ç¯å¢ƒ
        block = (
            f"\n% IMAGE_TODO_START id={img_id} path={path_escaped} width={width}% inline={inline} "
            f"question_index={q_idx} sub_index={sub_idx}\n"
        )
        if context_before:
            block += f"% CONTEXT_BEFORE: {context_before}\n"
        if context_after:
            block += f"% CONTEXT_AFTER: {context_after}\n"
        block += (
            "\\begin{tikzpicture}[scale=0.8,baseline=-0.5ex]\n"
            f"  % TODO: AI_AGENT_REPLACE_ME (id={img_id})\n"
            "\\end{tikzpicture}\n"
            f"% IMAGE_TODO_END id={img_id}\n"
        )
    else:
        # ç‹¬ç«‹å›¾ç‰‡ï¼šä½¿ç”¨ center ç¯å¢ƒ
        block = (
            "\\begin{center}\n"
            f"% IMAGE_TODO_START id={img_id} path={path_escaped} width={width}% inline={inline} "
            f"question_index={q_idx} sub_index={sub_idx}\n"
        )
        if context_before:
            block += f"% CONTEXT_BEFORE: {context_before}\n"
        if context_after:
            block += f"% CONTEXT_AFTER: {context_after}\n"
        block += (
            "\\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]\n"
            f"  % TODO: AI_AGENT_REPLACE_ME (id={img_id})\n"
            "\\end{tikzpicture}\n"
            f"% IMAGE_TODO_END id={img_id}\n"
            "\\end{center}\n"  # ğŸ†• v1.7ï¼šä¸æ·»åŠ å°¾éšç©ºç™½è¡Œ
        )

    return block


def merge_explanations(analysis: str, explain: str) -> str:
    """æ™ºèƒ½åˆå¹¶è§£æå’Œè¯¦è§£

    Args:
        analysis: ã€åˆ†æã€‘å†…å®¹
        explain: ã€è¯¦è§£ã€‘å†…å®¹

    Returns:
        åˆå¹¶åçš„å†…å®¹
    """
    if not analysis:
        return explain or ""
    if not explain:
        return analysis or ""

    # æ£€æŸ¥æ˜¯å¦å†…å®¹ç›¸ä¼¼
    if analysis in explain or explain in analysis:
        return max(analysis, explain, key=len)  # é€‰æ‹©è¾ƒé•¿çš„

    # éƒ½æœ‰å†…å®¹ä¸”ä¸é‡å¤ï¼Œåˆå¹¶
    return f"{analysis}\n\n{explain}"


def clean_explain_content(explain_text: str) -> str:
    """æ¸…ç† explain å†…å®¹ä¸­çš„ç©ºè¡Œä¸æ®‹ç•™çš„ã€åˆ†æã€‘æ ‡è®°"""
    if not explain_text:
        return ""

    text = explain_text.replace("\r\n", "\n")
    text = re.sub(r'ã€\s*åˆ†æ\s*ã€‘.*?(?=ã€|$)', '', text, flags=re.DOTALL)
    text = re.sub(r'\n\s*\n+', r'\\par\n', text)
    return text.strip()


def build_question_tex(stem: str, options: List, meta: Dict, images: List, attachments: List,
                       section_type: str, question_index: int = 0, slug: str = "") -> str:
    """ç”Ÿæˆ question ç¯å¢ƒ

    ğŸ†• v1.9: æ”¯æŒé™„ä»¶æ¸²æŸ“ï¼ˆè¡¨æ ¼ã€æ–‡æœ¬ã€å›¾è¡¨ï¼‰
    ğŸ†• v1.8.8: å¢åŠ  meta å‘½ä»¤ä½¿ç”¨è®¡æ•°æ£€æµ‹
    ğŸ†• Prompt 3: æ”¯æŒå†…è”å›¾ç‰‡å ä½ç¬¦æ›¿æ¢
    ğŸ†• æ–°æ ¼å¼: ä½¿ç”¨ IMAGE_TODO_START/END å¸¦ ID çš„å ä½å—
    """
    # ğŸ†• v1.8.8ï¼šmeta ä½¿ç”¨è®¡æ•°ï¼ˆæ£€æµ‹é‡å¤çš„å…ƒä¿¡æ¯å‘½ä»¤ï¼‰
    meta_usage = {
        "answer": 0,
        "explain": 0,
        "topics": 0,
        "difficulty": 0,
    }

    # å…ˆå¤„ç†æ–‡æœ¬ï¼Œä½†ä¿ç•™å ä½ç¬¦
    stem_raw = stem  # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºä¸Šä¸‹æ–‡æå–
    stem = process_text_for_latex(stem, is_math_heavy=True)
    # ğŸ†• ä»»åŠ¡2ï¼šå¯¹é¢˜å¹²åº”ç”¨è½¯æ¢è¡Œ
    stem = soft_wrap_paragraph(stem)

    if section_type == "è§£ç­”é¢˜" and re.search(r'\(\d+\)', stem):
        stem = handle_subquestions(stem)

    explain_raw = meta.get("explain", "").strip()
    if explain_raw and re.search(r'ã€\s*åˆ†æ\s*ã€‘', explain_raw):
        print(f"âš ï¸  Q{question_index}: explain æ®µè½åŒ…å«ã€åˆ†æã€‘æ ‡è®°ï¼Œå·²è‡ªåŠ¨ç§»é™¤")
        explain_raw = re.sub(r'ã€\s*åˆ†æ\s*ã€‘.*?(?=ã€|$)', '', explain_raw, flags=re.DOTALL)
    if explain_raw:
        explain_raw = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', explain_raw)
        explain_raw = process_text_for_latex(explain_raw, is_math_heavy=True)
        explain_raw = soft_wrap_paragraph(explain_raw)
        explain_raw = clean_explain_content(explain_raw)

    topics_raw = meta.get("topics", "").strip()
    if topics_raw:
        topics_raw = topics_raw.replace("ã€", "ï¼›")
        topics_raw = escape_latex_special(topics_raw, in_math_mode=False)

    lines = []
    lines.append(r"\begin{question}")

    if stem:
        lines.append(stem)

    if options:
        lines.append(r"\begin{choices}")
        for opt in options:
            opt_processed = process_text_for_latex(opt, is_math_heavy=True)
            lines.append(f"  \\item {opt_processed}")
        lines.append(r"\end{choices}")

    # ğŸ†• æ–°æ ¼å¼: ä½¿ç”¨ IMAGE_TODO_START/END å ä½å—
    for idx, img in enumerate(images):
        # ç”Ÿæˆæ–°æ ¼å¼çš„å ä½å—
        img_todo_block = generate_image_todo_block(img, stem_raw, img.get('inline', False))

        if img.get('inline', False):
            # å†…è”å›¾ç‰‡ï¼šæ›¿æ¢å ä½ç¬¦
            placeholder = f"<<IMAGE_INLINE:{img.get('id', f'img{idx}')}>>"
            stem = stem.replace(placeholder, img_todo_block)
            explain_raw = explain_raw.replace(placeholder, img_todo_block) if explain_raw else explain_raw
            # æ›´æ–°å·²å¤„ç†çš„é€‰é¡¹
            for i, line in enumerate(lines):
                if placeholder in line:
                    lines[i] = line.replace(placeholder, img_todo_block)
        else:
            # ç‹¬ç«‹å›¾ç‰‡ï¼šè¿½åŠ åˆ°é¢˜ç›®æœ«å°¾
            lines.append("")
            lines.append(img_todo_block)

    if topics_raw:
        lines.append(f"\\topics{{{topics_raw}}}")
        meta_usage["topics"] += 1
    if meta.get("difficulty"):
        lines.append(f"\\difficulty{{{meta['difficulty']}}}")
        meta_usage["difficulty"] += 1
    if meta.get("answer"):
        # ä½¿ç”¨ä¸é¢˜å¹²/è§£æä¸€è‡´çš„å¤„ç†ï¼Œä»¥è§„èŒƒæ•°å­¦æ ¼å¼ï¼Œé¿å… $$...$$ æ®‹ç•™
        ans = process_text_for_latex(meta["answer"], is_math_heavy=True)
        lines.append(f"\\answer{{{ans}}}")
        meta_usage["answer"] += 1
    if explain_raw:
        lines.append(f"\\explain{{{explain_raw}}}")
        meta_usage["explain"] += 1

    # ğŸ†• v1.9: æ¸²æŸ“é™„ä»¶
    if attachments:
        lines.append("")
        lines.append("\\vspace{1em}")
        lines.append("\\textbf{é™„ï¼š}")
        lines.append("")

        for att in attachments:
            kind = att.get("kind", "text")
            att_lines = att.get("lines", [])

            if not att_lines:
                continue

            if kind == "table":
                att_text = "\n".join(att_lines)
                converted_md = None
                if "|" in att_text and any(re.match(r'^\s*\|[-:\s|]+\|$', line) for line in att_lines):
                    converted_md = convert_markdown_table_to_latex(att_text)
                ascii_table = convert_ascii_table_blocks(att_text)

                if converted_md:
                    lines.append(converted_md)
                elif ascii_table != att_text:
                    lines.append(ascii_table)
                else:
                    lines.append("\\begin{verbatim}")
                    for line in att_lines:
                        lines.append(line)
                    lines.append("\\end{verbatim}")
            elif kind == "text":
                # æ–‡æœ¬é™„ä»¶ï¼Œä½¿ç”¨ process_text_for_latex å¤„ç†
                att_text = "\n".join(att_lines)
                processed_att = process_text_for_latex(att_text, is_math_heavy=False)
                lines.append(processed_att)
            elif kind == "figure":
                # å›¾è¡¨é™„ä»¶ï¼ˆæš‚æ—¶ä½¿ç”¨ verbatimï¼‰
                lines.append("\\begin{verbatim}")
                for line in att_lines:
                    lines.append(line)
                lines.append("\\end{verbatim}")

            lines.append("")

    lines = ensure_choices_environment(lines, bool(options))
    lines.append(r"\end{question}")
    raw_question = "\n".join(lines)
    processed_lines = [
        postprocess_inline_math(line)
        for line in raw_question.splitlines()
    ]
    question_tex = "\n".join(processed_lines)

    # ğŸ†• v1.8.8ï¼šæ£€æŸ¥ meta å‘½ä»¤æ˜¯å¦é‡å¤ä½¿ç”¨
    if slug:  # åªåœ¨æœ‰ slug æ—¶æ‰è®°å½•æ—¥å¿—
        from pathlib import Path
        for key, cnt in meta_usage.items():
            if cnt > 1:
                # è®°å½•åˆ°ä¸“é—¨çš„ issue æ—¥å¿—
                debug_dir = Path("word_to_tex/output/debug")
                debug_dir.mkdir(parents=True, exist_ok=True)
                log_file = debug_dir / f"{slug}_meta_duplicates.log"

                with log_file.open("a", encoding="utf-8") as f:
                    if log_file.stat().st_size == 0:
                        # é¦–æ¬¡å†™å…¥ï¼Œæ·»åŠ å¤´éƒ¨
                        f.write(f"# Duplicate Meta Commands Detection Log for {slug}\n")
                        f.write(f"# Generated: {Path(__file__).name}\n\n")

                    f.write(f"{'='*80}\n")
                    f.write(f"Question {question_index}: meta '\\{key}' appears {cnt} times\n")
                    f.write(f"  Section: {section_type}\n")
                    f.write(f"  â†’ Please check duplicated ã€è¯¦è§£ã€‘/ã€è€ƒç‚¹ã€‘/ã€ç­”æ¡ˆã€‘/ã€éš¾åº¦ã€‘ blocks in Markdown\n\n")

    return question_tex


def convert_md_to_examx(md_text: str, title: str, slug: str = "", enable_issue_detection: bool = True) -> str:
    """ä¸»è½¬æ¢å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰

    ğŸ†• v1.6.3ï¼šå¢åŠ é—®é¢˜æ£€æµ‹å’Œæ—¥å¿—è®°å½•

    Args:
        md_text: Markdown æ–‡æœ¬
        title: è¯•å·æ ‡é¢˜
        slug: è¯•å· slugï¼ˆç”¨äºæ—¥å¿—æ–‡ä»¶åï¼‰
        enable_issue_detection: æ˜¯å¦å¯ç”¨é—®é¢˜æ£€æµ‹
    """
    md_text = clean_markdown(md_text)
    sections = split_sections(md_text)

    # ğŸ†• v1.6.3ï¼šåˆå§‹åŒ–é—®é¢˜æ—¥å¿—
    if enable_issue_detection and slug:
        init_issue_log(slug)

    out_lines = []
    out_lines.append(f"\\examxtitle{{{title}}}")

    q_index = 0  # å…¨å±€é¢˜å·è®¡æ•°å™¨
    for raw_title, body in sections:
        sec_label = SECTION_MAP.get(raw_title, raw_title)
        out_lines.append("")
        out_lines.append(f"\\section{{{sec_label}}}")

        for block in split_questions(body):
            if not block.strip():
                continue

            q_index += 1  # é¢˜å·é€’å¢
            raw_block = block  # ä¿å­˜åŸå§‹ Markdown ç‰‡æ®µ

            try:
                # ğŸ†• ä¼ é€’ question_index å’Œ slug ç”¨äºç”Ÿæˆå›¾ç‰‡ ID
                content, meta, images, attachments = extract_meta_and_images(block, question_index=q_index, slug=slug)

                # ä½¿ç”¨å¢å¼ºçš„è½¬æ¢å‡½æ•°ï¼ˆè¿”å›3ä¸ªå€¼ï¼‰
                stem, options, extracted_analysis = convert_choices(content)

                # åˆå¹¶æå–çš„è§£æå’Œå…ƒä¿¡æ¯ä¸­çš„è§£æ
                if extracted_analysis and not meta.get('explain'):
                    meta['explain'] = extracted_analysis
                elif extracted_analysis:
                    meta['explain'] = meta['explain'] + '\n' + extracted_analysis

                # ğŸ†• ä¼ é€’ question_index å’Œ slug åˆ° build_question_tex
                q_tex = build_question_tex(stem, options, meta, images, attachments, sec_label,
                                          question_index=q_index, slug=slug)

                # ğŸ†• v1.6.4ï¼šæ£€æµ‹é—®é¢˜å¹¶è®°å½•æ—¥å¿—ï¼ˆä¼ å…¥ meta & section_labelï¼‰
                if enable_issue_detection and slug:
                    issues = detect_question_issues(
                        slug=slug,
                        q_index=q_index,
                        raw_block=raw_block,
                        tex_block=q_tex,
                        meta=meta,
                        section_label=sec_label,
                    )
                    append_issue_log(
                        slug=slug,
                        q_index=q_index,
                        raw_block=raw_block,
                        tex_block=q_tex,
                        issues=issues,
                        meta=meta,
                        section_label=sec_label,
                    )

                # éªŒè¯ç”Ÿæˆçš„ TeX æ˜¯å¦å®Œæ•´
                if r'\begin{question}' in q_tex and r'\end{question}' not in q_tex:
                    print(f"âš ï¸  Q{q_index} ç¼ºå°‘ \\end{{question}}ï¼Œè‡ªåŠ¨è¡¥å…¨")
                    q_tex += "\n\\end{question}"

                out_lines.append("")
                out_lines.append(q_tex)
            except Exception as e:
                import traceback
                print(f"âš ï¸  Q{q_index} ({sec_label}) è½¬æ¢å¤±è´¥: {str(e)}")
                print(f"   {traceback.format_exc()}")
                out_lines.append("")
                out_lines.append(r"\begin{question}")
                out_lines.append(f"% ERROR: Q{q_index} è½¬æ¢å¤±è´¥ - {str(e)}")
                out_lines.append(r"\end{question}")

    out_lines.append("")

    # æœ€ç»ˆå¤„ç†ï¼šæ¸…ç†ç©ºè¡Œå’Œåˆ†å‰²è¶…é•¿è¡Œ
    result = "\n".join(out_lines)
    result = remove_blank_lines_in_macro_args(result)
    result = split_long_lines_in_explain(result, max_length=800)
    # ğŸ”¥ v1.8.3ï¼šé‡æ–°å¯ç”¨ï¼ˆå·²ä¿®å¤æ‹¬å·è®¡æ•°é€»è¾‘ï¼‰
    result = remove_par_breaks_in_explain(result)
    # ğŸ”¥ v1.8.1ï¼šclean_question_environments ä»ç„¶ç¦ç”¨ï¼ˆæ­£åˆ™åŒ¹é…é—®é¢˜ï¼‰

    # æœ€ç»ˆå…œåº•ï¼šè§„èŒƒ/ç§»é™¤æ®‹ç•™çš„ $$ æ˜¾ç¤ºæ•°å­¦æ ‡è®°
    # 1) å°†æˆå¯¹ $$...$$ ç»Ÿä¸€ä¸ºè¡Œå†… \(...\)ï¼ˆä¸ smart_inline_math è¡Œä¸ºä¸€è‡´ï¼‰
    # ğŸ†• v1.9.3ï¼šè·³è¿‡æ³¨é‡Šè¡Œï¼Œé¿å…ç ´å CONTEXT æ³¨é‡Š
    def convert_display_math_skip_comments(text: str) -> str:
        lines = text.split('\n')
        result_lines = []
        for line in lines:
            if line.strip().startswith('%'):
                # æ³¨é‡Šè¡Œï¼šä¸å¤„ç†ï¼Œç›´æ¥ä¿ç•™ï¼ˆåŒ…æ‹¬ $$...$$ï¼‰
                result_lines.append(line)
            else:
                # éæ³¨é‡Šè¡Œï¼šè½¬æ¢ $$...$$ ä¸º \(...\)
                line = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', line)
                result_lines.append(line)
        return '\n'.join(result_lines)
    
    result = convert_display_math_skip_comments(result)
    # 2) æ¸…ç†ä»»ä½•æ®‹ç•™çš„å­¤ç«‹ $$ï¼ˆé¿å…ç¼–è¯‘é”™è¯¯ï¼‰- åŒæ ·è·³è¿‡æ³¨é‡Šè¡Œ
    lines = result.split('\n')
    result = '\n'.join(
        line if line.strip().startswith('%') else line.replace('$$', '')
        for line in lines
    )

    # ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    result = cleanup_remaining_image_markers(result)

    # ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™ï¼ˆåˆ†ä¸¤æ­¥ï¼‰
    result = cleanup_guxuan_in_macros(result)

    # ğŸ†• v1.6.1ï¼šå…¨å±€æ¸…ç†ä»»ä½•æ®‹ç•™çš„"æ•…é€‰"ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
    # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰ï¼šX"ï¼Œæ— è®ºåœ¨ä»€ä¹ˆä½ç½®
    result = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n}]*', '', result)
    # æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
    result = re.sub(r'æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*\.?', '', result)

    # ğŸ†• v1.8.4ï¼šä¿®å¤åˆå¹¶é¢˜ç›®çš„ç»“æ„ï¼ˆé¢˜å¹² vs å°é—®ï¼‰
    result = fix_merged_questions_structure(result)

    # ğŸ†• v1.8.6ï¼šåå¤„ç†ä¿®å¤ \right. è¾¹ç•Œé”™è¯¯ï¼ˆæ”¶ç´§ç‰ˆ - P0 æœ€é«˜ä¼˜å…ˆçº§ï¼‰
    result = fix_right_boundary_errors(result)
    result = fix_reversed_delimiters(result)

    # ğŸ†• v1.8.5ï¼šéªŒè¯å¹¶ä¿®å¤ IMAGE_TODO å—æ ¼å¼é”™è¯¯ï¼ˆP0ï¼‰
    result = validate_and_fix_image_todo_blocks(result)

    # ğŸ†• v1.8.6ï¼šå¹³è¡¡ array/cases ç¯å¢ƒï¼ˆP0 - åˆ é™¤å¤šä½™çš„ \endï¼‰
    result = balance_array_and_cases_env(result)

    # ğŸ†• v1.8.7ï¼šä¿®å¤ç‰¹å®šçš„åå‘æ•°å­¦å®šç•Œç¬¦æ¨¡å¼ï¼ˆæçª„è‡ªåŠ¨ä¿®å¤ï¼‰
    result = fix_specific_reversed_pairs(result)

    # ğŸ†• v1.8.8ï¼šæåº¦ä¿å®ˆçš„åå‘å®šç•Œç¬¦è‡ªåŠ¨ä¿®å¤ï¼ˆä»…åœ¨ç®€å•åœºæ™¯å¯ç”¨ï¼‰
    result = fix_simple_reversed_inline_pairs(result)
    result = balance_left_right_delimiters(result)

    # ğŸ†• v1.8.8ï¼šæ£€æµ‹åå‘å®šç•Œç¬¦å¹¶è®°å½•æ—¥å¿—ï¼ˆä¸æ”¹å˜è¾“å‡ºï¼‰
    if slug:
        collect_reversed_math_samples(result, slug)

    # ğŸ†• v1.9.6ï¼šç¦ç”¨ fix_missing_items_in_enumerate
    # åŸå› ï¼šè¿™ä¸ªå‡½æ•°ä¼šæŠŠ enumerate ä¸­çš„æ¯è¡Œéƒ½åŠ  \itemï¼Œå¯¼è‡´å¤šè¡Œå­é—®é¢˜å˜æˆå¤šä¸ª \item
    # result = fix_missing_items_in_enumerate(result)

    # ğŸ†• v1.9.1ï¼šä¿®å¤ tabular ç¯å¢ƒç¼ºå¤±åˆ—æ ¼å¼ï¼ˆP1ï¼‰
    result = fix_tabular_environments(result)

    # ğŸ†• v1.9.6ï¼šä¿®å¤ä¸‰è§’å‡½æ•°ç©ºæ ¼å’Œæœªå®šä¹‰ç¬¦å·
    result = fix_trig_function_spacing(result)
    result = fix_undefined_symbols(result)
    result = fix_markdown_bold_residue(result)  # ğŸ†• v1.9.7ï¼šæ¸…ç†ç²—ä½“æ®‹ç•™
    result = fix_nested_subquestions(result)
    result = fix_spurious_items_in_enumerate(result)
    result = fix_keep_questions_together(result)

    return result


# ==================== ğŸ†• v1.6.3 æ–°å¢ï¼šé—®é¢˜æ£€æµ‹ä¸æ—¥å¿—ç³»ç»Ÿ ====================

def detect_question_issues(
    slug: str,
    q_index: int,
    raw_block: str,
    tex_block: str,
    meta: Optional[Dict[str, str]] = None,
    section_label: Optional[str] = None,
) -> List[str]:
    """ğŸ†• v1.7ï¼šæ£€æµ‹é¢˜ç›®ä¸­çš„å¯ç–‘æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆï¼‰
    ğŸ†• v1.6.4ï¼šæ£€æµ‹é¢˜ç›®ä¸­çš„å¯ç–‘æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        slug: è¯•å· slugï¼ˆå¦‚ "nanjing_2026_sep"ï¼‰
        q_index: é¢˜å·ï¼ˆä» 1 å¼€å§‹ï¼‰
        raw_block: åŸå§‹ Markdown ç‰‡æ®µ
        tex_block: ç”Ÿæˆçš„ TeX ç‰‡æ®µ
        meta: è§£æå¾—åˆ°çš„å…ƒä¿¡æ¯å­—å…¸ï¼ˆç­”æ¡ˆã€éš¾åº¦ã€çŸ¥è¯†ç‚¹ã€è§£æç­‰ï¼‰
        section_label: å½“å‰å¤§é¢˜æ ‡é¢˜ï¼ˆå¦‚ "å•é€‰é¢˜"ã€"å¤šé€‰é¢˜" ç­‰ï¼‰

    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    issues: List[str] = []
    tex_no_comments_lines: List[str] = []
    for _line in tex_block.splitlines():
        tex_no_comments_lines.append(_line.split('%', 1)[0])
    tex_no_comments = "\n".join(tex_no_comments_lines)

    # ---------- ğŸ†• v1.8.6ï¼šæ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®ï¼ˆå¢å¼ºç‰ˆ - å¸¦ä¸Šä¸‹æ–‡ï¼‰ ----------
    # æ£€æŸ¥é¢˜ç›®æ˜¯å¦ç›´æ¥ä» \item å¼€å§‹ï¼ˆç¼ºå°‘é¢˜å¹²ï¼‰
    # åœ¨ \begin{question} åï¼Œå¦‚æœç¬¬ä¸€ä¸ªéç©ºè¡Œæ˜¯ \item æˆ– \begin{choices}ï¼Œåˆ™ç¼ºå°‘é¢˜å¹²
    question_content = tex_block
    if r'\begin{question}' in question_content:
        # æå– \begin{question} å’Œ \begin{choices} ä¹‹é—´çš„å†…å®¹
        match = re.search(r'\\begin\{question\}(.*?)(?:\\begin\{choices\}|\\item|\\end\{question\})',
                         question_content, re.DOTALL)
        if match:
            content_between = match.group(1).strip()
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–åªæœ‰æ³¨é‡Šï¼Œåˆ™ç¼ºå°‘é¢˜å¹²
            # ç§»é™¤æ³¨é‡Šè¡Œ
            content_no_comments = re.sub(r'^\s*%.*$', '', content_between, flags=re.MULTILINE).strip()
            if not content_no_comments:
                # ğŸ†• v1.8.6ï¼šæå–åŸå§‹ Markdown ä¸Šä¸‹æ–‡ï¼ˆé¢˜å·å‰åå„ 1-2 è¡Œï¼‰
                raw_lines = raw_block.splitlines()
                context_lines = []

                # æå–å‰ 3 è¡Œï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                for i, line in enumerate(raw_lines[:3]):
                    truncated = line[:80] + '...' if len(line) > 80 else line
                    context_lines.append(f"  MD L{i+1}: {truncated}")

                context_str = '\n'.join(context_lines)

                issues.append(
                    f"âš ï¸ CRITICAL: é¢˜ç›®ç¼ºå°‘é¢˜å¹²ï¼Œç›´æ¥ä» \\item å¼€å§‹\n"
                    f"  é¢˜å‹: {section_label or 'N/A'}\n"
                    f"  é¢˜å·: Q{q_index}\n"
                    f"  åŸå§‹ Markdown ç‰‡æ®µ:\n{context_str}\n"
                    f"  â†’ è¯·åœ¨ Markdown ä¸­è¡¥å……é¢˜å¹²å†…å®¹"
                )

    # ---------- 1) åŸæœ‰æ£€æŸ¥é€»è¾‘ï¼ˆä¿ç•™ & å¤åˆ»ï¼‰ ----------

    # 1.1 æ£€æµ‹ meta å½¢å¼çš„ã€åˆ†æã€‘ï¼ˆä¸åº”è¯¥å‡ºç°ï¼‰
    if "ã€åˆ†æã€‘" in raw_block and "ã€åˆ†æã€‘" in tex_no_comments:
        issues.append("Contains meta ã€åˆ†æã€‘ in both raw and tex (should be discarded)")
    elif "ã€åˆ†æã€‘" in tex_no_comments:
        issues.append("Contains meta ã€åˆ†æã€‘ in tex (should be discarded)")

    # 1.2 æ£€æµ‹ *$x$* æˆ–å…¶ä»– star + math æ¨¡å¼
    if re.search(r'\*\s*\$', tex_no_comments) or re.search(r'\$\s*\*', tex_no_comments):
        issues.append("Star-emphasis around inline math, e.g. *$x$*")

    # 1.3 æ£€æµ‹ç©º $$ æˆ–å½¢å¦‚ $$\(
    if re.search(r'\$\s*\$', tex_no_comments):
        issues.append("Empty inline/ display math $$")
    if re.search(r'\$\s*\$\s*\\\(', tex_no_comments):
        issues.append("Suspicious pattern $$\\(")

    # 1.4 æ£€æµ‹è¡Œå†… math åˆ†éš”ç¬¦æ•°é‡æ˜æ˜¾ä¸åŒ¹é…
    open_count = tex_no_comments.count(r'\(')
    close_count = tex_no_comments.count(r'\)')
    if open_count != close_count:
        issues.append(f"Unbalanced inline math delimiters: ${open_count} vs$ {close_count}")

    # 1.5 æ£€æµ‹å…¨è§’æ‹¬å·æ®‹ç•™
    if 'ï¼ˆ' in tex_no_comments or 'ï¼‰' in tex_no_comments:
        issues.append("Fullwidth brackets ï¼ˆï¼‰found in tex")

    # 1.6 æ£€æµ‹"æ•…é€‰"æ®‹ç•™
    if re.search(r'æ•…é€‰[:ï¼š][ABCD]+', tex_no_comments):
        issues.append("'æ•…é€‰' pattern found in tex")

    # ---------- 2) æ–°å¢ï¼šåŸºäº meta çš„ä¸€è‡´æ€§æ£€æŸ¥ ----------

    if meta is not None:
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å–å€¼å¹¶ strip
        def _get(key: str) -> str:
            return (meta.get(key) or "").strip()

        answer = _get("answer")
        difficulty = _get("difficulty")
        topics = _get("topics")
        explain = _get("explain")
        analysis = _get("analysis")

        # 2.1 æ£€æŸ¥"åˆ†æ"å­—æ®µæ˜¯å¦ä»ç„¶å­˜åœ¨ï¼ˆæŒ‰è§„èŒƒåº”ä¸¢å¼ƒï¼Œä»…å…è®¸ä½œä¸ºä¸­é—´æ€ï¼Œè€Œä¸åº”å†™å…¥ TeXï¼‰
        if analysis:
            issues.append("Meta contains 'analysis' field (ã€åˆ†æã€‘) â€“ it should not be used in output")

        # 2.2 æ£€æŸ¥ section/å¤§é¢˜ ä¸ç­”æ¡ˆå¿…éœ€æ€§
        sec = section_label or ""
        is_choice_section = ("å•é€‰" in sec) or ("å¤šé€‰" in sec)

        # å¯¹é€‰æ‹©é¢˜ï¼Œå°é¢˜é€šå¸¸å¿…é¡»æœ‰ç­”æ¡ˆ
        if is_choice_section and not answer:
            issues.append("Choice question in section '{0}' has no ã€ç­”æ¡ˆã€‘ meta".format(sec or "?"))

        # å¯¹äºéé€‰æ‹©é¢˜ï¼Œç­”æ¡ˆç¼ºå¤±ä¸ä¸€å®šæ˜¯è‡´å‘½é”™è¯¯ï¼Œä½†å¯ä»¥æç¤º
        if not is_choice_section and not answer:
            issues.append("Question has no ã€ç­”æ¡ˆã€‘ meta (section='{0}')".format(sec or "?"))

        # 2.3 meta ä¸ TeX çš„æ˜ å°„ä¸€è‡´æ€§
        has_answer_macro = "\\answer{" in tex_block
        has_explain_macro = "\\explain{" in tex_block

        if answer and not has_answer_macro:
            issues.append("Meta has answer but TeX is missing \\answer{}")
        if has_answer_macro and not answer:
            issues.append("TeX has \\answer{} but meta.answer is empty")

        if explain and not has_explain_macro:
            issues.append("Meta has explain but TeX is missing \\explain{}")
        if has_explain_macro and not explain:
            issues.append("TeX has \\explain{} but meta.explain is empty")

        # 2.4 ç¡®ä¿ \\explain{} ä¸ä¼šå·å·åƒè¿›ã€åˆ†æã€‘å†…å®¹
        # è¿™é‡Œåªåšç®€å•æ–‡æœ¬çº§æ£€æµ‹ï¼šå¦‚æœ raw_block é‡Œæœ‰"ã€åˆ†æã€‘"ä¸” meta.explain ä¸ºç©ºï¼Œåˆ™é¢å¤–æç¤º
        if "ã€åˆ†æã€‘" in raw_block and not explain:
            issues.append("Raw block contains ã€åˆ†æã€‘ but meta.explain is empty â€“ this question is treated as 'no explain'")

        # 2.5 æ£€æµ‹è¶…é•¿ explain å†…å®¹ï¼ˆ>500è¡Œï¼‰
        if explain:
            explain_lines = explain.count('\n') + 1
            if explain_lines > 500:
                issues.append(f"âš ï¸  LONG_EXPLAIN: {explain_lines} lines (>500) â€“ may cause conversion issues")
            elif explain_lines > 200:
                issues.append(f"Long explain: {explain_lines} lines (>200) â€“ consider simplification")

    return issues


def append_issue_log(
    slug: str,
    q_index: int,
    raw_block: str,
    tex_block: str,
    issues: List[str],
    meta: Optional[Dict[str, str]] = None,
    section_label: Optional[str] = None,
) -> None:
    """ğŸ†• v1.6.4ï¼šå°†é—®é¢˜è®°å½•åˆ° debug æ—¥å¿—ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        slug: è¯•å· slug
        q_index: é¢˜å·
        raw_block: åŸå§‹ Markdown ç‰‡æ®µ
        tex_block: ç”Ÿæˆçš„ TeX ç‰‡æ®µ
        issues: é—®é¢˜åˆ—è¡¨
        meta: è§£æå¾—åˆ°çš„å…ƒä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰
        section_label: å½“å‰å¤§é¢˜æ ‡é¢˜ï¼ˆå¦‚ "å•é€‰é¢˜" / "å¤šé€‰é¢˜" ç­‰ï¼‰
    """
    if not issues:
        return

    debug_dir = Path("word_to_tex/output/debug")
    debug_dir.mkdir(parents=True, exist_ok=True)
    log_file = debug_dir / f"{slug}_issues.log"

    with log_file.open("a", encoding="utf-8") as f:
        f.write(f"{'='*80}\n")
        f.write(f"# Q{q_index} issues (section={section_label or 'N/A'})\n\n")

        # ç®€è¦ meta æ¦‚è§ˆï¼ˆå¦‚æœæœ‰ï¼‰
        if meta is not None:
            # åªå±•ç¤ºå…³é”®ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—å¤ªå†—é•¿
            summary_keys = ["answer", "difficulty", "topics", "explain", "analysis"]
            f.write("## Meta summary:\n")
            for key in summary_keys:
                if key in meta:
                    val = (meta.get(key) or "").strip()
                    if len(val) > 80:
                        val_display = val[:77] + "..."
                    else:
                        val_display = val
                    f.write(f"- {key}: {val_display}\n")
            f.write("\n")

        f.write("## Issues:\n")
        for issue in issues:
            f.write(f"- {issue}\n")

        f.write("\n## Raw Markdown:\n")
        f.write("```markdown\n")
        f.write(raw_block.strip() + "\n")
        f.write("```\n\n")

        f.write("## Generated TeX:\n")
        f.write("```tex\n")
        f.write(tex_block.strip() + "\n")
        f.write("```\n\n")


def init_issue_log(slug: str) -> None:
    """ğŸ†• v1.6.3ï¼šåˆå§‹åŒ–é—®é¢˜æ—¥å¿—æ–‡ä»¶

    Args:
        slug: è¯•å· slug
    """
    debug_dir = Path("word_to_tex/output/debug")
    debug_dir.mkdir(parents=True, exist_ok=True)
    log_file = debug_dir / f"{slug}_issues.log"

    # æ¸…ç©ºæ—§æ—¥å¿—
    with log_file.open("w", encoding="utf-8") as f:
        f.write(f"# Issue Detection Log for {slug}\n")
        f.write(f"# Generated: {Path(__file__).name} v{VERSION}\n")
        f.write(f"# Date: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n")


# ==================== ğŸ†• v1.3 æ–°å¢ï¼šè‡ªåŠ¨éªŒè¯å‡½æ•° ====================

def assert_no_analysis_meta_in_auto_tex(slug: str) -> None:
    """ğŸ†• v1.6.3ï¼šæ£€æŸ¥ auto ç›®å½•ä¸­æ˜¯å¦æ®‹ç•™ã€åˆ†æã€‘meta æ®µ

    Args:
        slug: è¯•å· slugï¼ˆå¦‚ "nanjing_2026_sep"ï¼‰

    Raises:
        RuntimeError: å¦‚æœå‘ç°ã€åˆ†æã€‘æ®‹ç•™
    """
    root = Path("content/exams/auto") / slug
    if not root.exists():
        return

    for tex in root.rglob("*.tex"):
        txt = tex.read_text(encoding="utf-8")
        # åªæ‹¦ç±»ä¼¼ã€åˆ†æã€‘è¿™ç±» meta æ®µï¼Œè€Œä¸æ˜¯è‡ªç„¶è¯­è¨€ä¸­çš„"åˆ†æ"äºŒå­—
        if re.search(r"ã€\s*åˆ†æ\s*ã€‘", txt):
            raise RuntimeError(f"[ANALYSIS-META-LEFTOVER] {tex} still contains ã€åˆ†æã€‘.")


def validate_latex_output(tex_content: str) -> List[str]:
    """
    ğŸ†• v1.3 æ–°å¢ï¼šéªŒè¯LaTeXè¾“å‡ºï¼Œè¿”å›è­¦å‘Šåˆ—è¡¨
    ğŸ†• v1.6.3ï¼šå¢åŠ ã€åˆ†æã€‘æ®‹ç•™æ£€æŸ¥

    Args:
        tex_content: ç”Ÿæˆçš„LaTeXå†…å®¹

    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    warnings = []

    # å»é™¤æ³¨é‡Šå†…å®¹ï¼Œé¿å… IMAGE_TODO çš„ CONTEXT æ³¨é‡Šè§¦å‘è¯¯æŠ¥
    tex_no_comments_lines: List[str] = []
    for line in tex_content.splitlines():
        tex_no_comments_lines.append(line.split('%', 1)[0])
    tex_no_comments = "\n".join(tex_no_comments_lines)

    # ğŸ†• æ£€æŸ¥0ï¼šã€åˆ†æã€‘meta æ®µæ®‹ç•™
    analysis_meta = re.findall(r'ã€\s*åˆ†æ\s*ã€‘', tex_no_comments)
    if analysis_meta:
        warnings.append(f"âŒ å‘ç° {len(analysis_meta)} å¤„ã€åˆ†æã€‘meta æ®µæ®‹ç•™ï¼ˆåº”å·²è¢«ä¸¢å¼ƒï¼‰")

    # æ£€æŸ¥1ï¼šæ®‹ç•™çš„ $ ç¬¦å·
    dollar_matches = re.findall(r'(?<!\\)\$[^\$]+\$', tex_no_comments)
    if dollar_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(dollar_matches)} å¤„æ®‹ç•™çš„ $ æ ¼å¼")
        for i, match in enumerate(dollar_matches[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            warnings.append(f"     ç¤ºä¾‹{i}: {match}")

    # æ£€æŸ¥2ï¼šæ®‹ç•™çš„"æ•…é€‰"
    guxuan_matches = re.findall(r'æ•…é€‰[:ï¼š][ABCD]+', tex_no_comments)
    if guxuan_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(guxuan_matches)} å¤„æ®‹ç•™çš„'æ•…é€‰'")

    # æ£€æŸ¥3ï¼šä¸­æ–‡æ‹¬å·
    chinese_paren = re.findall(r'[ï¼ˆï¼‰]', tex_no_comments)
    if chinese_paren:
        warnings.append(f"âš ï¸  å‘ç° {len(chinese_paren)} å¤„ä¸­æ–‡æ‹¬å·")

    # æ£€æŸ¥4ï¼šç¯å¢ƒé—­åˆ
    begin_count = tex_content.count(r'\begin{question}')
    end_count = tex_content.count(r'\end{question}')
    if begin_count != end_count:
        warnings.append(f"âŒ question ç¯å¢ƒä¸åŒ¹é…: {begin_count} ä¸ª begin, {end_count} ä¸ª end")

    begin_choices = tex_content.count(r'\begin{choices}')
    end_choices = tex_content.count(r'\end{choices}')
    if begin_choices != end_choices:
        warnings.append(f"âŒ choices ç¯å¢ƒä¸åŒ¹é…: {begin_choices} ä¸ª begin, {end_choices} ä¸ª end")

    # æ£€æŸ¥5ï¼šç©ºè¡Œåœ¨å®å‚æ•°ä¸­
    problematic_macros = []
    for macro in ['explain', 'topics', 'answer']:
        pattern = rf'\\{macro}\{{[^}}]*\n\s*\n[^}}]*\}}'
        if re.search(pattern, tex_content):
            problematic_macros.append(macro)
    if problematic_macros:
        warnings.append(f"âš ï¸  ä»¥ä¸‹å®å‚æ•°ä¸­å¯èƒ½æœ‰ç©ºè¡Œ: {', '.join(problematic_macros)}")

    return warnings


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description=f"OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ†• v1.5 æ ¸å¿ƒåŠŸèƒ½ï¼š
  - ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆ$$\\(...\\)$$ â†’ \\(...\\)ï¼‰
  - ç»Ÿä¸€æ•°å­¦å…¬å¼æ ¼å¼ï¼šæ‰€æœ‰ $$...$$ è½¬æ¢ä¸ºè¡Œå†… \\(...\\)
  - è‡ªåŠ¨å±•å¼€å•è¡Œé€‰é¡¹ï¼ˆ> A... B... â†’ å¤šè¡Œï¼‰
  - å¼ºåˆ¶æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™ï¼ˆç¡®ä¿å·²è¢«ä¸¢å¼ƒï¼‰

âœ… v1.4 æ”¹è¿›å›é¡¾ï¼š
  - æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ä¿®å¤ï¼ˆåˆç‰ˆï¼‰
  - å•è¡Œé€‰é¡¹è‡ªåŠ¨å±•å¼€ï¼ˆåˆç‰ˆï¼‰

âœ… v1.3 æ”¹è¿›å›é¡¾ï¼š
  - ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢
  - æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™
  - ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆæ‹¬å·ã€å¼•å·ï¼‰
  - æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

âœ… v1.2 æ”¹è¿›å›é¡¾ï¼š
  - åŠ å¼ºç©ºè¡Œæ¸…ç†ï¼ˆè§£å†³80%çš„Runaway argumenté”™è¯¯ï¼‰
  - è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²ï¼ˆè§£å†³ç¼–è¯‘æ…¢é—®é¢˜ï¼‰
  - å¢å¼ºæ•°å­¦å˜é‡æ£€æµ‹ï¼ˆå‡å°‘Missing $é”™è¯¯ï¼‰
  - å¢å¼ºé€‰é¡¹è§£æï¼ˆå¤„ç†åµŒå…¥çš„è§£æå†…å®¹ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
  python3 ocr_to_examx.py "æµ™æ±Ÿçœé‡‘ååæ ¡/" output/
        """
    )
    
    parser.add_argument("input", help="è¾“å…¥è·¯å¾„ï¼ˆ.md æ–‡ä»¶æˆ– OCR æ–‡ä»¶å¤¹ï¼‰")
    parser.add_argument("output", help="è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ– .tex æ–‡ä»¶ï¼‰")
    parser.add_argument("--title", help="è¯•å·æ ‡é¢˜", default=None)
    parser.add_argument("--figures-dir", help="æŒ‡å®šå›¾ç‰‡èµ„æºæ‰€åœ¨ç›®å½•ï¼ˆä¼˜å…ˆäºè‡ªåŠ¨æ£€æµ‹ï¼‰", default=None)
    parser.add_argument("--legacy-math", action="store_true", help="ä½¿ç”¨æ—§æ•°å­¦æ­£åˆ™ç®¡çº¿ (smart_inline_math ç­‰) è¿›è¡Œæ•°å­¦å¤„ç†ï¼Œä»…æµ‹è¯•æ¯”è¾ƒç”¨")
    parser.add_argument("--version", action="version", version=f"%(prog)s {VERSION}")
    
    args = parser.parse_args()
    
    try:
        print(f"ğŸ” OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}")
        print("â”" * 60)
        # å¯é€‰ï¼šåˆ‡æ¢åˆ°æ—§æ•°å­¦ç®¡çº¿ï¼ˆA/B æµ‹è¯•ç”¨ï¼‰
        _orig_process = None
        if args.legacy_math:
            print("âš ï¸ ä½¿ç”¨ legacy æ•°å­¦ç®¡çº¿ (smart_inline_math ç­‰) â€” ä»…ä¾›æ¯”è¾ƒæµ‹è¯•")
            _orig_process = process_text_for_latex
            def _legacy_wrapper(t: str, is_math_heavy: bool = False):
                if not t:
                    return t
                # å‰ç½®æ¸…ç†ï¼ˆå¤ç”¨ç°è¡Œç‰ˆæœ¬çš„åˆæ®µé€»è¾‘ï¼‰
                t = re.sub(r'\*\s*(\$[^$]+\$)\s*\*', r'\1', t)
                t = re.sub(r'\*([A-Za-z0-9])\*', r'\\emph{\1}', t)
                t = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t)
                t = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t)
                t = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', t)
                t = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', t)
                t = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t, flags=re.MULTILINE)
                t = re.sub(r'[ï¼Œ,]?\s*æ•…é€‰[:ï¼š]\s*[ABCD]+[ã€‚ï¼.]*\s*$', '', t, flags=re.MULTILINE)
                t = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', t)
                if 'âˆµ' in t or 'âˆ´' in t:
                    t = t.replace('âˆµ', '$\\because$').replace('âˆ´', '$\\therefore$')
                if not is_math_heavy:
                    t = escape_latex_special(t, in_math_mode=False)
                t = smart_inline_math(t)
                t = fix_double_wrapped_math(t)
                t = fix_inline_math_glitches(t)
                return t
            process_text_for_latex = _legacy_wrapper  # type: ignore

        md_file, images_dir = find_markdown_and_images(args.input)

        # å¤„ç†å›¾ç‰‡ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™å°è¯•æ™ºèƒ½æ¨æ–­
        if args.figures_dir:
            manual_dir = Path(args.figures_dir).expanduser().resolve()
            if manual_dir.exists():
                images_dir = manual_dir
            else:
                print(f"âš ï¸  æŒ‡å®šçš„å›¾ç‰‡ç›®å½• {manual_dir} ä¸å­˜åœ¨ï¼Œå°†å°è¯•è‡ªåŠ¨æ£€æµ‹ç»“æœ")
        elif not images_dir:
            # å¦‚æœ find_markdown_and_images æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ç›®å½•ï¼Œå°è¯•æ™ºèƒ½æ¨æ–­
            inferred_dir = infer_figures_dir(str(md_file))
            if inferred_dir:
                images_dir = Path(inferred_dir)
                print(f"ğŸ” è‡ªåŠ¨æ¨æ–­å›¾ç‰‡ç›®å½•: {images_dir}")

        print(f"ğŸ“„ Markdown: {md_file.name}")
        if images_dir:
            img_count = len(list(images_dir.glob('*')))
            print(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {images_dir} ({img_count} ä¸ªæ–‡ä»¶)")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•")
        
        output_path = Path(args.output)
        if output_path.suffix == '.tex':
            output_tex = output_path
            output_dir = output_path.parent
        else:
            output_dir = output_path
            output_tex = output_dir / f"{md_file.stem.replace('_local', '_raw')}.tex"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if images_dir:
            img_count = copy_images_to_output(images_dir, output_dir)
            print(f"âœ… å·²å¤åˆ¶ {img_count} ä¸ªå›¾ç‰‡åˆ° {output_dir}/images/")
        
        title = args.title
        if title is None:
            input_path = Path(args.input)
            if input_path.is_dir():
                title = input_path.name
            else:
                title = md_file.stem.replace('_local', '')

        # ğŸ†• v1.6.3ï¼šæå– slug ç”¨äºé—®é¢˜æ—¥å¿—
        slug = md_file.stem.replace('_local', '').replace('_preprocessed', '').replace('_raw', '')

        print(f"\nğŸ“– æ­£åœ¨è½¬æ¢...")
        print(f"ğŸ“ æ ‡é¢˜: {title}")
        print(f"ğŸ·ï¸  Slug: {slug}")

        md_text = md_file.read_text(encoding='utf-8')
        tex_text = convert_md_to_examx(md_text, title, slug=slug, enable_issue_detection=True)
        
        # ğŸ†• v1.6 P0 ä¿®å¤ï¼šåå¤„ç†æ¸…ç†
        tex_text = fix_array_boundaries(tex_text)
        tex_text = clean_residual_image_attrs(tex_text)
        
        # ğŸ†• v1.3ï¼šéªŒè¯è¾“å‡º
        warnings = validate_latex_output(tex_text)
        integrity_issues = validate_math_integrity(tex_text)
        # ğŸ†• v1.8.6ï¼šèŠ±æ‹¬å·å¹³è¡¡æ£€æŸ¥
        brace_issues = validate_brace_balance(tex_text)
        
        output_tex.write_text(tex_text, encoding='utf-8')
        
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
        print("â”" * 60)
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶: {output_tex}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(tex_text):,} å­—èŠ‚")
        
        question_count = tex_text.count(r'\begin{question}')
        image_count = tex_text.count('IMAGE_TODO')
        print(f"ğŸ“‹ é¢˜ç›®æ•°é‡: {question_count}")
        if image_count > 0:
            print(f"ğŸ–¼ï¸  å›¾ç‰‡å ä½: {image_count}")
        
        # ğŸ†• v1.8.6ï¼šæ˜¾ç¤ºéªŒè¯ç»“æœï¼ˆåŒ…å«èŠ±æ‹¬å·æ£€æŸ¥ï¼‰
        if warnings or integrity_issues or brace_issues:
            combined = warnings + integrity_issues + brace_issues
            print(f"\nâš ï¸  éªŒè¯å‘ç° {len(combined)} ä¸ªæ½œåœ¨é—®é¢˜:")

            # åˆ†ç±»æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºå‰å‡ æ¡ï¼‰
            if warnings:
                print(f"  ğŸ“‹ ç»“æ„é—®é¢˜: {len(warnings)} ä¸ª")
                for issue in warnings[:3]:
                    print(f"    {issue}")
                if len(warnings) > 3:
                    print(f"    ... è¿˜æœ‰ {len(warnings) - 3} ä¸ª")

            if brace_issues:
                print(f"  ğŸ”§ èŠ±æ‹¬å·é—®é¢˜: {len(brace_issues)} ä¸ª")
                for issue in brace_issues[:3]:
                    print(f"    {issue}")
                if len(brace_issues) > 3:
                    print(f"    ... è¿˜æœ‰ {len(brace_issues) - 3} ä¸ª")

            if integrity_issues:
                print(f"  ğŸ”¢ æ•°å­¦å®šç•Œç¬¦é—®é¢˜: {len(integrity_issues)} ä¸ª")
                for issue in integrity_issues[:3]:
                    print(f"    {issue}")
                if len(integrity_issues) > 3:
                    print(f"    ... è¿˜æœ‰ {len(integrity_issues) - 3} ä¸ª")

            print("\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ AI Agent æ£€æŸ¥å¹¶äººå·¥ç¡®è®¤æ•°å­¦ç»“æ„")
        else:
            print(f"\nâœ… éªŒè¯é€šè¿‡ï¼šæœªå‘ç°æ˜æ˜¾é—®é¢˜")

        # ğŸ†• Prompt 1: å¼ºåˆ¶æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™
        if slug:
            print(f"\nğŸ” æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™...")
            try:
                assert_no_analysis_meta_in_auto_tex(slug)
                print(f"âœ… æœªå‘ç°ã€åˆ†æã€‘æ®‹ç•™")
            except RuntimeError as e:
                print(f"âŒ {e}")
                raise

        # æ¢å¤åŸæ•°å­¦å¤„ç†å‡½æ•°ï¼ˆè‹¥å¯ç”¨ legacyï¼‰
        if _orig_process is not None:
            process_text_for_latex = _orig_process  # type: ignore
        return 0
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


# ==================== ğŸ†• v1.6.3 æ–°å¢ï¼šç®€å•å•å…ƒæµ‹è¯• ====================

def run_self_tests() -> bool:
    """ğŸ†• v1.6.3ï¼šè¿è¡Œç®€å•çš„è‡ªæµ‹ç”¨ä¾‹

    Returns:
        True if all tests pass, False otherwise
    """
    print("ğŸ§ª è¿è¡Œè‡ªæµ‹ç”¨ä¾‹...")
    print("=" * 60)

    all_passed = True

    # æµ‹è¯• 1ï¼šã€åˆ†æã€‘æ®µè¢«æ­£ç¡®ä¸¢å¼ƒ
    print("\næµ‹è¯• 1: ã€åˆ†æã€‘æ®µè¢«æ­£ç¡®ä¸¢å¼ƒ")
    test_md = """
# ä¸€ã€å•é€‰é¢˜

1. æµ‹è¯•é¢˜ç›®

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€åˆ†æã€‘è¿™æ˜¯åˆ†æå†…å®¹ï¼Œåº”è¯¥è¢«ä¸¢å¼ƒ
ã€è¯¦è§£ã€‘è¿™æ˜¯è¯¦è§£å†…å®¹ï¼Œåº”è¯¥è¢«ä¿ç•™
ã€ç­”æ¡ˆã€‘A
"""
    result = convert_md_to_examx(test_md, "æµ‹è¯•", slug="", enable_issue_detection=False)
    if "ã€åˆ†æã€‘" in result:
        print("  âŒ FAILED: ã€åˆ†æã€‘æœªè¢«ä¸¢å¼ƒ")
        all_passed = False
    elif "è¿™æ˜¯åˆ†æå†…å®¹" in result:
        print("  âŒ FAILED: åˆ†æå†…å®¹æœªè¢«ä¸¢å¼ƒ")
        all_passed = False
    elif "è¿™æ˜¯è¯¦è§£å†…å®¹" not in result:
        print("  âŒ FAILED: è¯¦è§£å†…å®¹æœªè¢«ä¿ç•™")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 2ï¼šã€è¯¦è§£ã€‘è¢«æ­£ç¡®ä¿ç•™åˆ° \explain{}
    print("\næµ‹è¯• 2: ã€è¯¦è§£ã€‘è¢«æ­£ç¡®ä¿ç•™åˆ° \\explain{}")
    if "\\explain{" in result and "è¿™æ˜¯è¯¦è§£å†…å®¹" in result:
        print("  âœ… PASSED")
    else:
        print("  âŒ FAILED: è¯¦è§£æœªæ­£ç¡®ä¿ç•™")
        all_passed = False

    # æµ‹è¯• 3ï¼š*$x$* æ¨¡å¼è¢«æ­£ç¡®ä¿®å¤
    print("\næµ‹è¯• 3: *$x$* æ¨¡å¼è¢«æ­£ç¡®ä¿®å¤")
    test_text = "è¿™æ˜¯ä¸€ä¸ª *$x$* å˜é‡å’Œ *y* å¼ºè°ƒ"
    result_text = process_text_for_latex(test_text, is_math_heavy=True)
    if "*$" in result_text or "$*" in result_text:
        print(f"  âŒ FAILED: *$x$* æ¨¡å¼æœªè¢«ä¿®å¤")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    elif "\\emph{y}" not in result_text:
        print(f"  âŒ FAILED: *y* æœªè½¬æ¢ä¸º \\emph{{y}}")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 4ï¼šå…¨è§’æ‹¬å·è¢«ç»Ÿä¸€
    print("\næµ‹è¯• 4: å…¨è§’æ‹¬å·è¢«ç»Ÿä¸€")
    test_text = "è¿™æ˜¯ï¼ˆå…¨è§’æ‹¬å·ï¼‰å’Œï½›èŠ±æ‹¬å·ï½"
    result_text = normalize_fullwidth_brackets(test_text)
    if "ï¼ˆ" in result_text or "ï¼‰" in result_text or "ï½›" in result_text or "ï½" in result_text:
        print(f"  âŒ FAILED: å…¨è§’æ‹¬å·æœªè¢«ç»Ÿä¸€")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 5ï¼šç©º $$ è¢«æ¸…ç†
    print("\næµ‹è¯• 5: ç©º $$ è¢«æ¸…ç†")
    test_text = "è¿™æ˜¯ $$ ç©ºæ•°å­¦å’Œ $x$ æ­£å¸¸æ•°å­¦"
    result_text = fix_inline_math_glitches(test_text)
    if "$$" in result_text:
        print(f"  âŒ FAILED: ç©º $$ æœªè¢«æ¸…ç†")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 6ï¼šå†…è”å›¾ç‰‡è¢«æ­£ç¡®å¤„ç†ï¼ˆæ—§ç‰ˆï¼‰
    print("\næµ‹è¯• 6: å†…è”å›¾ç‰‡è¢«æ­£ç¡®å¤„ç†ï¼ˆæ—§ç‰ˆï¼‰")
    test_md = """
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥é›†åˆ![](image2.wmf)ï¼Œåˆ™ Aâˆ©B ç­‰äº

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€ç­”æ¡ˆã€‘A
"""
    result = convert_md_to_examx(test_md, "æµ‹è¯•", slug="", enable_issue_detection=False)
    # æ£€æŸ¥ï¼šä¸åº”è¯¥æœ‰æ®‹ç•™çš„ ![](image2.wmf)
    if "![](image2.wmf)" in result:
        print(f"  âŒ FAILED: å†…è”å›¾ç‰‡æ ‡è®°æœªè¢«è½¬æ¢")
        all_passed = False
    # æ£€æŸ¥ï¼šåº”è¯¥æœ‰ IMAGE_TODO æ³¨é‡Š
    elif "IMAGE_TODO" not in result or "image2.wmf" not in result:
        print(f"  âŒ FAILED: å†…è”å›¾ç‰‡æœªç”Ÿæˆ IMAGE_TODO å ä½ç¬¦")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 7ï¼šæ–°æ ¼å¼ IMAGE_TODO_START/END å ä½å—
    print("\næµ‹è¯• 7: æ–°æ ¼å¼ IMAGE_TODO_START/END å ä½å—")
    test_md_new = """
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥å‡½æ•° f(x) åœ¨åŒºé—´ [0,1] ä¸Šå•è°ƒé€’å¢ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š

![](media/graph1.png)

åˆ™ä¸‹åˆ—ç»“è®ºä¸­æ­£ç¡®çš„æ˜¯

A. f(0) < f(1)
B. f(0) > f(1)

ã€ç­”æ¡ˆã€‘A

2. é›†åˆ A={x|x>0}ï¼Œé›†åˆ B å¦‚å›¾![](media/venn.wmf)æ‰€ç¤ºï¼Œåˆ™ Aâˆ©B ç­‰äº

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€ç­”æ¡ˆã€‘B
"""
    result_new = convert_md_to_examx(test_md_new, "æµ‹è¯•æ–°æ ¼å¼", slug="test2025", enable_issue_detection=False)

    # æ£€æŸ¥1ï¼šä¸åº”è¯¥æœ‰æ®‹ç•™çš„ Markdown å›¾ç‰‡è¯­æ³•
    if "![](media/graph1.png)" in result_new or "![](media/venn.wmf)" in result_new:
        print(f"  âŒ FAILED: Markdown å›¾ç‰‡è¯­æ³•æœªè¢«è½¬æ¢")
        all_passed = False
    # æ£€æŸ¥2ï¼šåº”è¯¥æœ‰ä¸¤ä¸ª IMAGE_TODO_START æ ‡è®°
    elif result_new.count("IMAGE_TODO_START") != 2:
        print(f"  âŒ FAILED: IMAGE_TODO_START æ•°é‡ä¸æ­£ç¡® (æœŸæœ›2ä¸ªï¼Œå®é™…{result_new.count('IMAGE_TODO_START')}ä¸ª)")
        all_passed = False
    # æ£€æŸ¥3ï¼šåº”è¯¥æœ‰ä¸¤ä¸ª IMAGE_TODO_END æ ‡è®°
    elif result_new.count("IMAGE_TODO_END") != 2:
        print(f"  âŒ FAILED: IMAGE_TODO_END æ•°é‡ä¸æ­£ç¡®")
        all_passed = False
    # æ£€æŸ¥4ï¼šç¬¬ä¸€ä¸ªå›¾ç‰‡åº”è¯¥æ˜¯ç‹¬ç«‹å›¾ç‰‡ (inline=false)
    elif "inline=false" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°ç‹¬ç«‹å›¾ç‰‡æ ‡è®° (inline=false)")
        all_passed = False
    # æ£€æŸ¥5ï¼šç¬¬äºŒä¸ªå›¾ç‰‡åº”è¯¥æ˜¯å†…è”å›¾ç‰‡ (inline=true)
    elif "inline=true" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°å†…è”å›¾ç‰‡æ ‡è®° (inline=true)")
        all_passed = False
    # æ£€æŸ¥6ï¼šåº”è¯¥åŒ…å« question_index å­—æ®µ
    elif "question_index=" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ° question_index å­—æ®µ")
        all_passed = False
    # æ£€æŸ¥7ï¼šåº”è¯¥åŒ…å« AI_AGENT_REPLACE_ME æ ‡è®°
    elif "AI_AGENT_REPLACE_ME" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ° AI_AGENT_REPLACE_ME æ ‡è®°")
        all_passed = False
    # æ£€æŸ¥8ï¼šåº”è¯¥åŒ…å« CONTEXT_BEFORE æˆ– CONTEXT_AFTER
    elif "CONTEXT_BEFORE" not in result_new and "CONTEXT_AFTER" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ (CONTEXT_BEFORE/AFTER)")
        all_passed = False
    # æ£€æŸ¥9ï¼šID åº”è¯¥åŒ…å« slug å’Œé¢˜å·
    elif "test2025-Q1" not in result_new or "test2025-Q2" not in result_new:
        print(f"  âŒ FAILED: å›¾ç‰‡ ID æ ¼å¼ä¸æ­£ç¡® (åº”åŒ…å« slug-Q{n})")
        all_passed = False
    else:
        print("  âœ… PASSED")
        # æ‰“å°ä¸€ä¸ªç¤ºä¾‹ä¾›æ£€æŸ¥
        print("\n  ç¤ºä¾‹è¾“å‡ºç‰‡æ®µ:")
        lines = result_new.split('\n')
        for i, line in enumerate(lines):
            if 'IMAGE_TODO_START' in line:
                # æ‰“å°è¯¥è¡ŒåŠåç»­5è¡Œ
                for j in range(i, min(i+6, len(lines))):
                    print(f"    {lines[j]}")
                break

    # æµ‹è¯• 8ï¼šåå‘å®šç•Œç¬¦ç®€å•è‡ªåŠ¨ä¿®å¤ï¼ˆv1.8.8ï¼‰
    print("\n[è‡ªæµ‹] æµ‹è¯• 8: åå‘å®šç•Œç¬¦ç®€å•è‡ªåŠ¨ä¿®å¤")
    test_md_reversed = r"""
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥æ•°åˆ— a_n æ»¡è¶³ï¼Œå…¶ä¸­\) ï¼Œ\(x_i ä¸ºæ•´æ•°ã€‚

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€ç­”æ¡ˆã€‘A
"""
    result_reversed = convert_md_to_examx(test_md_reversed, "è‡ªæµ‹-åå‘", slug="selftest-reverse", enable_issue_detection=False)
    # æ£€æŸ¥ï¼šåŸå§‹çš„é”™è¯¯æ¨¡å¼ä¸åº”è¯¥å‡ºç°
    if r'\) ï¼Œ\(' in result_reversed:
        print("  âŒ FAILED: åå‘å®šç•Œç¬¦æœªè¢«ä¿®å¤")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 9ï¼šé¢˜å†… meta é‡å¤æ£€æµ‹ï¼ˆv1.8.8ï¼‰
    print("\n[è‡ªæµ‹] æµ‹è¯• 9: é¢˜å†… meta é‡å¤æ£€æµ‹")
    test_md_meta = """
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥å‡½æ•° f(x) = x^2 + 1 çš„æ€§è´¨

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€è¯¦è§£ã€‘ç¬¬ä¸€æ®µè¯¦è§£
ã€è¯¦è§£ã€‘ç¬¬äºŒæ®µè¯¦è§£

ã€ç­”æ¡ˆã€‘A
"""
    result_meta = convert_md_to_examx(test_md_meta, "è‡ªæµ‹-é‡å¤è¯¦è§£", slug="selftest-meta", enable_issue_detection=True)
    # æ£€æŸ¥ï¼šå¤šæ®µè¯¦è§£åº”è¯¥è¢«åˆå¹¶æˆ 1 ä¸ª \explain
    count_explain = result_meta.count(r'\explain{')
    if count_explain != 1:
        print(f"  âŒ FAILED: \\explain å®æ•°é‡ä¸º {count_explain}, é¢„æœŸä¸º 1")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 10ï¼šarray/cases æ–¹ç¨‹ç»„è¡¥ \left\{ï¼ˆv1.8.9ï¼‰
    print("\n[è‡ªæµ‹] æµ‹è¯• 10: _fix_array_left_braces å‡½æ•° - array ç¯å¢ƒ")
    # ç›´æ¥æµ‹è¯•å‡½æ•°ï¼Œé¿å…ä¾èµ–å¤æ‚çš„è½¬æ¢æµç¨‹
    test_block_array = r'\begin{array}{l} x + y = 1 \\ x - y = 3 \end{array} \right.'
    result_block_array = _fix_array_left_braces(test_block_array)
    if r'\left\{' in result_block_array and r'\begin{array}' in result_block_array:
        print("  âœ… PASSED")
    else:
        print(f"  âŒ FAILED: æœªè¡¥ä¸Š \\left\\{{")
        print(f"     è¾“å…¥: {test_block_array[:60]}...")
        print(f"     è¾“å‡º: {result_block_array[:60]}...")
        all_passed = False

    # æµ‹è¯• 11ï¼šcases æ–¹ç¨‹ç»„è¡¥ \left\{ï¼ˆv1.8.9ï¼‰
    print("\n[è‡ªæµ‹] æµ‹è¯• 11: _fix_array_left_braces å‡½æ•° - cases ç¯å¢ƒ")
    test_block_cases = r'f(x) = \begin{cases} x^2, & x > 0 \\ -x, & x \leq 0 \end{cases} \right.'
    result_block_cases = _fix_array_left_braces(test_block_cases)
    if r'\left\{' in result_block_cases and r'\begin{cases}' in result_block_cases:
        print("  âœ… PASSED")
    else:
        print(f"  âŒ FAILED: æœªè¡¥ä¸Š \\left\\{{")
        print(f"     è¾“å…¥: {test_block_cases[:60]}...")
        print(f"     è¾“å‡º: {result_block_cases[:60]}...")
        all_passed = False

    # æµ‹è¯• 12ï¼š_fix_array_left_braces å‡½æ•° - å·²æœ‰ \left çš„æƒ…å†µï¼ˆä¸åº”é‡å¤è¡¥ï¼‰
    print("\n[è‡ªæµ‹] æµ‹è¯• 12: _fix_array_left_braces å‡½æ•° - å·²æœ‰ \\left ä¸åº”é‡å¤è¡¥")
    test_block_exist = r'\left\{\begin{array}{l} x = 1 \\ y = 2 \end{array} \right.'
    result_block_exist = _fix_array_left_braces(test_block_exist)
    # åº”è¯¥åªæœ‰ä¸€ä¸ª \left\{
    left_brace_count = result_block_exist.count(r'\left\{')
    if left_brace_count == 1:
        print("  âœ… PASSED")
    else:
        print(f"  âŒ FAILED: \\left\\{{ æ•°é‡ä¸º {left_brace_count}, é¢„æœŸä¸º 1")
        print(f"     è¾“å‡º: {result_block_exist[:80]}...")
        all_passed = False

    # æµ‹è¯• 13ï¼šå•è¡Œå¤šé€‰é¡¹å±•å¼€ï¼ˆå« $$ï¼‰
    print("\næµ‹è¯• 13: å•è¡Œå¤šé€‰é¡¹å±•å¼€ï¼ˆå« $$ï¼‰")
    inline_choices = """> Aï¼$$\\left\\{2,3\\right\\}$$ Bï¼$$\\left\\{1,2\\right\\}$$
> Cï¼æ–‡å­—è¯´æ˜ Dï¼çº¯æ–‡æœ¬"""
    expanded = expand_inline_choices(inline_choices)
    expanded_lines = [ln for ln in expanded.splitlines() if re.match(r'^[A-D]', ln)]
    if len(expanded_lines) == 4 and all(ln.startswith(letter) for ln, letter in zip(expanded_lines, ['A', 'B', 'C', 'D'])):
        print("  âœ… PASSED")
    else:
        print(f"  âŒ FAILED: å•è¡Œé€‰é¡¹å±•å¼€ç»“æœå¼‚å¸¸: {expanded_lines}")
        all_passed = False

    # æµ‹è¯• 14ï¼šå›¾ç‰‡å±æ€§/è£…é¥°å›¾ç‰‡æ¸…ç†
    print("\næµ‹è¯• 14: å›¾ç‰‡å±æ€§æ¸…ç†ä¸è£…é¥°å›¾ç‰‡ç§»é™¤")
    attr_sample = '![](img.png){width="120px" height="60px"}'
    cleaned_attr = clean_image_attributes(attr_sample)
    tiny_sample = '![](tiny.png){width="1.0e-2in" height="1.0e-2in"}'
    removed_tiny = remove_decorative_images(tiny_sample)
    if '{' in cleaned_attr or 'width' in cleaned_attr:
        print("  âŒ FAILED: å›¾ç‰‡å±æ€§æœªè¢«æ¸…ç†")
        all_passed = False
    elif removed_tiny.strip():
        print("  âŒ FAILED: æå°è£…é¥°å›¾ç‰‡æœªè¢«ç§»é™¤")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 15ï¼šclean_explain_content ç©ºè¡Œå¤„ç†
    print("\næµ‹è¯• 15: clean_explain_content ç©ºè¡Œå¤„ç†")
    explain_text = "ç¬¬ä¸€æ®µå†…å®¹\n\nç¬¬äºŒæ®µ\n\n\nç¬¬ä¸‰æ®µ"
    cleaned_explain = clean_explain_content(explain_text)
    if "\\par" not in cleaned_explain or "\n\n" in cleaned_explain:
        print(f"  âŒ FAILED: clean_explain_content æœªæ­£ç¡®æ›¿æ¢ç©ºè¡Œ: {cleaned_explain}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 16ï¼š\\left/\\right å¹³è¡¡
    print("\næµ‹è¯• 16: balance_left_right_delimiters ä¿®å¤å­¤ç«‹å®šç•Œç¬¦")
    lr_sample = "\\left( x + y"
    lr_fixed = balance_left_right_delimiters(lr_sample)
    if "\\left" in lr_fixed:
        print(f"  âŒ FAILED: æœªé™çº§å­¤ç«‹ \\left: {lr_fixed}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 17ï¼šASCII è¡¨æ ¼è½¬æ¢
    print("\næµ‹è¯• 17: convert_ascii_table_blocks è½¬æ¢ç ´æŠ˜å·è¡¨æ ¼")
    ascii_table = """
  ---------------------- ----------------------
           çº§æ•°                   åç§°

            2                     è½»é£

  ---------------------- ----------------------
"""
    converted = convert_ascii_table_blocks(ascii_table)
    if "\\begin{tabular}" in converted:
        print("  âœ… PASSED")
    else:
        print("  âŒ FAILED: ASCII è¡¨æ ¼æœªè¢«è½¬æ¢")
        all_passed = False

    print("\n" + "=" * 60)
    if all_passed:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--selftest":
        success = run_self_tests()
        exit(0 if success else 1)
    else:
        exit(main())
