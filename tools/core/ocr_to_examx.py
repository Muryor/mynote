#!/usr/bin/env python3
# -*- coding: utf-8 -*-
r"""
ocr_to_examx_v1.8.py - v1.8 æ”¹è¿›ç‰ˆ

ğŸ†• v1.8 P0/P1 ä¿®å¤ï¼ˆ2025-11-20ï¼‰ï¼š
1. âœ… ä¿®å¤æ•°å­¦æ¨¡å¼è¾¹ç•Œè§£æé”™è¯¯ï¼š\right.\ $$ â†’ \right.\) ï¼ˆP0ï¼‰
   - ä¿®å¤åˆ†æ®µå‡½æ•°/çŸ©é˜µåç´§è·Ÿæ–‡æœ¬æ—¶çš„æ•°å­¦æ¨¡å¼é—­åˆé—®é¢˜
   - é¿å…ä¸­æ–‡æ–‡æœ¬è¢«é”™è¯¯åœ°æ”¾å…¥æ•°å­¦æ¨¡å¼
2. âœ… å¢å¼ºé¢˜å¹²ç¼ºå¤±æ£€æµ‹ï¼šè‡ªåŠ¨æ’å…¥ TODO æ³¨é‡Šï¼ˆP1ï¼‰
   - æ£€æµ‹ç›´æ¥ä» \item å¼€å§‹çš„é¢˜ç›®
   - åœ¨ \begin{question} åè‡ªåŠ¨æ·»åŠ è­¦å‘Šæ³¨é‡Š

v1.7 æ”¹è¿›ï¼ˆ2025-11-20ï¼‰ï¼š
1. âœ… é¢˜å¹²æ£€æµ‹ä¸è­¦å‘Šï¼šæ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®ï¼ˆç›´æ¥ä» \item å¼€å§‹ï¼‰
2. âœ… æ¸…ç† Markdown å›¾ç‰‡å±æ€§æ®‹ç•™ï¼šåˆ é™¤ height="..." å’Œ width="..." æ®‹ç•™
3. âœ… å°é—®ç¼–å·æ ¼å¼ç»Ÿä¸€ï¼šä¸è‡ªåŠ¨æ·»åŠ  \mathrmï¼Œä½¿ç”¨æ™®é€šæ–‡æœ¬
4. âœ… IMAGE_TODO å—åä¸æ·»åŠ ç©ºè¡Œï¼šä¼˜åŒ–æ ¼å¼
5. âœ… \explain ä¸­çš„ç©ºè¡Œè‡ªåŠ¨å¤„ç†ï¼šç©ºè¡Œæ›¿æ¢ä¸º \par

v1.6 P0 ä¿®å¤ï¼ˆ2025-11-19ï¼‰ï¼š
1. âœ… ä¿®å¤æ•°ç»„ç¯å¢ƒé—­åˆé”™è¯¯ï¼ˆ\right.\\) â†’ \right.\)ï¼‰
2. âœ… æ¸…ç†å›¾ç‰‡å±æ€§æ®‹ç•™ï¼ˆ{width="..." height="..."}ï¼‰

v1.5 æ ¸å¿ƒä¿®å¤ï¼ˆ2025-11-18ï¼‰ï¼š
1. âœ… å½»åº•ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆ$$\(...\)$$ â†’ \(...\)ï¼‰
   - æ”¹è¿› smart_inline_math é¿å…åµŒå¥—
   - æ–°å¢ fix_double_wrapped_math åå¤„ç†æ¸…ç†
   - ç»Ÿä¸€å°†æ‰€æœ‰ $$...$$ è½¬æ¢ä¸ºè¡Œå†… \(...\)ï¼ˆexamx å…¼å®¹ï¼‰
2. âœ… æ”¹è¿›å•è¡Œé€‰é¡¹å±•å¼€ï¼ˆ> A... B... C... D... â†’ å¤šè¡Œï¼‰
   - æ›´ç²¾ç¡®çš„é€‰é¡¹åˆ†å‰²æ­£åˆ™
   - ä¿ç•™é€‰é¡¹å†…çš„æ•°å­¦å…¬å¼å’Œæ ‡ç‚¹
3. âœ… å‡å°‘æ‰‹åŠ¨ä¿®æ­£å·¥ä½œé‡ï¼š2å°æ—¶ â†’ 15åˆ†é’Ÿ (ç›®æ ‡ -87.5%)

v1.4 æ”¹è¿›å›é¡¾ï¼š
- ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆåˆç‰ˆï¼‰
- è‡ªåŠ¨å±•å¼€å•è¡Œé€‰é¡¹ï¼ˆåˆç‰ˆï¼‰
- ç»Ÿä¸€æ•°å­¦å…¬å¼æ ¼å¼ï¼ˆ$$...$$ â†’ \(...\)ï¼‰

v1.3 æ”¹è¿›å›é¡¾ï¼š
- ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢
- æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™
- ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
- æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

ç‰ˆæœ¬ï¼šv1.7
ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2025-11-20
"""

import re
import argparse
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from enum import Enum, auto  # å¼•å…¥æšä¸¾æ”¯æŒï¼ˆçŠ¶æ€æœºéœ€è¦ï¼‰

# ==================== æ•°å­¦çŠ¶æ€æœºï¼ˆæ¥è‡ª ocr_to_examx_complete.pyï¼‰ ====================
# æ³¨æ„ï¼šæ­¤çŠ¶æ€æœºå®Œå…¨å–ä»£åŸå…ˆåŸºäºæ­£åˆ™çš„ smart_inline_math / sanitize_math ç­‰ç®¡çº¿ã€‚
# æ—§å‡½æ•°ä¿ç•™ä½†æ ‡è®°ä¸º DEPRECATEDï¼Œä¸»æµç¨‹ä¸å†è°ƒç”¨ï¼Œé¿å…ç›¸äº’å¹²æ‰°ã€‚

class TokenType(Enum):
    TEXT = auto()
    DOLLAR_SINGLE = auto()
    DOLLAR_DOUBLE = auto()
    LATEX_OPEN = auto()
    LATEX_CLOSE = auto()
    RIGHT_BOUNDARY = auto()
    NEWLINE = auto()
    EOF = auto()


class MathStateMachine:
    r"""æ•°å­¦æ¨¡å¼çŠ¶æ€æœº - ç»Ÿä¸€è§£æ/è§„èŒƒæ‰€æœ‰æ•°å­¦å®šç•Œç¬¦

    è®¾è®¡ç›®æ ‡ï¼š
    1. æ”¯æŒæ··åˆå‡ºç°çš„ $ ... $ã€$$ ... $$ã€\( ... \) ä»¥åŠ OCR ç”Ÿæˆçš„ \right. $$ ç­‰ç•¸å½¢è¾¹ç•Œ
    2. å°†æ‰€æœ‰æ˜¾ç¤º/è¡Œå†…æ•°å­¦ç»Ÿä¸€è§„èŒƒä¸ºè¡Œå†…å½¢å¼ï¼š\( ... \)ï¼ˆä¸ examx åŒ…å…¼å®¹ï¼‰
    3. ä¿æŒå·²æœ‰æ­£ç¡®çš„ \( ... \) / \) ä¸è¢«äºŒæ¬¡åŒ…è£¹
    4. é˜²æ­¢è·¨è¡Œå•ç¾å…ƒæœªé—­åˆé€ æˆåå¹¶åç»­æ–‡æœ¬
    """

    def tokenize(self, text: str) -> List:
        tokens = []
        i = 0
        n = len(text)
        while i < n:
            # ğŸ”¥ v1.8.3ï¼šå¢å¼º \right. åçš„ OCR è¾¹ç•Œæ£€æµ‹
            # å¤„ç† \right. åå¯èƒ½è·Ÿéšçš„å„ç§ç•¸å½¢æ ¼å¼ï¼š
            # - \right. $$
            # - \right.\ $$
            # - \right. \ $$
            # - \right.  $$
            if text[i:].startswith(r'\right.'):
                j = i + 7  # è·³è¿‡ \right.
                # è·³è¿‡æ‰€æœ‰ç©ºç™½ã€åæ–œæ ã€ç©ºæ ¼çš„ç»„åˆ
                while j < n and text[j] in ' \t\n\\':
                    j += 1
                # æ£€æŸ¥æ˜¯å¦ç´§è·Ÿ $$
                if j < n - 1 and text[j:j+2] == '$$':
                    tokens.append((TokenType.RIGHT_BOUNDARY, r'\right.', i))
                    i = j + 2  # è·³è¿‡ $$
                    continue
                else:
                    # ä¸æ˜¯ OCR è¾¹ç•Œé”™è¯¯ï¼Œä¿æŒåŸæ ·
                    tokens.append((TokenType.TEXT, r'\right.', i))
                    i += 7
                    continue

            # $$ æ˜¾ç¤ºæ•°å­¦
            if i < n - 1 and text[i:i+2] == '$$':
                tokens.append((TokenType.DOLLAR_DOUBLE, '$$', i))
                i += 2
                continue

            # å• $ è¡Œå†…æ•°å­¦
            if text[i] == '$':
                tokens.append((TokenType.DOLLAR_SINGLE, '$', i))
                i += 1
                continue

            # \( ä¸ \)
            if i < n - 1 and text[i:i+2] == r'\(':
                tokens.append((TokenType.LATEX_OPEN, r'\(', i))
                i += 2
                continue
            if i < n - 1 and text[i:i+2] == r'\)':
                tokens.append((TokenType.LATEX_CLOSE, r'\)', i))
                i += 2
                continue

            # æ™®é€šæ–‡æœ¬å—æ”¶é›†
            j = i
            while j < n:
                if text[j] in '$\n':
                    break
                if j < n - 1 and text[j:j+2] in [r'\(', r'\)', '$$']:
                    break
                if text[j:].startswith(r'\right.'):
                    break
                j += 1
            if j > i:
                tokens.append((TokenType.TEXT, text[i:j], i))
                i = j
            else:
                tokens.append((TokenType.TEXT, text[i], i))
                i += 1
        return tokens

    def process(self, text: str) -> str:
        tokens = self.tokenize(text)
        out = []
        i = 0
        math_depth = 0  # è·Ÿè¸ªæ•°å­¦æ¨¡å¼æ·±åº¦

        while i < len(tokens):
            t_type, val, pos = tokens[i]

            # ğŸ”¥ v1.8.3ï¼šæ™ºèƒ½å¤„ç† \right. è¾¹ç•Œ
            if t_type == TokenType.RIGHT_BOUNDARY:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ•°å­¦æ¨¡å¼å†…ï¼ˆæœ‰æœªé—­åˆçš„ \(ï¼‰
                if math_depth > 0:
                    out.append(r'\right.\)')
                    math_depth -= 1
                else:
                    # ä¸åœ¨æ•°å­¦æ¨¡å¼å†…ï¼Œä¿æŒåŸæ ·ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ \right.ï¼‰
                    out.append(r'\right.')
                i += 1
                continue
            if t_type == TokenType.DOLLAR_DOUBLE:
                # æ”¶é›†ç›´åˆ°ä¸‹ä¸€ä¸ª $$
                i += 1
                buf = []
                while i < len(tokens):
                    tt, tv, _ = tokens[i]
                    if tt == TokenType.DOLLAR_DOUBLE:
                        i += 1
                        break
                    buf.append(tv)
                    i += 1
                out.append(r'\(' + ''.join(buf).strip() + r'\)')
                continue

            if t_type == TokenType.DOLLAR_SINGLE:
                i += 1
                buf = []
                while i < len(tokens):
                    tt, tv, _ = tokens[i]
                    if tt == TokenType.DOLLAR_SINGLE:
                        i += 1
                        break
                    # ç¦æ­¢è·¨è¡Œçš„å•ç¾å…ƒå»¶ä¼¸
                    if '\n' in tv:
                        out.append('$')
                        out.extend(buf)
                        break
                    buf.append(tv)
                    i += 1
                if buf:
                    out.append(r'\(' + ''.join(buf) + r'\)')
                continue

            if t_type == TokenType.LATEX_OPEN:
                out.append(val)
                math_depth += 1
                i += 1
                continue

            if t_type == TokenType.LATEX_CLOSE:
                out.append(val)
                math_depth = max(0, math_depth - 1)
                i += 1
                continue
            out.append(val)
            i += 1
        return ''.join(out)


# å•ä¾‹å®ä¾‹ä¾›å…¨å±€è°ƒç”¨
math_sm = MathStateMachine()


# ==================== é…ç½® ====================

VERSION = "v1.8.3"

SECTION_MAP = {
    "ä¸€ã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å¤šé€‰é¢˜": "å¤šé€‰é¢˜",
    "ä¸‰ã€å¡«ç©ºé¢˜": "å¡«ç©ºé¢˜",
    "å››ã€è§£ç­”é¢˜": "è§£ç­”é¢˜",
}

META_PATTERNS = {
    "answer": r"^ã€ç­”æ¡ˆã€‘(.*)$",
    "difficulty": r"^ã€éš¾åº¦ã€‘([\d.]+)",
    "topics": r"^ã€çŸ¥è¯†ç‚¹ã€‘(.*)$",
    "analysis": r"^ã€åˆ†æã€‘(.*)$",
    "explain": r"^ã€è¯¦è§£ã€‘(.*)$",
}

# ğŸ†• æ‰©å±•å›¾ç‰‡æ£€æµ‹ï¼šæ”¯æŒç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ã€å¤šè¡Œå±æ€§å—
# åŒ¹é…ä¸¤ç§å½¢å¼ï¼š
#   1) å¸¦ID: ![@@@id](path){...}
#   2) æ— ID: ![](path){...}
# å±æ€§å—å¯è·¨å¤šè¡Œï¼Œå¯é€‰
IMAGE_PATTERN_WITH_ID = re.compile(
    r"!\[@@@([^\]]+)\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
IMAGE_PATTERN_NO_ID = re.compile(
    r"!\[\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
# å…¼å®¹æ—§ç‰ˆï¼ˆä¿ç•™ç”¨äºç®€å•åœºæ™¯ï¼‰
IMAGE_PATTERN = re.compile(r"!\[\]\((images/[^)]+)\)(?:\{width=(\d+)%\})?")

LATEX_SPECIAL_CHARS = {
    "%": r"\%",
    "&": r"\&",
    "#": r"\#",
    "~": r"\textasciitilde{}",
}

# è§£ææ ‡è®°è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼‰
ANALYSIS_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—',
    'ç»¼ä¸Š', 'æ•…', 'å³', 'åˆ™', 'å¯å¾—'
]


# æ›´ä¸¥æ ¼çš„è§£æèµ·å§‹è¯ï¼Œåªç”¨äºåˆ¤æ–­æ˜¯å¦è¿›å…¥è§£ææ®µè½ï¼ˆé¿å…åƒâ€œåˆ™â€è¿™æ ·åœ¨é¢˜å¹²ä¸­å‡ºç°æ—¶è¢«è¯¯åˆ¤ï¼‰
ANALYSIS_START_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—', 'ç»¼ä¸Š'
]

# ==================== æ–‡ä»¶å¤¹å¤„ç†å‡½æ•° ====================

def find_markdown_and_images(input_path: Path) -> Tuple[Path, Optional[Path]]:
    """æ™ºèƒ½è¯†åˆ«è¾“å…¥è·¯å¾„"""
    input_path = Path(input_path).resolve()
    
    if input_path.is_file() and input_path.suffix == '.md':
        md_file = input_path
        images_dir = input_path.parent / 'images'
        if not images_dir.exists():
            images_dir = None
        return md_file, images_dir
    
    if input_path.is_dir():
        md_files = list(input_path.glob('*_local.md'))
        if not md_files:
            md_files = list(input_path.glob('*.md'))
        
        if not md_files:
            raise FileNotFoundError(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        
        if len(md_files) > 1:
            print(f"âš ï¸  æ‰¾åˆ°å¤šä¸ª .md æ–‡ä»¶ï¼Œä½¿ç”¨ï¼š{md_files[0].name}")
        
        md_file = md_files[0]
        images_dir = input_path / 'images'
        if not images_dir.exists():
            images_dir = None
        
        return md_file, images_dir
    
    raise ValueError(f"æ— æ•ˆçš„è¾“å…¥ï¼š{input_path}")


def copy_images_to_output(images_dir: Path, output_dir: Path) -> int:
    """å¤åˆ¶å›¾ç‰‡"""
    if images_dir is None or not images_dir.exists():
        return 0
    
    output_images_dir = output_dir / 'images'
    if output_images_dir.exists():
        shutil.rmtree(output_images_dir)
    
    shutil.copytree(images_dir, output_images_dir)
    return len(list(output_images_dir.glob('*')))


# ==================== LaTeX å¤„ç†å‡½æ•° ====================

def escape_latex_special(text: str, in_math_mode: bool = False) -> str:
    """è½¬ä¹‰ LaTeX ç‰¹æ®Šå­—ç¬¦"""
    if in_math_mode:
        for char in ["%", "&", "#", "~"]:
            if char in LATEX_SPECIAL_CHARS:
                text = text.replace(char, LATEX_SPECIAL_CHARS[char])
    else:
        protected = []
        def save_comment(match):
            protected.append(match.group(0))
            return f"@@COMMENT_{len(protected)-1}@@"
        text = re.sub(r'%.*$', save_comment, text, flags=re.MULTILINE)
        
        for char, escaped in LATEX_SPECIAL_CHARS.items():
            text = text.replace(char, escaped)
        
        for i, comment in enumerate(protected):
            text = text.replace(f"@@COMMENT_{i}@@", comment)
    return text


# DEPRECATED: å·²è¢« MathStateMachine æ›¿æ¢ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§æµ‹è¯•ï¼›ä¸»æµç¨‹ä¸å†è°ƒç”¨
def smart_inline_math(text: str) -> str:
    r"""æ™ºèƒ½è½¬æ¢è¡Œå†…å…¬å¼ï¼š$...$ -> \(...\)ï¼Œ$$...$$ -> \(...\)

    ğŸ†• v1.5 æ”¹è¿›ï¼šå½»åº•é¿å…åŒé‡åŒ…è£¹ï¼Œexamx ç»Ÿä¸€ä½¿ç”¨ \(...\)

    æ³¨æ„ï¼šæ‰€æœ‰ $$...$$ æ˜¾ç¤ºå…¬å¼éƒ½ä¼šè¢«è½¬æ¢ä¸ºè¡Œå†… \(...\) æ ¼å¼ï¼Œ
    è¿™æ˜¯ä¸ºäº†ä¸ examx åŒ…çš„å…¼å®¹æ€§ã€‚å¦‚æœéœ€è¦çœŸæ­£çš„æ˜¾ç¤ºå…¬å¼ï¼Œ
    åº”åœ¨åç»­æ‰‹åŠ¨è°ƒæ•´ä¸º \[...\] æ ¼å¼ã€‚
    """
    if not text:
        return text
    
    # æ­¥éª¤1: ä¿æŠ¤å·²æœ‰çš„è¡Œå†…å…¬å¼ \(...\)ï¼ˆé¿å…é‡å¤è½¬æ¢ï¼‰
    inline_math_blocks = []
    def save_inline(match):
        inline_math_blocks.append(match.group(0))
        return f"@@INLINEMATH{len(inline_math_blocks)-1}@@"
    text = re.sub(r'\\\((.+?)\\\)', save_inline, text, flags=re.DOTALL)
    
    # æ­¥éª¤2: ä¿æŠ¤å·²æœ‰çš„æ˜¾ç¤ºå…¬å¼ \[...\]ï¼ˆä¿æŒä¸å˜ï¼‰
    display_math_blocks = []
    def save_display(match):
        display_math_blocks.append(match.group(0))
        return f"@@DISPLAYMATH{len(display_math_blocks)-1}@@"
    text = re.sub(r'\\\[(.+?)\\\]', save_display, text, flags=re.DOTALL)
    
    # æ­¥éª¤3: ä¿æŠ¤TikZåæ ‡ $(A)$ æˆ– $(A)!0.5!(B)$ æˆ– $(A)+(1,2)$
    tikz_coords = []
    def save_tikz_coord(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)!0.5!(B)$' æˆ– '$(0,1)$'
        inner = block[2:-2]         # å»æ‰å¤–å±‚ '$(' å’Œ ')$'
        # ä»…å½“å†…éƒ¨åŒ…å« '!' æˆ– å¤§å†™å­—æ¯ æ—¶ï¼Œè®¤ä¸ºæ˜¯ TikZ åæ ‡è¡¨è¾¾å¼
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZCOORD{len(tikz_coords)-1}@@"
        else:
            # å¦åˆ™è®¤ä¸ºæ˜¯æ™®é€šæ•°å­¦åæ ‡/åŒºé—´ï¼ŒåŸæ ·è¿”å›
            return block
    # åŒ¹é… TikZ åæ ‡ï¼š$(...)$ å†…éƒ¨æ˜¯ç®€å•çš„åæ ‡è®¡ç®—è¡¨è¾¾å¼
    # åŒ…å«å­—æ¯ã€æ•°å­—ã€æ‹¬å·ã€åŠ å‡ä¹˜é™¤ã€ç‚¹ã€æ„Ÿå¹å·ã€å†’å·ç­‰ä½†ä¸åŒ…å«å¤æ‚æ•°å­¦
    text = re.sub(r'\$\([A-Za-z0-9!+\-*/\.\(\):,\s]+\)\$', save_tikz_coord, text)

    # æ­¥éª¤3.5: ğŸ†• v1.8 ä¿®å¤ \right.\ $$ è¾¹ç•Œé—®é¢˜
    # å°† \right.\ $$ è½¬æ¢ä¸º \right.\) ï¼ˆé—­åˆå½“å‰æ•°å­¦æ¨¡å¼ï¼‰
    text = re.sub(r'\\right\.\\\s+\$\$', r'\\right.\\) ', text)

    # æ­¥éª¤4: è½¬æ¢æ˜¾ç¤ºå…¬å¼ $$ ... $$ ä¸º \(...\)ï¼ˆexamx ç»Ÿä¸€é£æ ¼ï¼‰
    # æ³¨æ„ï¼šæ‰€æœ‰ $$...$$ éƒ½è½¬ä¸ºè¡Œå†…æ ¼å¼ï¼Œä¸ç”Ÿæˆ \[...\]
    text = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # æ­¥éª¤5: è½¬æ¢å• $ ... $ ä¸º \(...\)
    text = re.sub(r'(?<!\\)\$([^\$]+?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤6: å…œåº•æ£€æŸ¥ï¼Œæ¸…ç†æ®‹ç•™çš„å• $ï¼ˆå•è¡Œå†…ï¼Œé™åˆ¶200å­—ç¬¦ï¼‰
    text = re.sub(r'(?<!\\)\$([^\$\n]{1,200}?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤7: æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, block in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZCOORD{i}@@", block)
    for i, block in enumerate(display_math_blocks):
        text = text.replace(f"@@DISPLAYMATH{i}@@", block)
    for i, block in enumerate(inline_math_blocks):
        text = text.replace(f"@@INLINEMATH{i}@@", block)
    
    return text


# DEPRECATED: å·²è¢« MathStateMachine ç»Ÿä¸€å¤„ç†åŒé‡åŒ…è£¹
def fix_double_wrapped_math(text: str) -> str:
    r"""ä¿®æ­£åŒé‡åŒ…è£¹çš„æ•°å­¦å…¬å¼
    
    ğŸ†• v1.6 å¢å¼ºï¼šæ¸…ç†æ›´å¤šåµŒå¥—æ¨¡å¼
    ğŸ†• v1.5 æ–°å¢ï¼šæ¸…ç†å¯èƒ½æ®‹ç•™çš„åµŒå¥—æ ¼å¼
    ä¾‹å¦‚ï¼š$$\(...\)$$ â†’ \(...\)
    """
    if not text:
        return text
    
    # ä¿®æ­£ $$\(...\)$$ æˆ– $$\[...\]$$
    # æ³¨æ„ï¼š\\\( åŒ¹é…å­—é¢çš„ \(
    text = re.sub(r'\$\$\s*\\\((.+?)\\\)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\$\s*\\\[(.+?)\\\]\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ $\(...\)$ æˆ– $\[...\]$
    text = re.sub(r'\$\s*\\\((.+?)\\\)\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\s*\\\[(.+?)\\\]\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ä¸‰é‡åµŒå¥—ï¼ˆæç«¯æƒ…å†µï¼‰
    text = re.sub(r'\\\(\s*\\\((.+?)\\\)\s*\\\)', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† \because\(\) æˆ– \therefore\(\) çš„ç©ºåµŒå¥—
    # æ³¨æ„ï¼šæ›¿æ¢åä¿ç•™ç©ºæ ¼ï¼Œé¿å…ä¸åç»­å­—æ¯è¿æ¥
    text = re.sub(r'\\because\s*\\\(\\\)\s*', r'\\because ', text)
    text = re.sub(r'\\therefore\s*\\\(\\\)\s*', r'\\therefore ', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† \(\because\(\) æˆ– \(\therefore\(\) å½¢å¼
    text = re.sub(r'\\\(\\because\s*\\\(\\\)\s*', r'\\(\\because ', text)
    text = re.sub(r'\\\(\\therefore\s*\\\(\\\)\s*', r'\\(\\therefore ', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç†ç‹¬ç«‹çš„ç©ºæ‹¬å· \(\)ï¼ˆå¯èƒ½å‡ºç°åœ¨ä»»ä½•ä½ç½®ï¼‰
    text = re.sub(r'\\\(\s*\\\)', r'', text)
    
    # ğŸ†• v1.6 P0 ä¿®å¤ï¼šä¿®æ­£ \(...\(\)...\) å½¢å¼çš„åµŒå¥—ï¼ˆç©ºå ä½ç¬¦ï¼‰
    # è¿­ä»£æ¸…ç†ï¼Œæœ€å¤š3æ¬¡
    for _ in range(3):
        before = text
        text = re.sub(r'\\\(([^)]*?)\\\(\\\)([^)]*?)\\\)', r'\\(\1\2\\)', text, flags=re.DOTALL)
        if text == before:
            break
    
    return text


def fix_array_boundaries(text: str) -> str:
    r"""ä¿®å¤ array ç¯å¢ƒçš„è¾¹ç•Œç¬¦é”™è¯¯
    
    ğŸ†• v1.6 P0 ä¿®å¤ï¼šä¿®æ­£ \right.\\) â†’ \right.\)
    """
    # ä¿®æ­£ \right. åçš„åŒåæ–œæ 
    text = re.sub(r'\\right\.\\\\\)', r'\\right.\\)', text)
    
    # ä¿®æ­£å…¶ä»–è¾¹ç•Œç¬¦
    text = re.sub(r'\\right\)\\\\\)', r'\\right)\\)', text)
    text = re.sub(r'\\right\]\\\\\)', r'\\right]\\)', text)
    text = re.sub(r'\\right\}\\\\\)', r'\\right}\\)', text)
    
    # åŒæ ·ä¿®æ­£ \left çš„æƒ…å†µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    text = re.sub(r'\\\\\(\\left', r'\\(\\left', text)
    
    return text


def clean_residual_image_attrs(text: str) -> str:
    r"""æ¸…ç†æ®‹ç•™çš„å›¾ç‰‡å±æ€§å—

    ğŸ†• v1.7 å¢å¼ºï¼šæ¸…ç†æ›´å¤š Markdown å›¾ç‰‡å±æ€§æ®‹ç•™
    ğŸ†• v1.6 P0 ä¿®å¤ï¼šæ¸…ç† Pandoc ç”Ÿæˆçš„å›¾ç‰‡å±æ€§
    """
    # æ¸…ç†å•ç‹¬æˆè¡Œçš„å±æ€§å—å¼€å§‹
    text = re.sub(r'^\s*\{width="[^"]*"\s*$', '', text, flags=re.MULTILINE)
    # æ¸…ç†å•ç‹¬æˆè¡Œçš„å±æ€§å—ç»“æŸ
    text = re.sub(r'^\s*height="[^"]*"\}\s*$', '', text, flags=re.MULTILINE)

    # æ¸…ç†è·¨è¡Œçš„å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s*\n\s*height="[^"]*"\}', '', text, flags=re.MULTILINE)

    # æ¸…ç†å•è¡Œå®Œæ•´å±æ€§å—
    text = re.sub(r'\{width="[^"]*"\s+height="[^"]*"\}', '', text)

    # ğŸ†• v1.7ï¼šæ¸…ç†æ®‹ç•™çš„ height="..." å’Œ width="..." ï¼ˆå¸¦å¯èƒ½çš„å°¾éš }ï¼‰
    text = re.sub(r'height="[^"]*"[}]*', '', text)
    text = re.sub(r'width="[^"]*"[}]*', '', text)

    return text


# DEPRECATED: çŠ¶æ€æœºåä¸å†éœ€è¦å˜é‡è‡ªåŠ¨åŒ…è£¹ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦åŒ…è£¹
def wrap_math_variables(text: str) -> str:
    """æ™ºèƒ½åŒ…è£¹æ•°å­¦å˜é‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    # ä¿æŠ¤å·²æœ‰çš„æ•°å­¦æ¨¡å¼
    protected = []
    def save_math(match):
        protected.append(match.group(0))
        return f"@@MATH{len(protected)-1}@@"
    
    text = re.sub(r'\\\(.*?\\\)', save_math, text, flags=re.DOTALL)
    text = re.sub(r'\\\[.*?\\\]', save_math, text, flags=re.DOTALL)
    
    # ä¿æŠ¤ TikZ åæ ‡
    tikz_coords = []
    def save_tikz(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)$' æˆ– '$(0,1)$'
        inner = block[2:-2]
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZ{len(tikz_coords)-1}@@"
        else:
            return block
    text = re.sub(r'\$\([\d\w\s,+\-*/\.]+\)\$', save_tikz, text)
    
    # è§„åˆ™1ï¼šå•å­—æ¯å˜é‡ + è¿ç®—ç¬¦/ä¸‹æ ‡/ä¸Šæ ‡
    text = re.sub(
        r'\b([a-zA-Z])(?=\s*[=+\-*/^<>]|_{|\^{)',
        r'\\(\1\\)',
        text
    )
    
    # è§„åˆ™2ï¼šæ•°å­¦å‡½æ•°å¿…é¡»æœ‰åæ–œæ 
    math_functions = [
        'sin', 'cos', 'tan', 'cot', 'sec', 'csc',
        'arcsin', 'arccos', 'arctan',
        'sinh', 'cosh', 'tanh',
        'log', 'ln', 'lg', 'exp',
        'lim', 'sup', 'inf',
        'max', 'min', 'det', 'dim', 'ker'
    ]
    for func in math_functions:
        text = re.sub(rf'(?<!\\)\b{func}\b(?!\w)', rf'\\{func}', text)
    
    # è§„åˆ™3ï¼šè™šæ•°å•ä½ iï¼ˆé¿å…è¯¯è½¬æ¢ç½—é©¬æ•°å­—ï¼‰
    # åªåœ¨æ˜ç¡®çš„æ•°å­¦ä¸Šä¸‹æ–‡ä¸­è½¬æ¢ï¼Œé¿å… (i), (ii) ç­‰ç½—é©¬æ•°å­—è¢«è½¬æ¢
    # åŒ¹é…ï¼šç‹¬ç«‹çš„ i åé¢è·Ÿç€æ•°å­¦è¿ç®—ç¬¦æˆ–ç»“æŸï¼Œä½†ä¸åœ¨æ‹¬å·å†…
    text = re.sub(r'(?<!\\)(?<!\()\bi\b(?=[^a-zA-Z\)])', r'\\mathrm{i}', text)
    
    # æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, coord in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZ{i}@@", coord)
    for i, math in enumerate(protected):
        text = text.replace(f"@@MATH{i}@@", math)
    
    return text


def _sanitize_math_block(block: str) -> str:
    """ä¿®æ­£æ•°å­¦å—å†…éƒ¨çš„ OCR é”™è¯¯
    
    ä¿®å¤ï¼š
    - \\left / \\right ä¸åŒ¹é…æ—¶é™çº§ä¸ºæ™®é€šæ‹¬å·
    - \\right.\\ ) ç­‰ç•¸å½¢ç»„åˆ
    """
    if not block:
        return block
    
    # ç»Ÿä¸€æ•°å­¦ç¯å¢ƒå†…çš„ä¸­æ–‡æ ‡ç‚¹ä¸ºè‹±æ–‡æ ‡ç‚¹
    block = (block
             .replace('ï¼Œ', ',')
             .replace('ï¼š', ':')
             .replace('ï¼›', ';')
             .replace('ã€‚', '.')
             .replace('ã€', ','))

    # æ›¿æ¢å¸¸è§çš„ Unicode ç¬¦å·ä¸º LaTeX å‘½ä»¤ï¼ˆé¿å…ç¼ºå­—å½¢ï¼‰
    block = block.replace('âˆµ', r'\\because').replace('âˆ´', r'\\therefore')

    # å°†ä¸Šä¸‹æ ‡ä¸­çš„ä¸­æ–‡åŒ…è£…ä¸º \text{...}
    # å½¢å¼ä¸€ï¼š_[{...ä¸­æ–‡...}] æˆ– ^[{...ä¸­æ–‡...}]
    def _wrap_cjk_in_braced_subsup(m: re.Match) -> str:
        lead = m.group(1)
        inner = m.group(2)
        if '\\text{' in inner:
            return f"{lead}{{{inner}}}"
        return f"{lead}{{\\text{{{inner}}}}}"
    block = re.sub(r'([_^])\{([^{}]*?[\u4e00-\u9fff]+[^{}]*?)\}', _wrap_cjk_in_braced_subsup, block)

    # å½¢å¼äºŒï¼šå•å­—ç¬¦ä¸Šä¸‹æ ‡ï¼š_æ°´ æˆ– ^é«˜
    block = re.sub(r'([_^])([\u4e00-\u9fff])', r'\1{\\text{\2}}', block)

    # æ•°å­¦å†…å¸¸è§ä¸­æ–‡è¿æ¥è¯ï¼Œæ›¿æ¢ä¸º \text{...}ï¼ˆä¿å®ˆé›†ï¼‰
    for w in ['ä¸”', 'æˆ–', 'åˆ™', 'å³', 'æ•…', 'æ‰€ä»¥', 'å› ä¸º']:
        block = re.sub(fr'(?<!\\text\{{){re.escape(w)}(?![^\{{]*\}})', rf'\\text{{{w}}}', block)
    
    # ç»Ÿè®¡ left/right æ•°é‡
    left_count = len(re.findall(r'\\left\b', block))
    right_count = len(re.findall(r'\\right\b', block))
    
    # ä¿®å¤ç•¸å½¢ \right.\ ) å’Œ \right.\\)
    block = re.sub(r'\\right\.\s*\\\s*\)', r'\\right.', block)
    # ä¿®å¤ \right.\\\) æ¨¡å¼ï¼ˆarrayç»“å°¾çš„å¸¸è§OCRé”™è¯¯ï¼‰
    block = re.sub(r'\\right\.\\\\+\)', r'\\right.', block)
    
    # å¦‚æœ left/right ä¸åŒ¹é…ï¼Œé™çº§ä¸ºæ™®é€šæ‹¬å·
    if left_count != right_count:
        block = re.sub(r'\\left\s*([\(\[\{])', r'\1', block)
        block = re.sub(r'\\right\s*([\)\]\}])', r'\1', block)
        block = re.sub(r'\\left\.', '', block)
        block = re.sub(r'\\right\.', '', block)
    
    return block


# DEPRECATED: çŠ¶æ€æœºå·²å¤„ç†æ•°å­¦å®šç•Œç¬¦ä¸ OCR è¾¹ç•Œï¼Œæ­¤å‡½æ•°ä»…ä¿ç•™å…¼å®¹æ€§
def sanitize_math(text: str) -> str:
    """æ‰«æå…¨æ–‡ï¼Œä»…ä¿®æ­£æ•°å­¦ç¯å¢ƒå†…çš„ OCR é”™è¯¯
    
    åªå¤„ç† \\(...\\) å’Œ \\[...\\] å†…éƒ¨çš„å†…å®¹ã€‚
    """
    if not text:
        return text
    
    result = []
    i = 0
    n = len(text)
    
    while i < n:
        # åŒ¹é… \(..\)
        if text.startswith(r"\(", i):
            j = text.find(r"\)", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\(" + inner + r"\)")
            i = j + 2
            continue
        
        # åŒ¹é… \[..\]
        if text.startswith(r"\[", i):
            j = text.find(r"\]", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\[" + inner + r"\]")
            i = j + 2
            continue
        
        result.append(text[i])
        i += 1
    
    return "".join(result)


def remove_blank_lines_in_macro_args(text: str) -> str:
    """åˆ é™¤å®å‚æ•°ä¸­çš„ç©ºè¡Œï¼ˆå¢å¼ºç‰ˆï¼‰"""
    macros = ['explain', 'topics', 'answer', 'difficulty', 'source']
    
    for macro in macros:
        pattern = rf'(\\{macro}\{{)([^{{}}]*(?:\{{[^{{}}]*\}}[^{{}}]*)*?)(\}})'
        
        def clean_arg(match):
            prefix = match.group(1)
            arg = match.group(2)
            suffix = match.group(3)
            
            # æ”¹è¿›1ï¼šåˆ é™¤è¿ç»­ç©ºè¡Œ
            arg = re.sub(r'\n\s*\n+', '\n', arg)
            
            # æ”¹è¿›2ï¼šåˆ é™¤æ®µé¦–æ®µå°¾ç©ºè¡Œ
            arg = arg.strip()
            
            # æ”¹è¿›3ï¼šæ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼ˆä¿ç•™ç¼©è¿›ï¼‰
            lines = arg.split('\n')
            lines = [line.rstrip() for line in lines]
            arg = '\n'.join(lines)
            
            return prefix + arg + suffix
        
        text = re.sub(pattern, clean_arg, text, flags=re.DOTALL)
    
    return text


def clean_question_environments(text: str) -> str:
    """æ¸…ç† question ç¯å¢ƒå†…éƒ¨çš„å¤šä½™ç©ºè¡Œï¼Œå¹¶æ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®"""
    pattern = r'(\\begin\{question\})(.*?)(\\end\{question\})'

    def clean_env(match):
        begin = match.group(1)
        content = match.group(2)
        end = match.group(3)

        # åˆ é™¤è¿ç»­çš„3ä¸ªä»¥ä¸Šæ¢è¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)

        # ğŸ†• v1.8: æ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›®ï¼ˆç›´æ¥ä» \item å¼€å§‹ï¼‰
        # å»é™¤å‰å¯¼ç©ºç™½åæ£€æŸ¥æ˜¯å¦ä»¥ \item å¼€å¤´
        content_stripped = content.lstrip()
        if content_stripped.startswith('\\item'):
            # åœ¨ \begin{question} åæ’å…¥ TODO æ³¨é‡Š
            warning = '\n% âš ï¸ TODO: è¡¥å……é¢˜å¹² - æ­¤é¢˜ç›´æ¥ä» \\item å¼€å§‹ï¼Œè¯·åœ¨ä¸Šæ–¹æ·»åŠ é¢˜ç›®ä¸»å¹²æè¿°\n'
            content = warning + content

        return begin + content + end

    return re.sub(pattern, clean_env, text, flags=re.DOTALL)


def split_long_lines_in_explain(text: str, max_length: int = 800) -> str:
    """åœ¨ explain{} ä¸­è‡ªåŠ¨åˆ†å‰²è¶…é•¿è¡Œ"""
    pattern = r'(\\explain\{)([^{}]*(?:\{[^{}]*\}[^{}]*)*?)(\})'
    
    def split_content(match):
        prefix = match.group(1)
        content = match.group(2)
        suffix = match.group(3)
        
        lines = content.split('\n')
        new_lines = []
        
        for line in lines:
            if len(line) <= max_length:
                new_lines.append(line)
            else:
                # åœ¨æ ‡ç‚¹ååˆ†å‰²
                segments = re.split(r'([ï¼Œã€‚ï¼›ï¼ï¼Ÿ])', line)
                current = ""
                for seg in segments:
                    if len(current + seg) > max_length and current:
                        new_lines.append(current.rstrip())
                        current = seg
                    else:
                        current += seg
                if current:
                    new_lines.append(current.rstrip())
        
        return prefix + '\n'.join(new_lines) + suffix
    
    return re.sub(pattern, split_content, text, flags=re.DOTALL)


def remove_par_breaks_in_explain(text: str) -> str:
    r"""ç§»é™¤ \explain{...} ä¸­çš„ç©ºæ®µè½ï¼ˆæ”¹è¿›ç‰ˆï¼šæ­£ç¡®å¤„ç†åµŒå¥—æ‹¬å·ï¼‰

    ğŸ†• v1.8.2ï¼šå®Œå…¨é‡å†™ï¼Œä¿®å¤æ‹¬å·è®¡æ•°é”™è¯¯
    - æ­£ç¡®å¤„ç† \{ \} è½¬ä¹‰æ‹¬å·ï¼ˆä¸è®¡å…¥ depthï¼‰
    - æ­£ç¡®å¤„ç†åæ–œæ è½¬ä¹‰ï¼ˆ\\ åçš„å­—ç¬¦ä¸å¤„ç†ï¼‰
    - å°†ç©ºæ®µè½æ›¿æ¢ä¸º % æ³¨é‡Šè€Œé \parï¼ˆæ›´å®‰å…¨ï¼‰
    """
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    out = []
    i = 0
    n = len(text)

    while i < n:
        if text.startswith("\\explain{", i):
            out.append("\\explain{")
            i += len("\\explain{")
            depth = 1
            buf = []

            while i < n and depth > 0:
                # å¤„ç†åæ–œæ è½¬ä¹‰åºåˆ—
                if text[i] == '\\' and i + 1 < n:
                    next_char = text[i + 1]
                    # \{ å’Œ \} ä¸è®¡å…¥æ‹¬å·æ·±åº¦
                    if next_char in '{}':
                        buf.append(text[i:i+2])
                        i += 2
                        continue
                    # å…¶ä»–åæ–œæ åºåˆ—ï¼ˆå¦‚ \\, \left, \right ç­‰ï¼‰ç›´æ¥å¤åˆ¶
                    buf.append(text[i])
                    i += 1
                    continue

                # æ£€æµ‹ç©ºæ®µè½ï¼ˆè¿ç»­ä¸¤ä¸ªæ¢è¡Œï¼Œä¸­é—´åªæœ‰ç©ºç™½ï¼‰
                if text[i] == '\n':
                    j = i + 1
                    while j < n and text[j] in ' \t':
                        j += 1
                    if j < n and text[j] == '\n':
                        # ç©ºæ®µè½ï¼šæ›¿æ¢ä¸ºæ³¨é‡Šè¡Œ
                        buf.append('\n%\n')
                        i = j + 1
                        continue

                # æ™®é€šå¤§æ‹¬å·è®¡æ•°
                if text[i] == '{':
                    depth += 1
                    buf.append(text[i])
                    i += 1
                elif text[i] == '}':
                    depth -= 1
                    if depth == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„é—­æ‹¬å·
                        out.append(''.join(buf))
                        out.append('}')
                        i += 1
                        break
                    buf.append(text[i])
                    i += 1
                else:
                    buf.append(text[i])
                    i += 1

            # å¦‚æœå¾ªç¯ç»“æŸä½† depth > 0ï¼Œè¯´æ˜æ‹¬å·ä¸åŒ¹é…ï¼ˆä¿ç•™åŸå†…å®¹ï¼‰
            if depth > 0:
                out.append(''.join(buf))
        else:
            out.append(text[i])
            i += 1

    return ''.join(out)


def cleanup_remaining_image_markers(text: str) -> str:
    """ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    
    ğŸ†• v1.6.2ï¼šå¢å¼ºå†…è”å…¬å¼å¤„ç†
    - ç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡ â†’ TikZå ä½ç¬¦å—ï¼ˆå¤§å›¾ï¼‰
    - å†…è”å›¾ç‰‡ï¼ˆå…¬å¼ï¼‰â†’ ç®€å•æ–‡æœ¬å ä½ç¬¦ [å…¬å¼:filename]
    
    å°†æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…åœ¨ PDF ä¸­æ˜¾ç¤º
    ä¸ºåŸå§‹è·¯å¾„æ–‡æœ¬ã€‚æ”¯æŒï¼š
      - ![@@@id](path){...}
      - ![](path){...}
    """
    if not text:
        return text
    
    def _make_tikz_placeholder(label: str) -> str:
        """åˆ›å»º TikZ å ä½ç¬¦å—ï¼ˆç”¨äºç‹¬ç«‹å›¾ç‰‡ï¼‰"""
        label = label.strip() or "image"
        return (
            "\n\\begin{center}\n"
            "\\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]\n"
            f"  \\node[draw, minimum width=6cm, minimum height=4cm] {{å›¾ç•¥ï¼ˆå›¾ ID: {label}ï¼‰}};\n"
            "\\end{tikzpicture}\n"
            "\\end{center}\n"
        )
    
    def _make_inline_placeholder(label: str) -> str:
        """åˆ›å»ºå†…è”å ä½ç¬¦ï¼ˆç”¨äºå…¬å¼å›¾ç‰‡ï¼‰"""
        label = label.strip() or "formula"
        # ä½¿ç”¨ç®€å•çš„æ–‡æœ¬å ä½ç¬¦ï¼Œå¯ä»¥åœ¨åç»­è¢«è¯†åˆ«å’Œæ›¿æ¢
        return f"[å…¬å¼:{label}]"
    
    def is_standalone_line(match_obj: re.Match, full_text: str) -> bool:
        """åˆ¤æ–­åŒ¹é…æ˜¯å¦ä¸ºç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡"""
        # è·å–åŒ¹é…å‰åçš„æ–‡æœ¬
        start = match_obj.start()
        end = match_obj.end()
        
        # å‘å‰æŸ¥æ‰¾åˆ°è¡Œé¦–
        line_start = start
        while line_start > 0 and full_text[line_start - 1] not in '\n':
            line_start -= 1
        
        # å‘åæŸ¥æ‰¾åˆ°è¡Œå°¾
        line_end = end
        while line_end < len(full_text) and full_text[line_end] not in '\n':
            line_end += 1
        
        # æ£€æŸ¥è¡Œå†…å®¹ï¼šå»é™¤ç©ºç™½åæ˜¯å¦åªæœ‰è¿™ä¸ªå›¾ç‰‡æ ‡è®°
        line_content = full_text[line_start:line_end].strip()
        match_content = match_obj.group(0).strip()
        
        return line_content == match_content
    
    # å¤„ç†å¸¦IDçš„å›¾ç‰‡æ ‡è®°
    def repl_with_id(m: re.Match) -> str:
        img_id = m.group(1)
        basename = os.path.basename(img_id) if img_id else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(basename)
        else:
            return _make_inline_placeholder(basename)
    
    import os  # ç¡®ä¿å¯¼å…¥
    text = IMAGE_PATTERN_WITH_ID.sub(repl_with_id, text)
    
    # å¤„ç†æ— IDçš„å›¾ç‰‡æ ‡è®°
    def repl_no_id(m: re.Match) -> str:
        path = m.group(1).strip()
        basename = os.path.basename(path)
        label = basename if basename else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(label)
        else:
            return _make_inline_placeholder(label)
    
    text = IMAGE_PATTERN_NO_ID.sub(repl_no_id, text)
    
    return text


def cleanup_guxuan_in_macros(text: str) -> str:
    """ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™
    
    é’ˆå¯¹ \\topics{...} å’Œ \\explain{...} ç­‰å®å‚æ•°å†…çš„"æ•…é€‰ï¼šX"è¿›è¡Œæ¸…ç†ã€‚
    
    Args:
        text: LaTeX æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text or 'æ•…é€‰' not in text:
        return text
    
    # å®šä¹‰è¦æ¸…ç†çš„å®åˆ—è¡¨
    macros = ['topics', 'explain', 'keywords', 'analysis']
    
    for macro_name in macros:
        # åŒ¹é… \macro{content}ï¼Œä½¿ç”¨é€’å½’åŒ¹é…åµŒå¥—å¤§æ‹¬å·
        # ç”±äºPython reä¸æ”¯æŒé€’å½’ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…+æ‰‹å·¥è§£æ
        pattern = rf'\\{macro_name}\{{'
        
        pos = 0
        result_parts = []
        
        while True:
            start_idx = text.find(pattern, pos)
            if start_idx == -1:
                result_parts.append(text[pos:])
                break
            
            # æ·»åŠ å‰é¢çš„æ–‡æœ¬
            result_parts.append(text[pos:start_idx])
            
            # æ‰‹å·¥è§£æåµŒå¥—å¤§æ‹¬å·
            brace_count = 0
            content_start = start_idx + len(pattern)
            i = content_start
            
            while i < len(text):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    if brace_count == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·
                        content = text[content_start:i]
                        
                        # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰"
                        # 1. æ¸…ç†è¡Œæœ«çš„"æ•…é€‰ï¼šX"ï¼ˆå«å„ç§æ ‡ç‚¹ç»„åˆå’Œå¯èƒ½çš„åç»­æ–‡æœ¬ï¼‰
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 2. æ¸…ç†å•ç‹¬ä¸€è¡Œçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 3. æ¸…ç†æ¢è¡Œç¬¦åçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*(?=\n|$)', '', content)
                        # 4. æ¸…ç†ä»»æ„ä½ç½®çš„"æ•…é€‰ï¼šX"ï¼ˆæ›´æ¿€è¿›ï¼‰
                        content = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*', '', content)
                        # 5. æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*[.ã€‚]?\s*', '', content)
                        
                        result_parts.append(rf'\{macro_name}{{{content}}}')
                        pos = i + 1
                        break
                    else:
                        brace_count -= 1
                elif text[i] == '\\' and i + 1 < len(text):
                    # è·³è¿‡è½¬ä¹‰å­—ç¬¦
                    i += 1
                i += 1
            else:
                # æ²¡æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·ï¼Œä¿ç•™åŸæ–‡
                result_parts.append(text[start_idx:])
                break
        
        text = ''.join(result_parts)
    
    return text


def convert_markdown_table_to_latex(text: str) -> str:
    """å°† Markdown è¡¨æ ¼è½¬æ¢ä¸º LaTeX tabular"""
    table_pattern = r'(\|[^\n]+\|\n)+\|[-:\s|]+\|\n(\|[^\n]+\|\n)+'
    
    def convert_one_table(match):
        table_text = match.group(0)
        lines = [line.strip() for line in table_text.split('\n') if line.strip()]
        
        data_lines = [line for line in lines if not re.match(r'^\|[-:\s|]+\|$', line)]
        
        if not data_lines:
            return table_text
        
        rows = []
        for line in data_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            rows.append(cells)
        
        if not rows:
            return table_text
        
        ncols = len(rows[0])
        latex = "\\begin{center}\n"
        latex += f"\\begin{{tabular}}{{{'c' * ncols}}}\n"
        latex += "\\hline\n"
        
        header = rows[0]
        latex += " & ".join(escape_latex_special(cell, False) for cell in header)
        latex += " \\\\\n\\hline\n"
        
        for row in rows[1:]:
            latex += " & ".join(escape_latex_special(cell, False) for cell in row)
            latex += " \\\\\n"
        
        latex += "\\hline\n\\end{tabular}\n\\end{center}"
        return latex
    
    return re.sub(table_pattern, convert_one_table, text)


def normalize_fullwidth_brackets(text: str) -> str:
    """ğŸ†• v1.6.3ï¼šç»Ÿä¸€å…¨è§’æ‹¬å·ä¸ºåŠè§’

    æ³¨æ„ï¼šä¸è¦æ›¿æ¢ç”¨äº meta æ ‡è®°çš„ã€ã€‘
    """
    pairs = {
        "ï¼ˆ": "(",
        "ï¼‰": ")",
        "ï½›": "{",
        "ï½": "}",
        # ä¸æ›¿æ¢ ï¼»ï¼½ï¼Œé¿å…å½±å“æŸäº› Markdown è¯­æ³•
    }
    for fw, hw in pairs.items():
        text = text.replace(fw, hw)
    return text


def clean_markdown(text: str) -> str:
    """æ¸…ç† markdown åƒåœ¾

    ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    ğŸ†• v1.6.3ï¼šå¢å¼ºå…¨è§’æ‹¬å·ç»Ÿä¸€
    """
    # ğŸ†• v1.6.3ï¼šé¦–å…ˆç»Ÿä¸€å…¨è§’æ‹¬å·
    text = normalize_fullwidth_brackets(text)

    text = re.sub(
        r"<br><span class='markdown-page-line'>.*?</span><br><br>",
        "\n", text, flags=re.S,
    )
    text = re.sub(
        r"<span id='page\d+' class='markdown-page-text'>\[.*?\]</span>",
        "", text,
    )

    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text)

    # ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    # ä¿æŠ¤å·²æœ‰çš„LaTeXå‘½ä»¤
    protected = []
    def save_latex_cmd(match):
        protected.append(match.group(0))
        return f"@@LATEXCMD{len(protected)-1}@@"
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', save_latex_cmd, text)

    # ç»Ÿä¸€æ‹¬å·ï¼ˆå…¨è§’â†’åŠè§’ï¼‰- å·²åœ¨ normalize_fullwidth_brackets ä¸­å¤„ç†
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    # ç»Ÿä¸€å¼•å·ï¼ˆå¼¯å¼•å·â†’ç›´å¼•å·ï¼‰
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")

    # æ¢å¤LaTeXå‘½ä»¤
    for i, cmd in enumerate(protected):
        text = text.replace(f"@@LATEXCMD{i}@@", cmd)

    # æ¸…ç†ä»£ç å—æ ‡è®°
    text = re.sub(r'```[a-z]*\n?', '', text)
    text = re.sub(r'```', '', text)

    # è½¬æ¢è¡¨æ ¼
    if '|' in text and '---' in text:
        text = convert_markdown_table_to_latex(text)

    # å¤„ç†ä¸‹åˆ’çº¿
    text = text.replace(r'\_', '@@ESCAPED_UNDERSCORE@@')
    text = re.sub(r'(?<!\\)_(?![{_])', r'\\_', text)
    text = text.replace('@@ESCAPED_UNDERSCORE@@', r'\_')

    return text.strip()


# ==================== é¢˜ç›®è§£æå‡½æ•° ====================

def split_sections(text: str) -> List[Tuple[str, str]]:
    """æ‹†åˆ†ç« èŠ‚ï¼ˆæ”¯æŒ markdown æ ‡é¢˜å’ŒåŠ ç²—æ ¼å¼ï¼‰
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. Markdown æ ‡é¢˜ï¼š# ä¸€ã€å•é€‰é¢˜
    2. åŠ ç²—æ ¼å¼ï¼š**ä¸€ã€å•é€‰é¢˜**
    """
    lines = text.splitlines()
    sections = []
    current_title = None
    current_lines = []

    for line in lines:
        stripped = line.strip()
        # ä¼˜å…ˆåŒ¹é… markdown æ ‡é¢˜æ ¼å¼
        m = re.match(
            r"^#+\s*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)",
            stripped,
        )
        # å¦‚æœä¸åŒ¹é…ï¼Œå°è¯•åŒ¹é…åŠ ç²—æ ¼å¼ **ç« èŠ‚æ ‡é¢˜**
        if not m:
            m = re.match(
                r"^\*\*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)\*\*",
                stripped,
            )
        
        if m:
            if current_title is not None:
                sections.append((current_title, "\n".join(current_lines).strip()))
                current_lines = []
            current_title = m.group(1)
        else:
            if current_title is not None:
                current_lines.append(line)

    if current_title is not None and current_lines:
        sections.append((current_title, "\n".join(current_lines).strip()))

    return sections


def split_questions(section_body: str) -> List[str]:
    """æ‹†åˆ†é¢˜ç›®"""
    lines = section_body.splitlines()
    blocks = []
    current = []

    def flush():
        if current:
            blocks.append("\n".join(current).strip())
            current.clear()

    for line in lines:
        stripped = line.strip()
        if re.match(r"^\d+[\.ï¼ã€]\s*", stripped):
            flush()
            current.append(line)
        else:
            current.append(line)

    flush()
    return blocks


def extract_context_around_image(text: str, img_match_start: int, img_match_end: int,
                                  context_len: int = 50) -> Tuple[str, str]:
    """æå–å›¾ç‰‡å‰åçš„ä¸Šä¸‹æ–‡æ–‡æœ¬

    Args:
        text: å®Œæ•´æ–‡æœ¬
        img_match_start: å›¾ç‰‡åŒ¹é…çš„èµ·å§‹ä½ç½®
        img_match_end: å›¾ç‰‡åŒ¹é…çš„ç»“æŸä½ç½®
        context_len: ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰

    Returns:
        (context_before, context_after) å…ƒç»„
    """
    # æå–å‰æ–‡
    before_start = max(0, img_match_start - context_len)
    context_before = text[before_start:img_match_start].strip()
    # æ¸…ç†æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    context_before = ' '.join(context_before.split())

    # æå–åæ–‡
    after_end = min(len(text), img_match_end + context_len)
    context_after = text[img_match_end:after_end].strip()
    context_after = ' '.join(context_after.split())

    return context_before, context_after


def extract_meta_and_images(block: str, question_index: int = 0, slug: str = "") -> Tuple[str, Dict, List]:
    r"""æå–å…ƒä¿¡æ¯ä¸å›¾ç‰‡ï¼ˆçŠ¶æ€æœºé‡æ„ï¼šé˜²æ­¢è·¨é¢˜ç´¯ç§¯ï¼‰

    ğŸ†• æ–°å¢å‚æ•°ï¼šquestion_index å’Œ slug ç”¨äºç”Ÿæˆå›¾ç‰‡ ID

    ç›®æ ‡ï¼šé¿å…ä¸Šä¸€é¢˜çš„å¤šè¡Œã€è¯¦è§£ã€‘/ã€åˆ†æã€‘é”™è¯¯åå¹¶ä¸‹ä¸€é¢˜é¢˜å¹²ã€‚
    å…³é”®è¾¹ç•Œï¼š
      - æ–°çš„ meta å¼€å§‹ï¼ˆç­”æ¡ˆ/éš¾åº¦/çŸ¥è¯†ç‚¹/è¯¦è§£/åˆ†æï¼‰
      - é¢˜å·å¼€å§‹ï¼š^\s*>?\s*(?:\d+[\.ï¼ã€]\s+|ï¼ˆ\d+ï¼‰\s+|\d+\)\s+)
      - ç« èŠ‚æ ‡é¢˜ï¼š^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$
      - ç©ºè¡Œ + lookahead ä¸ºé¢˜å·æ—¶ï¼Œä½œä¸ºå®‰å…¨è¾¹ç•Œï¼ˆè‹¥ä¸Šä¸€è¡Œåƒç¯å¢ƒç»­è¡Œåˆ™è·³è¿‡è¯¥ç©ºè¡Œè¾¹ç•Œï¼‰
      - å¼•è¿°ç©ºè¡Œ ^>\s*$ å¿½ç•¥
    """
    # è§„èŒƒåŒ–å¹¶åˆ‡åˆ†è¡Œ
    lines = block.splitlines()

    # ç»“æœå®¹å™¨
    meta = {k: "" for k in META_PATTERNS}
    # ğŸ†• ä¿®å¤ï¼šanalysis å•ç‹¬å­˜åœ¨ï¼Œåç»­ä¼šè¢«ä¸¢å¼ƒ
    meta_alias_map = {
        "analysis": "analysis",  # analysis å•ç‹¬å­˜åœ¨ï¼Œåé¢ä¼šè¢«ä¸¢å¼ƒ
        "explain": "explain",
        "answer": "answer",
        "difficulty": "difficulty",
        "topics": "topics",
    }

    content_lines: List[str] = []
    images: List[Dict] = []

    # ç¼–è¯‘è¾¹ç•Œæ­£åˆ™ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šé¢˜å·æ ¼å¼å’Œç« èŠ‚æ ‡é¢˜ï¼‰
    question_start_perm = re.compile(r"^\s*>?\s*(?:\d{1,3}[\.ï¼ã€]\s+|ï¼ˆ\d{1,3}ï¼‰\s+|\d{1,3}\)\s+)")
    section_header = re.compile(r"^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$")
    quote_blank = re.compile(r"^>\s*$")
    env_cont_hint = re.compile(r"(\\\\\s*$)|\\begin\{|\\left|\\right")

    # ğŸ†• ä¿®å¤ï¼šå°† META_PATTERNS ç¼–è¯‘ï¼Œåˆ†ç¦» analysis å’Œ explain
    meta_starts = [
        ("answer", re.compile(r"^ã€\s*ç­”æ¡ˆ\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("difficulty", re.compile(r"^ã€\s*éš¾åº¦\s*ã€‘[:ï¼š]?\s*([\d.]+).*")),
        ("topics", re.compile(r"^ã€\s*(çŸ¥è¯†ç‚¹|è€ƒç‚¹)\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("analysis", re.compile(r"^ã€\s*åˆ†æ\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("explain", re.compile(r"^ã€\s*è¯¦è§£\s*ã€‘[:ï¼š]?\s*(.*)$")),
    ]

    # çŠ¶æ€
    state = "NORMAL"  # or "IN_META"
    current_meta_key: Optional[str] = None
    current_meta_lines: List[str] = []

    def flush_meta():
        nonlocal current_meta_key, current_meta_lines
        if current_meta_key is None:
            return
        # å½’ä¸€åŒ–åˆ°åˆ«åé”®
        key = meta_alias_map.get(current_meta_key, current_meta_key)
        # ğŸ†• ä¿®å¤ï¼šé‡åˆ° analysis æ—¶ç›´æ¥ä¸¢å¼ƒ
        if key == "analysis":
            # è¯´æ˜è¿™æ˜¯ã€åˆ†æã€‘æ®µï¼Œç›´æ¥èˆå¼ƒï¼Œä¸å†™å…¥ meta å­—å…¸
            current_meta_key = None
            current_meta_lines = []
            return
        # åˆå¹¶æ¸…ç†
        text = "\n".join(current_meta_lines)
        # å»æ‰å¯èƒ½æ®‹ç•™çš„æ ‡ç­¾å‰ç¼€
        text = re.sub(r"^ã€?(?:ç­”æ¡ˆ|éš¾åº¦|çŸ¥è¯†ç‚¹|è€ƒç‚¹|è¯¦è§£|åˆ†æ)ã€‘?[:ï¼š]?\s*", "", text)
        # å¯¹äº explain å­—æ®µï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼ˆä¸æŠ˜å ç©ºè¡Œï¼‰ï¼Œè®©åç»­ remove_par_breaks_in_explain å¤„ç†
        # å…¶ä»–å­—æ®µå‹ç¼©ç©ºè¡Œ
        if key != "explain":
            text = re.sub(r"\n\s*\n+", "\n", text)
        # åˆå¹¶ï¼šè‹¥å·²æœ‰ explainï¼Œåˆ™è¿½åŠ ä¸€è¡Œ
        if key == "explain" and meta.get("explain"):
            meta["explain"] = (meta["explain"] + "\n" + text.strip()).strip()
        else:
            meta[key] = text.strip()
        # é‡ç½®
        current_meta_key = None
        current_meta_lines = []

    def is_question_start(s: str) -> bool:
        return bool(question_start_perm.match(s))

    def is_section_header(s: str) -> bool:
        return bool(section_header.match(s))

    def image_match(s: str):
        # ä¼˜å…ˆåŒ¹é…å¸¦IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_WITH_ID.search(s)
        if m:
            return ('with_id', m)
        # ç„¶ååŒ¹é…æ— IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_NO_ID.search(s)
        if m:
            return ('no_id', m)
        # æœ€åå°è¯•æ—§ç‰ˆç®€å•æ ¼å¼
        m = IMAGE_PATTERN.search(s)
        if m:
            return ('simple', m)
        return None

    # æŸ¥æ‰¾ä¸Šä¸€æ¡éç©ºè¡Œï¼ˆç”¨äºç¯å¢ƒç»­è¡Œåˆ¤æ–­ï¼‰
    def find_prev_nonempty(idx: int) -> Optional[str]:
        j = idx - 1
        while j >= 0:
            if lines[j].strip():
                return lines[j]
            j -= 1
        return None

    # æŸ¥æ‰¾ä¸‹ä¸€æ¡éç©ºè¡Œï¼ˆç”¨äº blank+lookahead åˆ¤æ–­ï¼‰
    def find_next_nonempty(idx: int) -> Optional[str]:
        j = idx + 1
        while j < len(lines):
            if lines[j].strip():
                return lines[j]
            j += 1
        return None

    i = 0
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # ğŸ†• v1.6.2ï¼šå›¾ç‰‡è¡Œè¯†åˆ«å¢å¼º - åŒºåˆ†ç‹¬ç«‹å›¾ç‰‡å— vs å†…è”å…¬å¼å›¾ç‰‡
        # åªæœ‰å½“å›¾ç‰‡"ç‹¬å ä¸€è¡Œ"ä¸”æ˜¯å®Œæ•´åŒ¹é…æ—¶ï¼Œæ‰æå–ä¸ºå›¾ç‰‡å—
        # å†…è”å›¾ç‰‡ï¼ˆå¦‚ "å·²çŸ¥é›†åˆ![](image2.wmf)ï¼Œåˆ™..."ï¼‰ä¿ç•™åœ¨æ–‡æœ¬ä¸­
        # ğŸ†• Prompt 3: ç»Ÿä¸€å¤„ç†æ‰€æœ‰å›¾ç‰‡ï¼ˆç‹¬ç«‹å’Œå†…è”ï¼‰
        # æ£€æŸ¥æ•´è¡Œæ˜¯å¦åŒ…å«å›¾ç‰‡æ ‡è®°
        img_result = image_match(line)  # æ³¨æ„ï¼šä½¿ç”¨å®Œæ•´è¡Œè€Œéstripped
        if img_result:
            img_type, m_img = img_result
            # æ£€æŸ¥æ˜¯å¦ä¸ºç‹¬ç«‹å›¾ç‰‡è¡Œï¼šæ•´è¡Œåªæœ‰ä¸€ä¸ªå›¾ç‰‡æ ‡è®°
            is_standalone = (m_img.group(0).strip() == stripped)

            # ğŸ†• ç”Ÿæˆå›¾ç‰‡ ID å’Œæå–ä¸Šä¸‹æ–‡
            img_counter = len(images) + 1
            generated_id = f"{slug}-Q{question_index}-img{img_counter}" if slug else f"Q{question_index}-img{img_counter}"

            # æå–ä¸Šä¸‹æ–‡
            full_text = "\n".join(lines)
            img_start = full_text.find(m_img.group(0))
            img_end = img_start + len(m_img.group(0))
            context_before, context_after = extract_context_around_image(full_text, img_start, img_end)

            if is_standalone:
                # ç‹¬ç«‹å›¾ç‰‡å—ï¼šæå–åˆ°imagesåˆ—è¡¨
                if img_type == 'with_id':
                    # ![@@@id](path){...}
                    img_id = m_img.group(1)
                    path = m_img.group(2).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                elif img_type == 'no_id':
                    # ![](path){...}
                    path = m_img.group(1).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                else:
                    # ç®€å•æ ¼å¼: ![](images/...)
                    path = m_img.group(1)
                    width = int(m_img.group(2)) if m_img.group(2) else 60
                    images.append({
                        "path": path,
                        "width": width,
                        "id": generated_id,
                        "inline": False,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                i += 1
                continue
            else:
                # å†…è”å›¾ç‰‡ï¼šæ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œè®°å½•åˆ°imagesåˆ—è¡¨
                if img_type == 'with_id':
                    img_id = m_img.group(1)
                    path = m_img.group(2).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                elif img_type == 'no_id':
                    path = m_img.group(1).strip()
                    images.append({
                        "path": path,
                        "width": 60,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })
                else:
                    path = m_img.group(1)
                    width = int(m_img.group(2)) if m_img.group(2) else 60
                    images.append({
                        "path": path,
                        "width": width,
                        "id": generated_id,
                        "inline": True,
                        "question_index": question_index,
                        "sub_index": 1,
                        "context_before": context_before,
                        "context_after": context_after
                    })

                # æ›¿æ¢å›¾ç‰‡æ ‡è®°ä¸ºå ä½ç¬¦ï¼ˆä½¿ç”¨æ–°çš„ ID æ ¼å¼ï¼‰
                line = line.replace(m_img.group(0), f"<<IMAGE_INLINE:{generated_id}>>")
                # ç»§ç»­å¤„ç†è¯¥è¡Œï¼ˆfallthroughï¼‰

        # å¼•è¿°ç©ºè¡Œï¼šä¸¢å¼ƒ
        if quote_blank.match(stripped):
            i += 1
            continue

        if state == "NORMAL":
            # æ–°çš„ meta å¼€å§‹ï¼Ÿ
            started = False
            for key, pat in meta_starts:
                m = pat.match(stripped)
                if m:
                    state = "IN_META"
                    current_meta_key = key
                    seed = m.group(m.lastindex or 1) if m.groups() else ""
                    current_meta_lines = [seed.strip()] if seed.strip() else []
                    started = True
                    break
            if started:
                i += 1
                continue

            # æ™®é€šå†…å®¹
            content_lines.append(line)
            i += 1
            continue

        # state == IN_META
        # 1) æ–° meta å¼€å§‹ -> åˆ·æ–°å¹¶åˆ‡æ¢
        started = False
        for key, pat in meta_starts:
            m = pat.match(stripped)
            if m:
                flush_meta()
                state = "IN_META"
                current_meta_key = key
                seed = m.group(m.lastindex or 1) if m.groups() else ""
                current_meta_lines = [seed.strip()] if seed.strip() else []
                started = True
                break
        if started:
            i += 1
            continue

        # 2) ç¡®è®¤é¢˜å·æˆ–ç« èŠ‚è¾¹ç•Œ -> ç»“æŸ metaï¼Œä¿ç•™è¯¥è¡Œç»™é¢˜å¹²
        if is_question_start(stripped) or is_section_header(stripped):
            flush_meta()
            state = "NORMAL"
            content_lines.append(line)
            i += 1
            continue

        # 3) ç©ºè¡Œ + lookahead ä¸ºé¢˜å· -> å®‰å…¨åœ°ç»“æŸ meta
        if stripped == "":
            next_ne = find_next_nonempty(i)
            if next_ne and is_question_start(next_ne.strip()):
                prev_ne = find_prev_nonempty(i)
                # è‹¥ä¸Šä¸€éç©ºè¡Œçœ‹èµ·æ¥æ˜¯ç¯å¢ƒç»­è¡Œï¼Œåˆ™ä¸è¦åœ¨æ­¤ç©ºè¡Œåˆ‡æ–­
                if prev_ne and env_cont_hint.search(prev_ne):
                    # ç»§ç»­æŠŠç©ºè¡Œä¹Ÿå¹¶å…¥ metaï¼ˆä¿æŒåŸæ ·ï¼‰
                    current_meta_lines.append(line)
                    i += 1
                    continue
                # å¦åˆ™åˆ‡æ–­ metaï¼ˆä¸æ¶ˆè´¹ç©ºè¡Œï¼‰
                flush_meta()
                state = "NORMAL"
                i += 1  # è·³è¿‡è¯¥ç©ºè¡Œï¼Œä¸‹ä¸€è½®çœ‹åˆ°é¢˜å·è¡Œä¼šè¿›å…¥ NORMAL æµç¨‹
                continue

        # 4) ç»§ç»­ç´¯ç§¯ meta å†…å®¹
        current_meta_lines.append(line)
        i += 1

    # å¾ªç¯ç»“æŸï¼Œè‹¥è¿˜åœ¨ meta çŠ¶æ€åˆ™åˆ·æ–°
    if state == "IN_META":
        flush_meta()

    content = "\n".join(content_lines).strip()
    return content, meta, images


def parse_question_structure(content: str) -> Dict:
    """æ™ºèƒ½è¯†åˆ«é¢˜ç›®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    è§£æé¢˜å¹²ã€é€‰é¡¹ã€è§£æä¸‰éƒ¨åˆ†ï¼Œé¿å…å°†è§£ææ–‡æœ¬æ··å…¥é€‰é¡¹
    """
    lines = content.splitlines()
    
    structure = {
        'stem_lines': [],
        'choices': [],
        'analysis_lines': [],
        'in_choice': False,
        'in_analysis': False,
        'current_choice': '',
    }
    
    choice_pattern = re.compile(r'^([A-D])[\.ï¼ã€]\s*(.*)$')
    
    for line in lines:
        stripped = line.strip()

        # ğŸ†• ä¿®å¤ï¼šåªåœ¨é‡åˆ°ã€è¯¦è§£ã€‘æ—¶è¿›å…¥è§£ææ¨¡å¼ï¼Œé‡åˆ°ã€åˆ†æã€‘æ—¶è·³è¿‡
        # æ£€æŸ¥æ˜¯å¦ä¸ºã€åˆ†æã€‘æ ‡è®° - ç›´æ¥è·³è¿‡
        if re.match(r'^ã€?\s*åˆ†æ\s*ã€‘[:ï¼š]?', stripped):
            structure['in_choice'] = False
            structure['in_analysis'] = False
            # ä¸æŠŠè¿™ä¸€è¡Œå¡è¿›ä»»ä½•åœ°æ–¹ï¼Œå®Œå…¨èˆå¼ƒ
            continue

        # æ£€æŸ¥æ˜¯å¦ä¸ºã€è¯¦è§£ã€‘æ ‡è®° - è¿›å…¥è§£ææ¨¡å¼
        if re.match(r'^ã€?\s*è¯¦è§£\s*ã€‘[:ï¼š]?', stripped):
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
                structure['current_choice'] = ''
            structure['in_choice'] = False
            structure['in_analysis'] = True
            structure['analysis_lines'].append(stripped)
            continue

        # ä¿å®ˆå¤„ç†ï¼šåªåœ¨æ˜ç¡®çš„è§£æèµ·å§‹è¯å¼€å¤´æ—¶è¿›å…¥è§£æï¼ˆé¿å…è¯¯åˆ¤é¢˜å¹²ï¼‰
        # æ³¨æ„ï¼šä¸å†ä½¿ç”¨ ANALYSIS_START_MARKERS è‡ªåŠ¨è§¦å‘ï¼Œé¿å…"åˆ™"ç­‰è¯åœ¨é¢˜å¹²ä¸­è¯¯åˆ¤
        if structure['in_analysis']:
            # å·²ç»åœ¨è§£ææ¨¡å¼ä¸­ï¼Œç»§ç»­æ”¶é›†
            pass
        
        # åŒ¹é…é€‰é¡¹æ ‡è®° (A. B. C. D.)
        m = choice_pattern.match(stripped)
        if m:
            # ä¿å­˜ä¸Šä¸€ä¸ªé€‰é¡¹
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
            
            structure['current_choice'] = m.group(2)
            structure['in_choice'] = True
            structure['in_analysis'] = False
            continue
        
        # æ ¹æ®å½“å‰çŠ¶æ€åˆ†é…è¡Œ
        if structure['in_analysis']:
            structure['analysis_lines'].append(line)
        elif structure['in_choice']:
            # é€‰é¡¹ç»­è¡Œï¼ˆå¤šè¡Œé€‰é¡¹å†…å®¹ï¼‰
            structure['current_choice'] += ' ' + stripped
        else:
            # é¢˜å¹²éƒ¨åˆ†
            structure['stem_lines'].append(line)
    
    # ä¿å­˜æœ«å°¾ç´¯ç§¯çš„é€‰é¡¹
    if structure['current_choice']:
        structure['choices'].append(structure['current_choice'].strip())
    
    return structure


def expand_inline_choices(content: str) -> str:
    """å±•å¼€å•è¡Œ/å¤šè¡Œå¼•è¿°é€‰é¡¹å¹¶å»é™¤'>'å‰ç¼€
    - å•è¡Œï¼š> A... B... C... D... â†’ å¤šè¡Œç‹¬ç«‹é€‰é¡¹
    - å¤šè¡Œï¼š> A... B... / > C... D... â†’ åˆå¹¶åå±•å¼€ä¸ºç‹¬ç«‹é€‰é¡¹
    - ç©ºè¡Œï¼š> (ç©º) â†’ è·³è¿‡
    """
    lines = []
    accumulated_choice_text = ""
    
    for line in content.splitlines():
        stripped = line.strip()
        
        # å¤„ç†ä»¥'>'å¼€å¤´çš„è¡Œï¼ˆå¼•è¿°å—ï¼‰
        if stripped.startswith('>'):
            choice_text = stripped[1:].strip()
            
            # è·³è¿‡ç©ºçš„å¼•è¿°è¡Œ
            if not choice_text:
                continue
            
            # å¦‚æœè¿™ä¸€è¡Œæœ‰é€‰é¡¹æ ‡è®°ï¼Œç´¯ç§¯åˆ°ç¼“å†²åŒº
            if re.search(r'[A-D][ï¼\.\ã€]', choice_text):
                accumulated_choice_text += " " + choice_text if accumulated_choice_text else choice_text
                continue
            
            # éé€‰é¡¹å¼•è¿°ï¼ˆå¦‚å›¾ç‰‡è¯´æ˜ç­‰ï¼‰ï¼Œä¿ç•™åŸæ ·
            lines.append(line)
        else:
            # éå¼•è¿°è¡Œï¼šå¦‚æœæœ‰ç´¯ç§¯çš„é€‰é¡¹æ–‡æœ¬ï¼Œå…ˆå¤„ç†
            if accumulated_choice_text:
                # æ£€æŸ¥ç´¯ç§¯æ–‡æœ¬ä¸­æœ‰å¤šå°‘ä¸ªé€‰é¡¹æ ‡è®°
                choice_markers = re.findall(r'[A-D][ï¼\.\ã€]', accumulated_choice_text)
                if len(choice_markers) >= 2:
                    # åˆ†å‰²ä¸ºç‹¬ç«‹é€‰é¡¹
                    parts = re.split(r'(?=[A-D][ï¼\.\ã€])', accumulated_choice_text)
                    for part in parts:
                        part = part.strip()
                        if part and re.match(r'^[A-D][ï¼\.\ã€]', part):
                            lines.append(part)
                elif len(choice_markers) == 1:
                    # å•ä¸ªé€‰é¡¹ï¼Œç›´æ¥æ·»åŠ 
                    lines.append(accumulated_choice_text.strip())
                
                accumulated_choice_text = ""
            
            # æ·»åŠ å½“å‰è¡Œ
            lines.append(line)
    
    # å¤„ç†æœ«å°¾æ®‹ç•™çš„ç´¯ç§¯æ–‡æœ¬
    if accumulated_choice_text:
        choice_markers = re.findall(r'[A-D][ï¼\.\ã€]', accumulated_choice_text)
        if len(choice_markers) >= 2:
            parts = re.split(r'(?=[A-D][ï¼\.\ã€])', accumulated_choice_text)
            for part in parts:
                part = part.strip()
                if part and re.match(r'^[A-D][ï¼\.\ã€]', part):
                    lines.append(part)
        elif len(choice_markers) == 1:
            lines.append(accumulated_choice_text.strip())
    
    return '\n'.join(lines)


def convert_choices(content: str) -> Tuple[str, List[str], str]:
    """æ‹†åˆ†é¢˜å¹²ã€é€‰é¡¹ã€è§£æï¼ˆå¢å¼ºç‰ˆï¼‰
    
    ğŸ†• v1.4 æ”¹è¿›ï¼šå…ˆå±•å¼€å•è¡Œé€‰é¡¹å†è§£æ
    """
    # ğŸ†• å…ˆå±•å¼€å¯èƒ½çš„å•è¡Œé€‰é¡¹
    content = expand_inline_choices(content)
    
    structure = parse_question_structure(content)
    
    stem = '\n'.join(structure['stem_lines']).strip()
    stem = re.sub(r"^\s*\d+[\.ï¼ã€]\s*", "", stem)
    
    # æå–çš„è§£æå†…å®¹
    analysis = '\n'.join(structure['analysis_lines']).strip()
    
    return stem, structure['choices'], analysis


def handle_subquestions(content: str) -> str:
    r"""å¤„ç†è§£ç­”é¢˜çš„å°é¢˜ç¼–å·

    ğŸ†• v1.7ï¼šç»Ÿä¸€å°é—®ç¼–å·æ ¼å¼ï¼Œä¸æ·»åŠ  \mathrm
    """
    if not re.search(r'\(\d+\)', content):
        return content

    subquestions = re.findall(r'\((\d+)\)(.*?)(?=\(\d+\)|$)', content, re.DOTALL)

    if len(subquestions) < 2:
        return content

    result_lines = []
    for num, content_text in subquestions:
        # ğŸ†• v1.7ï¼šä½¿ç”¨æ™®é€šæ–‡æœ¬æ ¼å¼ï¼Œä¸æ·»åŠ  \mathrm
        # æ ¼å¼ï¼š(1) æˆ– (i) ç­‰ï¼Œä¿æŒåŸæ ·
        result_lines.append(f"\\item {content_text.strip()}")

    return '\n'.join(result_lines)


# DEPRECATED: çŠ¶æ€æœºå·²é¿å…è¿™äº›è¡Œå†…å¼‚å¸¸ï¼Œä¿ç•™å…œåº•æµ‹è¯•ä½¿ç”¨
def fix_inline_math_glitches(text: str) -> str:
    """ğŸ†• ä¿®å¤è¡Œå†…æ•°å­¦çš„å„ç§å¼‚å¸¸æ¨¡å¼

    ä¿®å¤ï¼š
    - ç©ºçš„ $$
    - $$$x$ â†’ $x$
    - \therefore$$ â†’ \therefore
    - \because$$ â†’ \because
    """
    if not text:
        return text

    # 1. å»æ‰å®Œå…¨ç©ºçš„ $$
    text = re.sub(r'\$\s*\$', '', text)

    # 2. ä¿®å¤ $$$x$ â†’ $x$
    text = re.sub(r'\$\s*\$\s*(\\\()', r'\1', text)

    # 3. ç‰¹ä¾‹ï¼š\therefore$$ â†’ \therefore
    text = re.sub(r'(\\therefore)\s*\$\s*\$', r'\1', text)

    # 4. ç‰¹ä¾‹ï¼š\because$$ â†’ \because
    text = re.sub(r'(\\because)\s*\$\s*\$', r'\1', text)

    return text


def process_text_for_latex(text: str, is_math_heavy: bool = False) -> str:
    r"""ç»Ÿä¸€å…¥å£ï¼šé¢˜å¹²/é€‰é¡¹/è§£ææ–‡æœ¬çš„ LaTeX å¤„ç†ï¼ˆçŠ¶æ€æœºç‰ˆï¼‰

    é‡æ„ç›®æ ‡ï¼š
    1. ä¿ç•™åŸæœ‰â€œéæ•°å­¦â€æ¸…ç†ä¸è½¬ä¹‰é€»è¾‘ï¼ˆæ•…é€‰/ã€è¯¦è§£ã€‘/OCR è¾¹ç•Œä¿®å¤ç­‰ï¼‰
    2. ç”¨ MathStateMachine å®Œå…¨æ›¿æ¢æ—§çš„ smart_inline_math / sanitize_math ç­‰æ­£åˆ™ç®¡çº¿
    3. æ•°å­¦å®šç•Œç¬¦ç»Ÿä¸€ï¼š$...$ / $$...$$ â†’ \(...\)ï¼Œä¿æŒå·²æœ‰ \(...\) ä¸é‡å¤åŒ…è£¹
    4. åœ¨çŠ¶æ€æœºå¤„ç†ååšè½»é‡å…œåº•æ¸…ç†ï¼ˆç©ºæ•°å­¦å—ã€å›¾ç‰‡å±æ€§æ®‹ç•™ç­‰ï¼‰
    """
    if not text:
        return text

    # ---------- 1. å‰ç½®ï¼šçº¯æ–‡æœ¬/éæ•°å­¦å±‚é¢æ¸…ç†ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰ ----------
    text = re.sub(r'\*\s*(\$[^$]+\$)\s*\*', r'\1', text)  # *$x$* â†’ $x$
    text = re.sub(r'\*([A-Za-z0-9])\*', r'\\emph{\1}', text)  # *x* â†’ \emph{x}

    # "æ•…é€‰" / "æ•…ç­”æ¡ˆä¸º" ç³»åˆ—æ¸…ç†
    text = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    text = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', text)
    text = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', text)
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'[ï¼Œ,]?\s*æ•…é€‰[:ï¼š]\s*[ABCD]+[ã€‚ï¼.]*\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', text)

    # OCR è¾¹ç•Œç•¸å½¢é¢„å¤„ç†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    text = re.sub(r'\\\\right\.\s*\\\\\\\)', r'\\\\right.', text)
    text = re.sub(r'\\\\right\.\\\\\+\)', r'\\\\right.', text)

    # Unicode ç¬¦å·æ›¿æ¢ï¼ˆå…ˆè¡ŒåŒ…è£¹ä¸ºæ•°å­¦ï¼Œåç»­çŠ¶æ€æœºä¼šè§„èŒƒï¼‰
    # âˆµ/âˆ´ ç›´æ¥æ›¿æ¢ä¸ºå‘½ä»¤ï¼ˆä¸å†åŒ…è£¹ç¾å…ƒï¼Œé¿å…ç”Ÿæˆå­¤ç«‹ $ï¼‰
    if 'âˆµ' in text or 'âˆ´' in text:
        text = text.replace('âˆµ', '\\because ').replace('âˆ´', '\\therefore ')

    # éæ•°å­¦æ¨¡å¼ä¸‹çš„ LaTeX ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
    if not is_math_heavy:
        text = escape_latex_special(text, in_math_mode=False)

    # ---------- 2. æ•°å­¦æ¨¡å¼ç»Ÿä¸€ï¼šçŠ¶æ€æœºå¤„ç† ----------
    global math_sm
    text = math_sm.process(text)

    # ---------- 3. è½»é‡åå¤„ç†ï¼šå¸¸è§ç©ºå—/æ®‹ç•™ä¿®å¤ ----------
    text = fix_common_issues_v2(text)

    return text


def fix_common_issues_v2(text: str) -> str:
    r"""çŠ¶æ€æœºåçš„å…œåº•çº¯æ–‡æœ¬ä¿®å¤ï¼ˆåªå¤„ç†ä¸æ”¹å˜æ•°å­¦è¯­ä¹‰çš„æ®‹ç•™ï¼‰

    åŒ…å«ï¼š
    - ç©ºçš„è¡Œå†…/æ˜¾ç¤ºæ•°å­¦å— \(\) / \[\] åˆ é™¤
    - \because\(\) / \therefore\(\) æ¸…ç†ä¸ºçº¯å‘½ä»¤
    - OCR äº§ç”Ÿçš„æ•°ç»„è¾¹ç•Œç•¸å½¢ï¼ˆ\right.\\) â†’ \right.\)ï¼‰
    - å›¾ç‰‡æ®‹ä½™å±æ€§æ¸…ç†ï¼ˆåˆ©ç”¨åŸ clean_residual_image_attrsï¼‰
    - å»é™¤å­¤ç«‹çš„é‡å¤æ˜¾ç¤ºå…¬å¼å®šç•Œç¬¦ï¼ˆçŠ¶æ€æœºå·²è§„èŒƒï¼Œå…œåº•é˜²å¾¡ï¼‰
    """
    if not text:
        return text
    # ç©ºæ•°å­¦å—
    text = re.sub(r'\\\(\s*\\\)', '', text)
    text = re.sub(r'\\\[\s*\\\]', '', text)
    # æ¸…ç† \because\(\) / \therefore\(\)
    text = re.sub(r'\\because\s*\\\(\\\)', r'\\because ', text)
    text = re.sub(r'\\therefore\s*\\\(\\\)', r'\\therefore ', text)
    # æ•°ç»„/åˆ†æ®µç­‰ç¯å¢ƒè¾¹ç•Œç•¸å½¢ï¼ˆä¸ complete ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    text = text.replace(r'\right.\\)', r'\right.\)')
    text = text.replace(r'\right)\\)', r'\right)\)')
    # å›¾ç‰‡å±æ€§æ®‹ç•™ï¼ˆå¤ç”¨å·²æœ‰å‡½æ•°ï¼‰
    text = clean_residual_image_attrs(text)
    # ç§»é™¤ä»»ä½•æ®‹ç•™çš„è£¸ $$ï¼ˆçŠ¶æ€æœºåç†è®ºä¸Šä¸ä¼šå‡ºç°ï¼‰
    text = text.replace('$$', '')

    # æ¸…ç†å¤–å±‚å¤šä½™ç¾å…ƒ: $\(x\)$ â†’ \(x\)
    text = re.sub(r'\$(\\\([^$]+?\\\))\$', r'\1', text)
    # æ¸…ç† $\because$ â†’ \because ï¼ˆä»¥åŠ \thereforeï¼‰
    text = re.sub(r'\$(\\because)\$', r'\1', text)
    text = re.sub(r'\$(\\therefore)\$', r'\1', text)
    # æ¸…ç†ç®€å•å˜é‡å½¢å¼ $x$ è‹¥å•å­—ç¬¦ä¸”ä¸åœ¨å·²æœ‰æ•°å­¦ï¼ˆä¿å®ˆï¼šä»…å­—æ¯/æ•°å­—ï¼‰â†’ \(x\)
    def _wrap_simple_var(m: re.Match) -> str:
        var = m.group(1)
        return f'\\({var}\\)'
    text = re.sub(r'(?<!\\)\$([a-zA-Z0-9])\$', _wrap_simple_var, text)
    # å†æ¬¡ç§»é™¤å¯èƒ½äº§ç”Ÿçš„ç©ºæ•°å­¦å— \(\)
    text = re.sub(r'\\\(\s*\\\)', '', text)
    # å»é™¤é—ç•™çš„å­¤ç«‹å•ç¾å…ƒï¼ˆä¸åœ¨é…å¯¹å†…ï¼‰ï¼šåˆ é™¤
    # åŒ¹é…å•ç‹¬ä¸€è¡ŒåªåŒ…å« $ æˆ–è¡Œé¦–/è¡Œæœ«çš„å•ç¾å…ƒ
    text = re.sub(r'(^|\s)(\$)(?=\s|$)', lambda m: m.group(1), text)
    return text


def validate_math_integrity(tex: str) -> List[str]:
    r"""åˆ†ææœ€ç»ˆ TeX æ•°å­¦å®Œæ•´æ€§é—®é¢˜å¹¶è¿”å›è­¦å‘Šåˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰

    æ£€æŸ¥é¡¹ï¼š
    - è¡Œå†…æ•°å­¦å®šç•Œç¬¦æ•°é‡å·®å¼‚ï¼ˆopens vs closesï¼‰
    - è£¸éœ²ç¾å…ƒç¬¦å·
    - åŒé‡åŒ…è£¹æ®‹ç•™
    - å³è¾¹ç•Œç•¸å½¢ï¼ˆ\right. $$ ç­‰ï¼‰
    - ç©ºæ•°å­¦å—
    - ğŸ†• æˆªæ–­/æœªé—­åˆçš„æ•°å­¦ç‰‡æ®µï¼ˆæ”¶é›†å‰è‹¥å¹²æ ·æœ¬ï¼‰
      å…¸å‹æ¥æºï¼šå›¾ç‰‡å ä½ç¬¦æˆ– explain åˆå¹¶æ—¶è·¨è¡Œè¢«æˆªæ–­ï¼Œå¯¼è‡´ç¼ºå¤± \)
    """
    issues: List[str] = []
    opens = tex.count('\\(')
    closes = tex.count('\\)')
    if opens != closes:
        issues.append(f"Math delimiter imbalance: opens={opens} closes={closes} diff={opens - closes}")

    stray = len(re.findall(r'(?<!\\)\$', tex))
    if stray:
        issues.append(f"Stray dollar signs detected: {stray}")

    double_wrapped = (
        len(re.findall(r'\$\s*\\\(.*?\\\)\s*\$', tex, flags=re.DOTALL)) +
        len(re.findall(r'\$\$\s*\\\(.*?\\\)\s*\$\$', tex, flags=re.DOTALL))
    )
    if double_wrapped:
        issues.append(f"Double-wrapped math segments: {double_wrapped}")

    right_glitch = (
        len(re.findall(r'\\right\.\s*\$\$', tex)) +
        len(re.findall(r'\\right\.\\\\\)', tex))
    )
    if right_glitch:
        issues.append(f"Right boundary glitches: {right_glitch}")

    empty_math = (
        len(re.findall(r'\\\(\s*\\\)', tex)) +
        len(re.findall(r'\\\[\s*\\\]', tex))
    )
    if empty_math:
        issues.append(f"Empty math blocks: {empty_math}")

    # ğŸ†• æˆªæ–­æ£€æµ‹ï¼šä½¿ç”¨é¡ºåºæ‰«æåŒ¹é…æœªé…å¯¹çš„ \\( å’Œ \\)
    unmatched_open_positions: List[int] = []
    unmatched_close_positions: List[int] = []

    token_iter = list(re.finditer(r'(\\\(|\\\))', tex))
    stack: List[int] = []
    for m in token_iter:
        tok = m.group(0)
        pos = m.start()
        if tok == '\\(':  # open
            stack.append(pos)
        else:  # ')'
            if stack:
                stack.pop()
            else:
                unmatched_close_positions.append(pos)
    # å‰©ä½™ stack ä¸­çš„æ˜¯æœªé—­åˆ open
    unmatched_open_positions.extend(stack)

    def _sample_at(pos: int, direction: str = 'forward', span: int = 140) -> str:
        """è·å–ä» pos èµ·çš„ä¸Šä¸‹æ–‡æ ·æœ¬ï¼Œå»é™¤æ¢è¡Œä¸å¤šä½™ç©ºæ ¼"""
        if direction == 'forward':
            raw = tex[pos:pos+span]
        else:
            start = max(0, pos-span)
            raw = tex[start:pos+10]
        # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ª '\\)' ï¼ˆè‹¥å­˜åœ¨ï¼‰
        end_delim = raw.find('\\)')
        if end_delim != -1:
            raw = raw[:end_delim+2]
        raw = re.sub(r'\s+', ' ', raw).strip()
        return raw

    # è¿›ä¸€æ­¥ç”„åˆ«â€œç–‘ä¼¼æˆªæ–­â€ï¼šå¼€æ‹¬å·å 120 å­—ç¬¦å†…æ²¡æœ‰é—­æ‹¬å·
    truncated_open_samples: List[str] = []
    for p in unmatched_open_positions:
        segment = tex[p:p+300]
        if '\\)' not in segment:  # æ˜æ˜¾æ²¡æœ‰é—­åˆ
            truncated_open_samples.append(_sample_at(p, 'forward'))
        else:
            # å¯èƒ½é—­æ‹¬å·è¿œåœ¨è¶…è¿‡ 120 ä¹‹åï¼Œä¹Ÿè®¤ä¸ºå¯ç–‘
            close_rel = segment.find('\\)')
            if close_rel > 120:
                truncated_open_samples.append(_sample_at(p, 'forward'))
        if len(truncated_open_samples) >= 5:  # åªå–å‰ 5 ä¸ªæ ·æœ¬
            break

    truncated_close_samples: List[str] = []
    for p in unmatched_close_positions[:5]:
        truncated_close_samples.append(_sample_at(p, 'backward'))

    if truncated_open_samples:
        issues.append(
            "Unmatched opens (samples): " +
            '; '.join(truncated_open_samples)
        )
    if truncated_close_samples:
        issues.append(
            "Unmatched closes (samples): " +
            '; '.join(truncated_close_samples)
        )

    # é’ˆå¯¹å›¾ç‰‡å ä½ç¬¦é™„è¿‘çš„æˆªæ–­ï¼š\( ... IMAGE_TODO_START æœªé—­åˆ
    image_trunc = re.findall(r'\\\([^\\)]{0,200}?% IMAGE_TODO_START', tex)
    if image_trunc:
        issues.append(f"Potential image-adjacent truncated math segments: {len(image_trunc)}")

    return issues


def generate_image_todo_block(img: Dict, stem_text: str = "", is_inline: bool = False) -> str:
    """ç”Ÿæˆæ–°æ ¼å¼çš„ IMAGE_TODO å ä½å—

    ğŸ†• v1.7ï¼šIMAGE_TODO å—åä¸æ·»åŠ é¢å¤–ç©ºè¡Œ

    Args:
        img: å›¾ç‰‡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« id, path, width, inline, question_index, sub_index
        stem_text: é¢˜å¹²æ–‡æœ¬ï¼Œç”¨äºæå–ä¸Šä¸‹æ–‡
        is_inline: æ˜¯å¦ä¸ºå†…è”å›¾ç‰‡

    Returns:
        æ ¼å¼åŒ–çš„ IMAGE_TODO å ä½å—
    """
    img_id = img.get('id', 'unknown')
    path = img.get('path', '')
    width = img.get('width', 60)
    inline = 'true' if img.get('inline', False) else 'false'
    q_idx = img.get('question_index', 0)
    sub_idx = img.get('sub_index', 1)

    # æå–ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼šå–å›¾ç‰‡å‰åå„50ä¸ªå­—ç¬¦ï¼‰
    # æ¸…ç† context å†…å®¹ï¼šå»é™¤ LaTeX å‘½ä»¤ï¼Œé™åˆ¶é•¿åº¦ï¼Œæ£€æŸ¥æ‹¬å·å¹³è¡¡
    def clean_context(text: str, max_len: int = 50) -> str:
        r"""æ¸…ç† CONTEXT æ³¨é‡Šå†…å®¹

        - å»é™¤ LaTeX å‘½ä»¤ï¼ˆ\xxx{...}ï¼‰
        - å»é™¤æ•°å­¦å®šç•Œç¬¦ \(...\) å’Œ \[...\]
        - æˆªæ–­åˆ°æœ€å¤š max_len å­—ç¬¦
        - æ£€æŸ¥æ‹¬å·å¹³è¡¡ï¼Œå¦‚æœä¸å¹³è¡¡åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not text:
            return ""

        # å»é™¤ LaTeX å‘½ä»¤ï¼ˆ\xxx{...}ï¼‰
        text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', text)
        # å»é™¤æ•°å­¦å®šç•Œç¬¦
        text = re.sub(r'\\\(|\\\)|\\\[|\\\]', '', text)
        # å»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        # æˆªæ–­åˆ°æœ€å¤š max_len å­—ç¬¦
        if len(text) > max_len:
            text = text[:max_len] + '...'

        # æ£€æŸ¥æ‹¬å·å¹³è¡¡
        open_count = text.count('{')
        close_count = text.count('}')
        if open_count != close_count:
            # æ‹¬å·ä¸å¹³è¡¡ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²é¿å…ç¼–è¯‘é”™è¯¯
            return ""

        return text

    context_before = clean_context(img.get('context_before', '').strip())
    context_after = clean_context(img.get('context_after', '').strip())

    # ğŸ†• v1.7ï¼šæ„å»ºå ä½å—ï¼ŒIMAGE_TODO_END åä¸æ·»åŠ é¢å¤–çš„ \n
    if is_inline:
        # å†…è”å›¾ç‰‡ï¼šä¸ä½¿ç”¨ center ç¯å¢ƒ
        block = (
            f"\n% IMAGE_TODO_START id={img_id} path={path} width={width}% inline={inline} "
            f"question_index={q_idx} sub_index={sub_idx}\n"
        )
        if context_before:
            block += f"% CONTEXT_BEFORE: {context_before}\n"
        if context_after:
            block += f"% CONTEXT_AFTER: {context_after}\n"
        block += (
            "\\begin{tikzpicture}[scale=0.8,baseline=-0.5ex]\n"
            f"  % TODO: AI_AGENT_REPLACE_ME (id={img_id})\n"
            "\\end{tikzpicture}\n"
            f"% IMAGE_TODO_END id={img_id}"  # ğŸ†• v1.7ï¼šä¸æ·»åŠ å°¾éš \n
        )
    else:
        # ç‹¬ç«‹å›¾ç‰‡ï¼šä½¿ç”¨ center ç¯å¢ƒ
        block = (
            "\\begin{center}\n"
            f"% IMAGE_TODO_START id={img_id} path={path} width={width}% inline={inline} "
            f"question_index={q_idx} sub_index={sub_idx}\n"
        )
        if context_before:
            block += f"% CONTEXT_BEFORE: {context_before}\n"
        if context_after:
            block += f"% CONTEXT_AFTER: {context_after}\n"
        block += (
            "\\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]\n"
            f"  % TODO: AI_AGENT_REPLACE_ME (id={img_id})\n"
            "\\end{tikzpicture}\n"
            f"% IMAGE_TODO_END id={img_id}\n"
            "\\end{center}"  # ğŸ†• v1.7ï¼šä¸æ·»åŠ å°¾éš \n
        )

    return block


def build_question_tex(stem: str, options: List, meta: Dict, images: List,
                       section_type: str, question_index: int = 0, slug: str = "") -> str:
    """ç”Ÿæˆ question ç¯å¢ƒ

    ğŸ†• Prompt 3: æ”¯æŒå†…è”å›¾ç‰‡å ä½ç¬¦æ›¿æ¢
    ğŸ†• æ–°æ ¼å¼: ä½¿ç”¨ IMAGE_TODO_START/END å¸¦ ID çš„å ä½å—
    """
    # å…ˆå¤„ç†æ–‡æœ¬ï¼Œä½†ä¿ç•™å ä½ç¬¦
    stem_raw = stem  # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºä¸Šä¸‹æ–‡æå–
    stem = process_text_for_latex(stem, is_math_heavy=True)

    if section_type == "è§£ç­”é¢˜" and re.search(r'\(\d+\)', stem):
        stem = handle_subquestions(stem)

    explain_raw = meta.get("explain", "").strip()
    if explain_raw:
        explain_raw = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', explain_raw)
        explain_raw = process_text_for_latex(explain_raw, is_math_heavy=True)

    topics_raw = meta.get("topics", "").strip()
    if topics_raw:
        topics_raw = topics_raw.replace("ã€", "ï¼›")
        topics_raw = escape_latex_special(topics_raw, in_math_mode=False)

    lines = []
    lines.append(r"\begin{question}")

    if stem:
        lines.append(stem)

    if options:
        lines.append(r"\begin{choices}")
        for opt in options:
            opt_processed = process_text_for_latex(opt, is_math_heavy=True)
            lines.append(f"  \\item {opt_processed}")
        lines.append(r"\end{choices}")

    # ğŸ†• æ–°æ ¼å¼: ä½¿ç”¨ IMAGE_TODO_START/END å ä½å—
    for idx, img in enumerate(images):
        # ç”Ÿæˆæ–°æ ¼å¼çš„å ä½å—
        img_todo_block = generate_image_todo_block(img, stem_raw, img.get('inline', False))

        if img.get('inline', False):
            # å†…è”å›¾ç‰‡ï¼šæ›¿æ¢å ä½ç¬¦
            placeholder = f"<<IMAGE_INLINE:{img.get('id', f'img{idx}')}>>"
            stem = stem.replace(placeholder, img_todo_block)
            explain_raw = explain_raw.replace(placeholder, img_todo_block) if explain_raw else explain_raw
            # æ›´æ–°å·²å¤„ç†çš„é€‰é¡¹
            for i, line in enumerate(lines):
                if placeholder in line:
                    lines[i] = line.replace(placeholder, img_todo_block)
        else:
            # ç‹¬ç«‹å›¾ç‰‡ï¼šè¿½åŠ åˆ°é¢˜ç›®æœ«å°¾
            lines.append("")
            lines.append(img_todo_block)

    if topics_raw:
        lines.append(f"\\topics{{{topics_raw}}}")
    if meta.get("difficulty"):
        lines.append(f"\\difficulty{{{meta['difficulty']}}}")
    if meta.get("answer"):
        # ä½¿ç”¨ä¸é¢˜å¹²/è§£æä¸€è‡´çš„å¤„ç†ï¼Œä»¥è§„èŒƒæ•°å­¦æ ¼å¼ï¼Œé¿å… $$...$$ æ®‹ç•™
        ans = process_text_for_latex(meta["answer"], is_math_heavy=True)
        lines.append(f"\\answer{{{ans}}}")
    if explain_raw:
        lines.append(f"\\explain{{{explain_raw}}}")

    lines.append(r"\end{question}")
    return "\n".join(lines)


def convert_md_to_examx(md_text: str, title: str, slug: str = "", enable_issue_detection: bool = True) -> str:
    """ä¸»è½¬æ¢å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰

    ğŸ†• v1.6.3ï¼šå¢åŠ é—®é¢˜æ£€æµ‹å’Œæ—¥å¿—è®°å½•

    Args:
        md_text: Markdown æ–‡æœ¬
        title: è¯•å·æ ‡é¢˜
        slug: è¯•å· slugï¼ˆç”¨äºæ—¥å¿—æ–‡ä»¶åï¼‰
        enable_issue_detection: æ˜¯å¦å¯ç”¨é—®é¢˜æ£€æµ‹
    """
    md_text = clean_markdown(md_text)
    sections = split_sections(md_text)

    # ğŸ†• v1.6.3ï¼šåˆå§‹åŒ–é—®é¢˜æ—¥å¿—
    if enable_issue_detection and slug:
        init_issue_log(slug)

    out_lines = []
    out_lines.append(f"\\examxtitle{{{title}}}")

    q_index = 0  # å…¨å±€é¢˜å·è®¡æ•°å™¨
    for raw_title, body in sections:
        sec_label = SECTION_MAP.get(raw_title, raw_title)
        out_lines.append("")
        out_lines.append(f"\\section{{{sec_label}}}")

        for block in split_questions(body):
            if not block.strip():
                continue

            q_index += 1  # é¢˜å·é€’å¢
            raw_block = block  # ä¿å­˜åŸå§‹ Markdown ç‰‡æ®µ

            try:
                # ğŸ†• ä¼ é€’ question_index å’Œ slug ç”¨äºç”Ÿæˆå›¾ç‰‡ ID
                content, meta, images = extract_meta_and_images(block, question_index=q_index, slug=slug)

                # ä½¿ç”¨å¢å¼ºçš„è½¬æ¢å‡½æ•°ï¼ˆè¿”å›3ä¸ªå€¼ï¼‰
                stem, options, extracted_analysis = convert_choices(content)

                # åˆå¹¶æå–çš„è§£æå’Œå…ƒä¿¡æ¯ä¸­çš„è§£æ
                if extracted_analysis and not meta.get('explain'):
                    meta['explain'] = extracted_analysis
                elif extracted_analysis:
                    meta['explain'] = meta['explain'] + '\n' + extracted_analysis

                # ğŸ†• ä¼ é€’ question_index å’Œ slug åˆ° build_question_tex
                q_tex = build_question_tex(stem, options, meta, images, sec_label,
                                          question_index=q_index, slug=slug)

                # ğŸ†• v1.6.4ï¼šæ£€æµ‹é—®é¢˜å¹¶è®°å½•æ—¥å¿—ï¼ˆä¼ å…¥ meta & section_labelï¼‰
                if enable_issue_detection and slug:
                    issues = detect_question_issues(
                        slug=slug,
                        q_index=q_index,
                        raw_block=raw_block,
                        tex_block=q_tex,
                        meta=meta,
                        section_label=sec_label,
                    )
                    append_issue_log(
                        slug=slug,
                        q_index=q_index,
                        raw_block=raw_block,
                        tex_block=q_tex,
                        issues=issues,
                        meta=meta,
                        section_label=sec_label,
                    )

                # éªŒè¯ç”Ÿæˆçš„ TeX æ˜¯å¦å®Œæ•´
                if r'\begin{question}' in q_tex and r'\end{question}' not in q_tex:
                    print(f"âš ï¸  Q{q_index} ç¼ºå°‘ \\end{{question}}ï¼Œè‡ªåŠ¨è¡¥å…¨")
                    q_tex += "\n\\end{question}"

                out_lines.append("")
                out_lines.append(q_tex)
            except Exception as e:
                import traceback
                print(f"âš ï¸  Q{q_index} ({sec_label}) è½¬æ¢å¤±è´¥: {str(e)}")
                print(f"   {traceback.format_exc()}")
                out_lines.append("")
                out_lines.append(r"\begin{question}")
                out_lines.append(f"% ERROR: Q{q_index} è½¬æ¢å¤±è´¥ - {str(e)}")
                out_lines.append(r"\end{question}")

    out_lines.append("")

    # æœ€ç»ˆå¤„ç†ï¼šæ¸…ç†ç©ºè¡Œå’Œåˆ†å‰²è¶…é•¿è¡Œ
    result = "\n".join(out_lines)
    result = remove_blank_lines_in_macro_args(result)
    result = split_long_lines_in_explain(result, max_length=800)
    # ğŸ”¥ v1.8.3ï¼šé‡æ–°å¯ç”¨ï¼ˆå·²ä¿®å¤æ‹¬å·è®¡æ•°é€»è¾‘ï¼‰
    result = remove_par_breaks_in_explain(result)
    # ğŸ”¥ v1.8.1ï¼šclean_question_environments ä»ç„¶ç¦ç”¨ï¼ˆæ­£åˆ™åŒ¹é…é—®é¢˜ï¼‰

    # æœ€ç»ˆå…œåº•ï¼šè§„èŒƒ/ç§»é™¤æ®‹ç•™çš„ $$ æ˜¾ç¤ºæ•°å­¦æ ‡è®°
    # 1) å°†æˆå¯¹ $$...$$ ç»Ÿä¸€ä¸ºè¡Œå†… \(...\)ï¼ˆä¸ smart_inline_math è¡Œä¸ºä¸€è‡´ï¼‰
    result = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', result, flags=re.DOTALL)
    # 2) æ¸…ç†ä»»ä½•æ®‹ç•™çš„å­¤ç«‹ $$ï¼ˆé¿å…ç¼–è¯‘é”™è¯¯ï¼‰
    result = result.replace('$$', '')

    # ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    result = cleanup_remaining_image_markers(result)

    # ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™ï¼ˆåˆ†ä¸¤æ­¥ï¼‰
    result = cleanup_guxuan_in_macros(result)

    # ğŸ†• v1.6.1ï¼šå…¨å±€æ¸…ç†ä»»ä½•æ®‹ç•™çš„"æ•…é€‰"ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
    # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰ï¼šX"ï¼Œæ— è®ºåœ¨ä»€ä¹ˆä½ç½®
    result = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n}]*', '', result)
    # æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
    result = re.sub(r'æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*\.?', '', result)

    return result


# ==================== ğŸ†• v1.6.3 æ–°å¢ï¼šé—®é¢˜æ£€æµ‹ä¸æ—¥å¿—ç³»ç»Ÿ ====================

def detect_question_issues(
    slug: str,
    q_index: int,
    raw_block: str,
    tex_block: str,
    meta: Optional[Dict[str, str]] = None,
    section_label: Optional[str] = None,
) -> List[str]:
    """ğŸ†• v1.7ï¼šæ£€æµ‹é¢˜ç›®ä¸­çš„å¯ç–‘æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆï¼‰
    ğŸ†• v1.6.4ï¼šæ£€æµ‹é¢˜ç›®ä¸­çš„å¯ç–‘æ¨¡å¼ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        slug: è¯•å· slugï¼ˆå¦‚ "nanjing_2026_sep"ï¼‰
        q_index: é¢˜å·ï¼ˆä» 1 å¼€å§‹ï¼‰
        raw_block: åŸå§‹ Markdown ç‰‡æ®µ
        tex_block: ç”Ÿæˆçš„ TeX ç‰‡æ®µ
        meta: è§£æå¾—åˆ°çš„å…ƒä¿¡æ¯å­—å…¸ï¼ˆç­”æ¡ˆã€éš¾åº¦ã€çŸ¥è¯†ç‚¹ã€è§£æç­‰ï¼‰
        section_label: å½“å‰å¤§é¢˜æ ‡é¢˜ï¼ˆå¦‚ "å•é€‰é¢˜"ã€"å¤šé€‰é¢˜" ç­‰ï¼‰

    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    issues: List[str] = []

    # ---------- ğŸ†• v1.7ï¼šæ£€æµ‹ç¼ºå°‘é¢˜å¹²çš„é¢˜ç›® ----------
    # æ£€æŸ¥é¢˜ç›®æ˜¯å¦ç›´æ¥ä» \item å¼€å§‹ï¼ˆç¼ºå°‘é¢˜å¹²ï¼‰
    # åœ¨ \begin{question} åï¼Œå¦‚æœç¬¬ä¸€ä¸ªéç©ºè¡Œæ˜¯ \item æˆ– \begin{choices}ï¼Œåˆ™ç¼ºå°‘é¢˜å¹²
    question_content = tex_block
    if r'\begin{question}' in question_content:
        # æå– \begin{question} å’Œ \begin{choices} ä¹‹é—´çš„å†…å®¹
        match = re.search(r'\\begin\{question\}(.*?)(?:\\begin\{choices\}|\\item|\\end\{question\})',
                         question_content, re.DOTALL)
        if match:
            content_between = match.group(1).strip()
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–åªæœ‰æ³¨é‡Šï¼Œåˆ™ç¼ºå°‘é¢˜å¹²
            # ç§»é™¤æ³¨é‡Šè¡Œ
            content_no_comments = re.sub(r'^\s*%.*$', '', content_between, flags=re.MULTILINE).strip()
            if not content_no_comments:
                issues.append("âš ï¸ CRITICAL: é¢˜ç›®ç¼ºå°‘é¢˜å¹²ï¼Œç›´æ¥ä» \\item å¼€å§‹ - è¯·åœ¨ Markdown ä¸­è¡¥å……é¢˜å¹²å†…å®¹")

    # ---------- 1) åŸæœ‰æ£€æŸ¥é€»è¾‘ï¼ˆä¿ç•™ & å¤åˆ»ï¼‰ ----------

    # 1.1 æ£€æµ‹ meta å½¢å¼çš„ã€åˆ†æã€‘ï¼ˆä¸åº”è¯¥å‡ºç°ï¼‰
    if "ã€åˆ†æã€‘" in raw_block and "ã€åˆ†æã€‘" in tex_block:
        issues.append("Contains meta ã€åˆ†æã€‘ in both raw and tex (should be discarded)")
    elif "ã€åˆ†æã€‘" in tex_block:
        issues.append("Contains meta ã€åˆ†æã€‘ in tex (should be discarded)")

    # 1.2 æ£€æµ‹ *$x$* æˆ–å…¶ä»– star + math æ¨¡å¼
    if re.search(r'\*\s*\$', tex_block) or re.search(r'\$\s*\*', tex_block):
        issues.append("Star-emphasis around inline math, e.g. *$x$*")

    # 1.3 æ£€æµ‹ç©º $$ æˆ–å½¢å¦‚ $$\(
    if re.search(r'\$\s*\$', tex_block):
        issues.append("Empty inline/ display math $$")
    if re.search(r'\$\s*\$\s*\\\(', tex_block):
        issues.append("Suspicious pattern $$\\(")

    # 1.4 æ£€æµ‹è¡Œå†… math åˆ†éš”ç¬¦æ•°é‡æ˜æ˜¾ä¸åŒ¹é…
    open_count = tex_block.count(r'\(')
    close_count = tex_block.count(r'\)')
    if open_count != close_count:
        issues.append(f"Unbalanced inline math delimiters: ${open_count} vs$ {close_count}")

    # 1.5 æ£€æµ‹å…¨è§’æ‹¬å·æ®‹ç•™
    if 'ï¼ˆ' in tex_block or 'ï¼‰' in tex_block:
        issues.append("Fullwidth brackets ï¼ˆï¼‰found in tex")

    # 1.6 æ£€æµ‹"æ•…é€‰"æ®‹ç•™
    if re.search(r'æ•…é€‰[:ï¼š][ABCD]+', tex_block):
        issues.append("'æ•…é€‰' pattern found in tex")

    # ---------- 2) æ–°å¢ï¼šåŸºäº meta çš„ä¸€è‡´æ€§æ£€æŸ¥ ----------

    if meta is not None:
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å–å€¼å¹¶ strip
        def _get(key: str) -> str:
            return (meta.get(key) or "").strip()

        answer = _get("answer")
        difficulty = _get("difficulty")
        topics = _get("topics")
        explain = _get("explain")
        analysis = _get("analysis")

        # 2.1 æ£€æŸ¥"åˆ†æ"å­—æ®µæ˜¯å¦ä»ç„¶å­˜åœ¨ï¼ˆæŒ‰è§„èŒƒåº”ä¸¢å¼ƒï¼Œä»…å…è®¸ä½œä¸ºä¸­é—´æ€ï¼Œè€Œä¸åº”å†™å…¥ TeXï¼‰
        if analysis:
            issues.append("Meta contains 'analysis' field (ã€åˆ†æã€‘) â€“ it should not be used in output")

        # 2.2 æ£€æŸ¥ section/å¤§é¢˜ ä¸ç­”æ¡ˆå¿…éœ€æ€§
        sec = section_label or ""
        is_choice_section = ("å•é€‰" in sec) or ("å¤šé€‰" in sec)

        # å¯¹é€‰æ‹©é¢˜ï¼Œå°é¢˜é€šå¸¸å¿…é¡»æœ‰ç­”æ¡ˆ
        if is_choice_section and not answer:
            issues.append("Choice question in section '{0}' has no ã€ç­”æ¡ˆã€‘ meta".format(sec or "?"))

        # å¯¹äºéé€‰æ‹©é¢˜ï¼Œç­”æ¡ˆç¼ºå¤±ä¸ä¸€å®šæ˜¯è‡´å‘½é”™è¯¯ï¼Œä½†å¯ä»¥æç¤º
        if not is_choice_section and not answer:
            issues.append("Question has no ã€ç­”æ¡ˆã€‘ meta (section='{0}')".format(sec or "?"))

        # 2.3 meta ä¸ TeX çš„æ˜ å°„ä¸€è‡´æ€§
        has_answer_macro = "\\answer{" in tex_block
        has_explain_macro = "\\explain{" in tex_block

        if answer and not has_answer_macro:
            issues.append("Meta has answer but TeX is missing \\answer{}")
        if has_answer_macro and not answer:
            issues.append("TeX has \\answer{} but meta.answer is empty")

        if explain and not has_explain_macro:
            issues.append("Meta has explain but TeX is missing \\explain{}")
        if has_explain_macro and not explain:
            issues.append("TeX has \\explain{} but meta.explain is empty")

        # 2.4 ç¡®ä¿ \\explain{} ä¸ä¼šå·å·åƒè¿›ã€åˆ†æã€‘å†…å®¹
        # è¿™é‡Œåªåšç®€å•æ–‡æœ¬çº§æ£€æµ‹ï¼šå¦‚æœ raw_block é‡Œæœ‰"ã€åˆ†æã€‘"ä¸” meta.explain ä¸ºç©ºï¼Œåˆ™é¢å¤–æç¤º
        if "ã€åˆ†æã€‘" in raw_block and not explain:
            issues.append("Raw block contains ã€åˆ†æã€‘ but meta.explain is empty â€“ this question is treated as 'no explain'")

        # 2.5 æ£€æµ‹è¶…é•¿ explain å†…å®¹ï¼ˆ>500è¡Œï¼‰
        if explain:
            explain_lines = explain.count('\n') + 1
            if explain_lines > 500:
                issues.append(f"âš ï¸  LONG_EXPLAIN: {explain_lines} lines (>500) â€“ may cause conversion issues")
            elif explain_lines > 200:
                issues.append(f"Long explain: {explain_lines} lines (>200) â€“ consider simplification")

    return issues


def append_issue_log(
    slug: str,
    q_index: int,
    raw_block: str,
    tex_block: str,
    issues: List[str],
    meta: Optional[Dict[str, str]] = None,
    section_label: Optional[str] = None,
) -> None:
    """ğŸ†• v1.6.4ï¼šå°†é—®é¢˜è®°å½•åˆ° debug æ—¥å¿—ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        slug: è¯•å· slug
        q_index: é¢˜å·
        raw_block: åŸå§‹ Markdown ç‰‡æ®µ
        tex_block: ç”Ÿæˆçš„ TeX ç‰‡æ®µ
        issues: é—®é¢˜åˆ—è¡¨
        meta: è§£æå¾—åˆ°çš„å…ƒä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰
        section_label: å½“å‰å¤§é¢˜æ ‡é¢˜ï¼ˆå¦‚ "å•é€‰é¢˜" / "å¤šé€‰é¢˜" ç­‰ï¼‰
    """
    if not issues:
        return

    debug_dir = Path("word_to_tex/output/debug")
    debug_dir.mkdir(parents=True, exist_ok=True)
    log_file = debug_dir / f"{slug}_issues.log"

    with log_file.open("a", encoding="utf-8") as f:
        f.write(f"{'='*80}\n")
        f.write(f"# Q{q_index} issues (section={section_label or 'N/A'})\n\n")

        # ç®€è¦ meta æ¦‚è§ˆï¼ˆå¦‚æœæœ‰ï¼‰
        if meta is not None:
            # åªå±•ç¤ºå…³é”®ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—å¤ªå†—é•¿
            summary_keys = ["answer", "difficulty", "topics", "explain", "analysis"]
            f.write("## Meta summary:\n")
            for key in summary_keys:
                if key in meta:
                    val = (meta.get(key) or "").strip()
                    if len(val) > 80:
                        val_display = val[:77] + "..."
                    else:
                        val_display = val
                    f.write(f"- {key}: {val_display}\n")
            f.write("\n")

        f.write("## Issues:\n")
        for issue in issues:
            f.write(f"- {issue}\n")

        f.write("\n## Raw Markdown:\n")
        f.write("```markdown\n")
        f.write(raw_block.strip() + "\n")
        f.write("```\n\n")

        f.write("## Generated TeX:\n")
        f.write("```tex\n")
        f.write(tex_block.strip() + "\n")
        f.write("```\n\n")


def init_issue_log(slug: str) -> None:
    """ğŸ†• v1.6.3ï¼šåˆå§‹åŒ–é—®é¢˜æ—¥å¿—æ–‡ä»¶

    Args:
        slug: è¯•å· slug
    """
    debug_dir = Path("word_to_tex/output/debug")
    debug_dir.mkdir(parents=True, exist_ok=True)
    log_file = debug_dir / f"{slug}_issues.log"

    # æ¸…ç©ºæ—§æ—¥å¿—
    with log_file.open("w", encoding="utf-8") as f:
        f.write(f"# Issue Detection Log for {slug}\n")
        f.write(f"# Generated: {Path(__file__).name} v{VERSION}\n")
        f.write(f"# Date: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n")


# ==================== ğŸ†• v1.3 æ–°å¢ï¼šè‡ªåŠ¨éªŒè¯å‡½æ•° ====================

def assert_no_analysis_meta_in_auto_tex(slug: str) -> None:
    """ğŸ†• v1.6.3ï¼šæ£€æŸ¥ auto ç›®å½•ä¸­æ˜¯å¦æ®‹ç•™ã€åˆ†æã€‘meta æ®µ

    Args:
        slug: è¯•å· slugï¼ˆå¦‚ "nanjing_2026_sep"ï¼‰

    Raises:
        RuntimeError: å¦‚æœå‘ç°ã€åˆ†æã€‘æ®‹ç•™
    """
    root = Path("content/exams/auto") / slug
    if not root.exists():
        return

    for tex in root.rglob("*.tex"):
        txt = tex.read_text(encoding="utf-8")
        # åªæ‹¦ç±»ä¼¼ã€åˆ†æã€‘è¿™ç±» meta æ®µï¼Œè€Œä¸æ˜¯è‡ªç„¶è¯­è¨€ä¸­çš„"åˆ†æ"äºŒå­—
        if re.search(r"ã€\s*åˆ†æ\s*ã€‘", txt):
            raise RuntimeError(f"[ANALYSIS-META-LEFTOVER] {tex} still contains ã€åˆ†æã€‘.")


def validate_latex_output(tex_content: str) -> List[str]:
    """
    ğŸ†• v1.3 æ–°å¢ï¼šéªŒè¯LaTeXè¾“å‡ºï¼Œè¿”å›è­¦å‘Šåˆ—è¡¨
    ğŸ†• v1.6.3ï¼šå¢åŠ ã€åˆ†æã€‘æ®‹ç•™æ£€æŸ¥

    Args:
        tex_content: ç”Ÿæˆçš„LaTeXå†…å®¹

    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    warnings = []

    # ğŸ†• æ£€æŸ¥0ï¼šã€åˆ†æã€‘meta æ®µæ®‹ç•™
    analysis_meta = re.findall(r'ã€\s*åˆ†æ\s*ã€‘', tex_content)
    if analysis_meta:
        warnings.append(f"âŒ å‘ç° {len(analysis_meta)} å¤„ã€åˆ†æã€‘meta æ®µæ®‹ç•™ï¼ˆåº”å·²è¢«ä¸¢å¼ƒï¼‰")

    # æ£€æŸ¥1ï¼šæ®‹ç•™çš„ $ ç¬¦å·
    dollar_matches = re.findall(r'(?<!\\)\$[^\$]+\$', tex_content)
    if dollar_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(dollar_matches)} å¤„æ®‹ç•™çš„ $ æ ¼å¼")
        for i, match in enumerate(dollar_matches[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            warnings.append(f"     ç¤ºä¾‹{i}: {match}")

    # æ£€æŸ¥2ï¼šæ®‹ç•™çš„"æ•…é€‰"
    guxuan_matches = re.findall(r'æ•…é€‰[:ï¼š][ABCD]+', tex_content)
    if guxuan_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(guxuan_matches)} å¤„æ®‹ç•™çš„'æ•…é€‰'")

    # æ£€æŸ¥3ï¼šä¸­æ–‡æ‹¬å·
    chinese_paren = re.findall(r'[ï¼ˆï¼‰]', tex_content)
    if chinese_paren:
        warnings.append(f"âš ï¸  å‘ç° {len(chinese_paren)} å¤„ä¸­æ–‡æ‹¬å·")

    # æ£€æŸ¥4ï¼šç¯å¢ƒé—­åˆ
    begin_count = tex_content.count(r'\begin{question}')
    end_count = tex_content.count(r'\end{question}')
    if begin_count != end_count:
        warnings.append(f"âŒ question ç¯å¢ƒä¸åŒ¹é…: {begin_count} ä¸ª begin, {end_count} ä¸ª end")

    begin_choices = tex_content.count(r'\begin{choices}')
    end_choices = tex_content.count(r'\end{choices}')
    if begin_choices != end_choices:
        warnings.append(f"âŒ choices ç¯å¢ƒä¸åŒ¹é…: {begin_choices} ä¸ª begin, {end_choices} ä¸ª end")

    # æ£€æŸ¥5ï¼šç©ºè¡Œåœ¨å®å‚æ•°ä¸­
    problematic_macros = []
    for macro in ['explain', 'topics', 'answer']:
        pattern = rf'\\{macro}\{{[^}}]*\n\s*\n[^}}]*\}}'
        if re.search(pattern, tex_content):
            problematic_macros.append(macro)
    if problematic_macros:
        warnings.append(f"âš ï¸  ä»¥ä¸‹å®å‚æ•°ä¸­å¯èƒ½æœ‰ç©ºè¡Œ: {', '.join(problematic_macros)}")

    return warnings


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description=f"OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ†• v1.5 æ ¸å¿ƒåŠŸèƒ½ï¼š
  - ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆ$$\\(...\\)$$ â†’ \\(...\\)ï¼‰
  - ç»Ÿä¸€æ•°å­¦å…¬å¼æ ¼å¼ï¼šæ‰€æœ‰ $$...$$ è½¬æ¢ä¸ºè¡Œå†… \\(...\\)
  - è‡ªåŠ¨å±•å¼€å•è¡Œé€‰é¡¹ï¼ˆ> A... B... â†’ å¤šè¡Œï¼‰
  - å¼ºåˆ¶æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™ï¼ˆç¡®ä¿å·²è¢«ä¸¢å¼ƒï¼‰

âœ… v1.4 æ”¹è¿›å›é¡¾ï¼š
  - æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ä¿®å¤ï¼ˆåˆç‰ˆï¼‰
  - å•è¡Œé€‰é¡¹è‡ªåŠ¨å±•å¼€ï¼ˆåˆç‰ˆï¼‰

âœ… v1.3 æ”¹è¿›å›é¡¾ï¼š
  - ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢
  - æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™
  - ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆæ‹¬å·ã€å¼•å·ï¼‰
  - æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

âœ… v1.2 æ”¹è¿›å›é¡¾ï¼š
  - åŠ å¼ºç©ºè¡Œæ¸…ç†ï¼ˆè§£å†³80%çš„Runaway argumenté”™è¯¯ï¼‰
  - è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²ï¼ˆè§£å†³ç¼–è¯‘æ…¢é—®é¢˜ï¼‰
  - å¢å¼ºæ•°å­¦å˜é‡æ£€æµ‹ï¼ˆå‡å°‘Missing $é”™è¯¯ï¼‰
  - å¢å¼ºé€‰é¡¹è§£æï¼ˆå¤„ç†åµŒå…¥çš„è§£æå†…å®¹ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
  python3 ocr_to_examx.py "æµ™æ±Ÿçœé‡‘ååæ ¡/" output/
        """
    )
    
    parser.add_argument("input", help="è¾“å…¥è·¯å¾„ï¼ˆ.md æ–‡ä»¶æˆ– OCR æ–‡ä»¶å¤¹ï¼‰")
    parser.add_argument("output", help="è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ– .tex æ–‡ä»¶ï¼‰")
    parser.add_argument("--title", help="è¯•å·æ ‡é¢˜", default=None)
    parser.add_argument("--legacy-math", action="store_true", help="ä½¿ç”¨æ—§æ•°å­¦æ­£åˆ™ç®¡çº¿ (smart_inline_math ç­‰) è¿›è¡Œæ•°å­¦å¤„ç†ï¼Œä»…æµ‹è¯•æ¯”è¾ƒç”¨")
    parser.add_argument("--version", action="version", version=f"%(prog)s {VERSION}")
    
    args = parser.parse_args()
    
    try:
        print(f"ğŸ” OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}")
        print("â”" * 60)
        # å¯é€‰ï¼šåˆ‡æ¢åˆ°æ—§æ•°å­¦ç®¡çº¿ï¼ˆA/B æµ‹è¯•ç”¨ï¼‰
        _orig_process = None
        if args.legacy_math:
            print("âš ï¸ ä½¿ç”¨ legacy æ•°å­¦ç®¡çº¿ (smart_inline_math ç­‰) â€” ä»…ä¾›æ¯”è¾ƒæµ‹è¯•")
            _orig_process = process_text_for_latex
            def _legacy_wrapper(t: str, is_math_heavy: bool = False):
                if not t:
                    return t
                # å‰ç½®æ¸…ç†ï¼ˆå¤ç”¨ç°è¡Œç‰ˆæœ¬çš„åˆæ®µé€»è¾‘ï¼‰
                t = re.sub(r'\*\s*(\$[^$]+\$)\s*\*', r'\1', t)
                t = re.sub(r'\*([A-Za-z0-9])\*', r'\\emph{\1}', t)
                t = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t)
                t = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t)
                t = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', t)
                t = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', t)
                t = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', t, flags=re.MULTILINE)
                t = re.sub(r'[ï¼Œ,]?\s*æ•…é€‰[:ï¼š]\s*[ABCD]+[ã€‚ï¼.]*\s*$', '', t, flags=re.MULTILINE)
                t = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', t)
                if 'âˆµ' in t or 'âˆ´' in t:
                    t = t.replace('âˆµ', '$\\because$').replace('âˆ´', '$\\therefore$')
                if not is_math_heavy:
                    t = escape_latex_special(t, in_math_mode=False)
                t = smart_inline_math(t)
                t = fix_double_wrapped_math(t)
                t = fix_inline_math_glitches(t)
                return t
            process_text_for_latex = _legacy_wrapper  # type: ignore

        md_file, images_dir = find_markdown_and_images(args.input)
        
        print(f"ğŸ“„ Markdown: {md_file.name}")
        if images_dir:
            img_count = len(list(images_dir.glob('*')))
            print(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {images_dir} ({img_count} ä¸ªæ–‡ä»¶)")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•")
        
        output_path = Path(args.output)
        if output_path.suffix == '.tex':
            output_tex = output_path
            output_dir = output_path.parent
        else:
            output_dir = output_path
            output_tex = output_dir / f"{md_file.stem.replace('_local', '_raw')}.tex"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if images_dir:
            img_count = copy_images_to_output(images_dir, output_dir)
            print(f"âœ… å·²å¤åˆ¶ {img_count} ä¸ªå›¾ç‰‡åˆ° {output_dir}/images/")
        
        title = args.title
        if title is None:
            input_path = Path(args.input)
            if input_path.is_dir():
                title = input_path.name
            else:
                title = md_file.stem.replace('_local', '')

        # ğŸ†• v1.6.3ï¼šæå– slug ç”¨äºé—®é¢˜æ—¥å¿—
        slug = md_file.stem.replace('_local', '').replace('_preprocessed', '').replace('_raw', '')

        print(f"\nğŸ“– æ­£åœ¨è½¬æ¢...")
        print(f"ğŸ“ æ ‡é¢˜: {title}")
        print(f"ğŸ·ï¸  Slug: {slug}")

        md_text = md_file.read_text(encoding='utf-8')
        tex_text = convert_md_to_examx(md_text, title, slug=slug, enable_issue_detection=True)
        
        # ğŸ†• v1.6 P0 ä¿®å¤ï¼šåå¤„ç†æ¸…ç†
        tex_text = fix_array_boundaries(tex_text)
        tex_text = clean_residual_image_attrs(tex_text)
        
        # ğŸ†• v1.3ï¼šéªŒè¯è¾“å‡º
        warnings = validate_latex_output(tex_text)
        integrity_issues = validate_math_integrity(tex_text)
        
        output_tex.write_text(tex_text, encoding='utf-8')
        
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
        print("â”" * 60)
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶: {output_tex}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(tex_text):,} å­—èŠ‚")
        
        question_count = tex_text.count(r'\begin{question}')
        image_count = tex_text.count('IMAGE_TODO')
        print(f"ğŸ“‹ é¢˜ç›®æ•°é‡: {question_count}")
        if image_count > 0:
            print(f"ğŸ–¼ï¸  å›¾ç‰‡å ä½: {image_count}")
        
        print(f"\nğŸ†• v1.4 æ”¹è¿›å·²åº”ç”¨:")
        print(f"  âœ… æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ä¿®å¤")
        print(f"  âœ… å•è¡Œé€‰é¡¹è‡ªåŠ¨å±•å¼€")
        print(f"  âœ… æ˜¾ç¤ºå…¬å¼æ­£ç¡®å¤„ç†")
        
        print(f"\nâœ… v1.3 æ”¹è¿›ï¼ˆå·²ä¿ç•™ï¼‰:")
        print(f"  âœ… $ æ ¼å¼å…œåº•è½¬æ¢")
        print(f"  âœ… å¢å¼ºçš„'æ•…é€‰'æ¸…ç†")
        print(f"  âœ… ä¸­è‹±æ–‡æ ‡ç‚¹ç»Ÿä¸€")
        print(f"  âœ… è‡ªåŠ¨éªŒè¯åŠŸèƒ½")
        
        print(f"\nâœ… v1.2 æ”¹è¿›ï¼ˆå·²ä¿ç•™ï¼‰:")
        print(f"  âœ… ç©ºè¡Œæ¸…ç†å¢å¼º")
        print(f"  âœ… è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²")
        print(f"  âœ… æ•°å­¦å˜é‡æ™ºèƒ½æ£€æµ‹")
        print(f"  âœ… é€‰é¡¹è§£æå¢å¼º")
        
        # ğŸ†• v1.6.3ï¼šæ˜¾ç¤ºé—®é¢˜æ—¥å¿—ä¿¡æ¯
        debug_log = Path("word_to_tex/output/debug") / f"{slug}_issues.log"
        if debug_log.exists():
            log_size = debug_log.stat().st_size
            if log_size > 100:  # å¦‚æœæ—¥å¿—æ–‡ä»¶æœ‰å®è´¨å†…å®¹
                print(f"\nğŸ“‹ é—®é¢˜æ£€æµ‹æ—¥å¿—: {debug_log}")
                print(f"   æ–‡ä»¶å¤§å°: {log_size:,} å­—èŠ‚")
            else:
                print(f"\nâœ… æœªæ£€æµ‹åˆ°é—®é¢˜ï¼ˆæ—¥å¿—ä¸ºç©ºï¼‰")

        # ğŸ†• v1.3ï¼šæ˜¾ç¤ºéªŒè¯ç»“æœ
        if warnings or integrity_issues:
            combined = warnings + integrity_issues
            print(f"\nâš ï¸  éªŒè¯å‘ç° {len(combined)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for issue in combined:
                print(f"  {issue}")
            print("\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ AI Agent æ£€æŸ¥å¹¶äººå·¥ç¡®è®¤æ•°å­¦ç»“æ„")
        else:
            print(f"\nâœ… éªŒè¯é€šè¿‡ï¼šæœªå‘ç°æ˜æ˜¾é—®é¢˜ (ç»“æ„ + æ•°å­¦)" )

        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. AI Agent è¯»å–æ­¤æ–‡ä»¶è¿›è¡Œç²¾ä¿®")
        print("  2. AI Agent æŸ¥çœ‹ images/ ä¸­çš„å›¾ç‰‡")
        print("  3. AI Agent ç”Ÿæˆ TikZ ä»£ç ")
        print("  4. è¾“å‡ºæœ€ç»ˆçš„ exam_final.tex")
        if debug_log.exists() and debug_log.stat().st_size > 100:
            print(f"  5. æŸ¥çœ‹é—®é¢˜æ—¥å¿—: {debug_log}")

        # ğŸ†• Prompt 1: å¼ºåˆ¶æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™
        if slug:
            print(f"\nğŸ” æ£€æŸ¥ã€åˆ†æã€‘æ®‹ç•™...")
            try:
                assert_no_analysis_meta_in_auto_tex(slug)
                print(f"âœ… æœªå‘ç°ã€åˆ†æã€‘æ®‹ç•™")
            except RuntimeError as e:
                print(f"âŒ {e}")
                raise

        # æ¢å¤åŸæ•°å­¦å¤„ç†å‡½æ•°ï¼ˆè‹¥å¯ç”¨ legacyï¼‰
        if _orig_process is not None:
            process_text_for_latex = _orig_process  # type: ignore
        return 0
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


# ==================== ğŸ†• v1.6.3 æ–°å¢ï¼šç®€å•å•å…ƒæµ‹è¯• ====================

def run_self_tests() -> bool:
    """ğŸ†• v1.6.3ï¼šè¿è¡Œç®€å•çš„è‡ªæµ‹ç”¨ä¾‹

    Returns:
        True if all tests pass, False otherwise
    """
    print("ğŸ§ª è¿è¡Œè‡ªæµ‹ç”¨ä¾‹...")
    print("=" * 60)

    all_passed = True

    # æµ‹è¯• 1ï¼šã€åˆ†æã€‘æ®µè¢«æ­£ç¡®ä¸¢å¼ƒ
    print("\næµ‹è¯• 1: ã€åˆ†æã€‘æ®µè¢«æ­£ç¡®ä¸¢å¼ƒ")
    test_md = """
# ä¸€ã€å•é€‰é¢˜

1. æµ‹è¯•é¢˜ç›®

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€åˆ†æã€‘è¿™æ˜¯åˆ†æå†…å®¹ï¼Œåº”è¯¥è¢«ä¸¢å¼ƒ
ã€è¯¦è§£ã€‘è¿™æ˜¯è¯¦è§£å†…å®¹ï¼Œåº”è¯¥è¢«ä¿ç•™
ã€ç­”æ¡ˆã€‘A
"""
    result = convert_md_to_examx(test_md, "æµ‹è¯•", slug="", enable_issue_detection=False)
    if "ã€åˆ†æã€‘" in result:
        print("  âŒ FAILED: ã€åˆ†æã€‘æœªè¢«ä¸¢å¼ƒ")
        all_passed = False
    elif "è¿™æ˜¯åˆ†æå†…å®¹" in result:
        print("  âŒ FAILED: åˆ†æå†…å®¹æœªè¢«ä¸¢å¼ƒ")
        all_passed = False
    elif "è¿™æ˜¯è¯¦è§£å†…å®¹" not in result:
        print("  âŒ FAILED: è¯¦è§£å†…å®¹æœªè¢«ä¿ç•™")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 2ï¼šã€è¯¦è§£ã€‘è¢«æ­£ç¡®ä¿ç•™åˆ° \explain{}
    print("\næµ‹è¯• 2: ã€è¯¦è§£ã€‘è¢«æ­£ç¡®ä¿ç•™åˆ° \\explain{}")
    if "\\explain{" in result and "è¿™æ˜¯è¯¦è§£å†…å®¹" in result:
        print("  âœ… PASSED")
    else:
        print("  âŒ FAILED: è¯¦è§£æœªæ­£ç¡®ä¿ç•™")
        all_passed = False

    # æµ‹è¯• 3ï¼š*$x$* æ¨¡å¼è¢«æ­£ç¡®ä¿®å¤
    print("\næµ‹è¯• 3: *$x$* æ¨¡å¼è¢«æ­£ç¡®ä¿®å¤")
    test_text = "è¿™æ˜¯ä¸€ä¸ª *$x$* å˜é‡å’Œ *y* å¼ºè°ƒ"
    result_text = process_text_for_latex(test_text, is_math_heavy=True)
    if "*$" in result_text or "$*" in result_text:
        print(f"  âŒ FAILED: *$x$* æ¨¡å¼æœªè¢«ä¿®å¤")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    elif "\\emph{y}" not in result_text:
        print(f"  âŒ FAILED: *y* æœªè½¬æ¢ä¸º \\emph{{y}}")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 4ï¼šå…¨è§’æ‹¬å·è¢«ç»Ÿä¸€
    print("\næµ‹è¯• 4: å…¨è§’æ‹¬å·è¢«ç»Ÿä¸€")
    test_text = "è¿™æ˜¯ï¼ˆå…¨è§’æ‹¬å·ï¼‰å’Œï½›èŠ±æ‹¬å·ï½"
    result_text = normalize_fullwidth_brackets(test_text)
    if "ï¼ˆ" in result_text or "ï¼‰" in result_text or "ï½›" in result_text or "ï½" in result_text:
        print(f"  âŒ FAILED: å…¨è§’æ‹¬å·æœªè¢«ç»Ÿä¸€")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 5ï¼šç©º $$ è¢«æ¸…ç†
    print("\næµ‹è¯• 5: ç©º $$ è¢«æ¸…ç†")
    test_text = "è¿™æ˜¯ $$ ç©ºæ•°å­¦å’Œ $x$ æ­£å¸¸æ•°å­¦"
    result_text = fix_inline_math_glitches(test_text)
    if "$$" in result_text:
        print(f"  âŒ FAILED: ç©º $$ æœªè¢«æ¸…ç†")
        print(f"     ç»“æœ: {result_text}")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 6ï¼šå†…è”å›¾ç‰‡è¢«æ­£ç¡®å¤„ç†ï¼ˆæ—§ç‰ˆï¼‰
    print("\næµ‹è¯• 6: å†…è”å›¾ç‰‡è¢«æ­£ç¡®å¤„ç†ï¼ˆæ—§ç‰ˆï¼‰")
    test_md = """
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥é›†åˆ![](image2.wmf)ï¼Œåˆ™ Aâˆ©B ç­‰äº

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€ç­”æ¡ˆã€‘A
"""
    result = convert_md_to_examx(test_md, "æµ‹è¯•", slug="", enable_issue_detection=False)
    # æ£€æŸ¥ï¼šä¸åº”è¯¥æœ‰æ®‹ç•™çš„ ![](image2.wmf)
    if "![](image2.wmf)" in result:
        print(f"  âŒ FAILED: å†…è”å›¾ç‰‡æ ‡è®°æœªè¢«è½¬æ¢")
        all_passed = False
    # æ£€æŸ¥ï¼šåº”è¯¥æœ‰ IMAGE_TODO æ³¨é‡Š
    elif "IMAGE_TODO" not in result or "image2.wmf" not in result:
        print(f"  âŒ FAILED: å†…è”å›¾ç‰‡æœªç”Ÿæˆ IMAGE_TODO å ä½ç¬¦")
        all_passed = False
    else:
        print("  âœ… PASSED")

    # æµ‹è¯• 7ï¼šæ–°æ ¼å¼ IMAGE_TODO_START/END å ä½å—
    print("\næµ‹è¯• 7: æ–°æ ¼å¼ IMAGE_TODO_START/END å ä½å—")
    test_md_new = """
# ä¸€ã€å•é€‰é¢˜

1. å·²çŸ¥å‡½æ•° f(x) åœ¨åŒºé—´ [0,1] ä¸Šå•è°ƒé€’å¢ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š

![](media/graph1.png)

åˆ™ä¸‹åˆ—ç»“è®ºä¸­æ­£ç¡®çš„æ˜¯

A. f(0) < f(1)
B. f(0) > f(1)

ã€ç­”æ¡ˆã€‘A

2. é›†åˆ A={x|x>0}ï¼Œé›†åˆ B å¦‚å›¾![](media/venn.wmf)æ‰€ç¤ºï¼Œåˆ™ Aâˆ©B ç­‰äº

A. é€‰é¡¹A
B. é€‰é¡¹B

ã€ç­”æ¡ˆã€‘B
"""
    result_new = convert_md_to_examx(test_md_new, "æµ‹è¯•æ–°æ ¼å¼", slug="test2025", enable_issue_detection=False)

    # æ£€æŸ¥1ï¼šä¸åº”è¯¥æœ‰æ®‹ç•™çš„ Markdown å›¾ç‰‡è¯­æ³•
    if "![](media/graph1.png)" in result_new or "![](media/venn.wmf)" in result_new:
        print(f"  âŒ FAILED: Markdown å›¾ç‰‡è¯­æ³•æœªè¢«è½¬æ¢")
        all_passed = False
    # æ£€æŸ¥2ï¼šåº”è¯¥æœ‰ä¸¤ä¸ª IMAGE_TODO_START æ ‡è®°
    elif result_new.count("IMAGE_TODO_START") != 2:
        print(f"  âŒ FAILED: IMAGE_TODO_START æ•°é‡ä¸æ­£ç¡® (æœŸæœ›2ä¸ªï¼Œå®é™…{result_new.count('IMAGE_TODO_START')}ä¸ª)")
        all_passed = False
    # æ£€æŸ¥3ï¼šåº”è¯¥æœ‰ä¸¤ä¸ª IMAGE_TODO_END æ ‡è®°
    elif result_new.count("IMAGE_TODO_END") != 2:
        print(f"  âŒ FAILED: IMAGE_TODO_END æ•°é‡ä¸æ­£ç¡®")
        all_passed = False
    # æ£€æŸ¥4ï¼šç¬¬ä¸€ä¸ªå›¾ç‰‡åº”è¯¥æ˜¯ç‹¬ç«‹å›¾ç‰‡ (inline=false)
    elif "inline=false" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°ç‹¬ç«‹å›¾ç‰‡æ ‡è®° (inline=false)")
        all_passed = False
    # æ£€æŸ¥5ï¼šç¬¬äºŒä¸ªå›¾ç‰‡åº”è¯¥æ˜¯å†…è”å›¾ç‰‡ (inline=true)
    elif "inline=true" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°å†…è”å›¾ç‰‡æ ‡è®° (inline=true)")
        all_passed = False
    # æ£€æŸ¥6ï¼šåº”è¯¥åŒ…å« question_index å­—æ®µ
    elif "question_index=" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ° question_index å­—æ®µ")
        all_passed = False
    # æ£€æŸ¥7ï¼šåº”è¯¥åŒ…å« AI_AGENT_REPLACE_ME æ ‡è®°
    elif "AI_AGENT_REPLACE_ME" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ° AI_AGENT_REPLACE_ME æ ‡è®°")
        all_passed = False
    # æ£€æŸ¥8ï¼šåº”è¯¥åŒ…å« CONTEXT_BEFORE æˆ– CONTEXT_AFTER
    elif "CONTEXT_BEFORE" not in result_new and "CONTEXT_AFTER" not in result_new:
        print(f"  âŒ FAILED: æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ (CONTEXT_BEFORE/AFTER)")
        all_passed = False
    # æ£€æŸ¥9ï¼šID åº”è¯¥åŒ…å« slug å’Œé¢˜å·
    elif "test2025-Q1" not in result_new or "test2025-Q2" not in result_new:
        print(f"  âŒ FAILED: å›¾ç‰‡ ID æ ¼å¼ä¸æ­£ç¡® (åº”åŒ…å« slug-Q{n})")
        all_passed = False
    else:
        print("  âœ… PASSED")
        # æ‰“å°ä¸€ä¸ªç¤ºä¾‹ä¾›æ£€æŸ¥
        print("\n  ç¤ºä¾‹è¾“å‡ºç‰‡æ®µ:")
        lines = result_new.split('\n')
        for i, line in enumerate(lines):
            if 'IMAGE_TODO_START' in line:
                # æ‰“å°è¯¥è¡ŒåŠåç»­5è¡Œ
                for j in range(i, min(i+6, len(lines))):
                    print(f"    {lines[j]}")
                break

    print("\n" + "=" * 60)
    if all_passed:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--selftest":
        success = run_self_tests()
        exit(0 if success else 1)
    else:
        exit(main())

