#!/usr/bin/env python3
# -*- coding: utf-8 -*-
r"""
ocr_to_examx_v1.5.py - v1.5 å¢å¼ºç‰ˆ OCR è¯•å·é¢„å¤„ç†è„šæœ¬

v1.5 æ ¸å¿ƒä¿®å¤ï¼ˆ2025-11-18ï¼‰ï¼š
1. âœ… å½»åº•ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆ$$\(...\)$$ â†’ \(...\)ï¼‰
   - æ”¹è¿› smart_inline_math é¿å…åµŒå¥—
   - æ–°å¢ fix_double_wrapped_math åå¤„ç†æ¸…ç†
   - ä¼˜å…ˆå°† $$ è½¬ä¸º \(...\) è€Œé \[...\]ï¼ˆexamx å…¼å®¹ï¼‰
2. âœ… æ”¹è¿›å•è¡Œé€‰é¡¹å±•å¼€ï¼ˆ> A... B... C... D... â†’ å¤šè¡Œï¼‰
   - æ›´ç²¾ç¡®çš„é€‰é¡¹åˆ†å‰²æ­£åˆ™
   - ä¿ç•™é€‰é¡¹å†…çš„æ•°å­¦å…¬å¼å’Œæ ‡ç‚¹
3. âœ… å‡å°‘æ‰‹åŠ¨ä¿®æ­£å·¥ä½œé‡ï¼š2å°æ—¶ â†’ 15åˆ†é’Ÿ (ç›®æ ‡ -87.5%)

v1.4 æ”¹è¿›å›é¡¾ï¼š
- ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆåˆç‰ˆï¼‰
- è‡ªåŠ¨å±•å¼€å•è¡Œé€‰é¡¹ï¼ˆåˆç‰ˆï¼‰
- æ­£ç¡®å¤„ç†æ˜¾ç¤ºå…¬å¼

v1.3 æ”¹è¿›å›é¡¾ï¼š
- ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢
- æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™
- ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
- æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

ç‰ˆæœ¬ï¼šv1.5
ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2025-11-18
"""

import re
import argparse
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# ==================== é…ç½® ====================

VERSION = "v1.5"

SECTION_MAP = {
    "ä¸€ã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å¤šé€‰é¢˜": "å¤šé€‰é¢˜",
    "ä¸‰ã€å¡«ç©ºé¢˜": "å¡«ç©ºé¢˜",
    "å››ã€è§£ç­”é¢˜": "è§£ç­”é¢˜",
}

META_PATTERNS = {
    "answer": r"^ã€ç­”æ¡ˆã€‘(.*)$",
    "difficulty": r"^ã€éš¾åº¦ã€‘([\d.]+)",
    "topics": r"^ã€çŸ¥è¯†ç‚¹ã€‘(.*)$",
    "analysis": r"^ã€åˆ†æã€‘(.*)$",
    "explain": r"^ã€è¯¦è§£ã€‘(.*)$",
}

# ğŸ†• æ‰©å±•å›¾ç‰‡æ£€æµ‹ï¼šæ”¯æŒç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ã€å¤šè¡Œå±æ€§å—
# åŒ¹é…ä¸¤ç§å½¢å¼ï¼š
#   1) å¸¦ID: ![@@@id](path){...}
#   2) æ— ID: ![](path){...}
# å±æ€§å—å¯è·¨å¤šè¡Œï¼Œå¯é€‰
IMAGE_PATTERN_WITH_ID = re.compile(
    r"!\[@@@([^\]]+)\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
IMAGE_PATTERN_NO_ID = re.compile(
    r"!\[\]\(([^)]+)\)(?:\s*\{[^}]*\})?",
    re.MULTILINE | re.DOTALL,
)
# å…¼å®¹æ—§ç‰ˆï¼ˆä¿ç•™ç”¨äºç®€å•åœºæ™¯ï¼‰
IMAGE_PATTERN = re.compile(r"!\[\]\((images/[^)]+)\)(?:\{width=(\d+)%\})?")

LATEX_SPECIAL_CHARS = {
    "%": r"\%",
    "&": r"\&",
    "#": r"\#",
    "~": r"\textasciitilde{}",
}

# è§£ææ ‡è®°è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼‰
ANALYSIS_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—',
    'ç»¼ä¸Š', 'æ•…', 'å³', 'åˆ™', 'å¯å¾—'
]


# æ›´ä¸¥æ ¼çš„è§£æèµ·å§‹è¯ï¼Œåªç”¨äºåˆ¤æ–­æ˜¯å¦è¿›å…¥è§£ææ®µè½ï¼ˆé¿å…åƒâ€œåˆ™â€è¿™æ ·åœ¨é¢˜å¹²ä¸­å‡ºç°æ—¶è¢«è¯¯åˆ¤ï¼‰
ANALYSIS_START_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—', 'ç»¼ä¸Š'
]

# ==================== æ–‡ä»¶å¤¹å¤„ç†å‡½æ•° ====================

def find_markdown_and_images(input_path: Path) -> Tuple[Path, Optional[Path]]:
    """æ™ºèƒ½è¯†åˆ«è¾“å…¥è·¯å¾„"""
    input_path = Path(input_path).resolve()
    
    if input_path.is_file() and input_path.suffix == '.md':
        md_file = input_path
        images_dir = input_path.parent / 'images'
        if not images_dir.exists():
            images_dir = None
        return md_file, images_dir
    
    if input_path.is_dir():
        md_files = list(input_path.glob('*_local.md'))
        if not md_files:
            md_files = list(input_path.glob('*.md'))
        
        if not md_files:
            raise FileNotFoundError(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        
        if len(md_files) > 1:
            print(f"âš ï¸  æ‰¾åˆ°å¤šä¸ª .md æ–‡ä»¶ï¼Œä½¿ç”¨ï¼š{md_files[0].name}")
        
        md_file = md_files[0]
        images_dir = input_path / 'images'
        if not images_dir.exists():
            images_dir = None
        
        return md_file, images_dir
    
    raise ValueError(f"æ— æ•ˆçš„è¾“å…¥ï¼š{input_path}")


def copy_images_to_output(images_dir: Path, output_dir: Path) -> int:
    """å¤åˆ¶å›¾ç‰‡"""
    if images_dir is None or not images_dir.exists():
        return 0
    
    output_images_dir = output_dir / 'images'
    if output_images_dir.exists():
        shutil.rmtree(output_images_dir)
    
    shutil.copytree(images_dir, output_images_dir)
    return len(list(output_images_dir.glob('*')))


# ==================== LaTeX å¤„ç†å‡½æ•° ====================

def escape_latex_special(text: str, in_math_mode: bool = False) -> str:
    """è½¬ä¹‰ LaTeX ç‰¹æ®Šå­—ç¬¦"""
    if in_math_mode:
        for char in ["%", "&", "#", "~"]:
            if char in LATEX_SPECIAL_CHARS:
                text = text.replace(char, LATEX_SPECIAL_CHARS[char])
    else:
        protected = []
        def save_comment(match):
            protected.append(match.group(0))
            return f"@@COMMENT_{len(protected)-1}@@"
        text = re.sub(r'%.*$', save_comment, text, flags=re.MULTILINE)
        
        for char, escaped in LATEX_SPECIAL_CHARS.items():
            text = text.replace(char, escaped)
        
        for i, comment in enumerate(protected):
            text = text.replace(f"@@COMMENT_{i}@@", comment)
    return text


def smart_inline_math(text: str) -> str:
    r"""æ™ºèƒ½è½¬æ¢è¡Œå†…å…¬å¼ï¼š$...$ -> \(...\)ï¼Œ$$...$$ -> \(...\)
    
    ğŸ†• v1.5 æ”¹è¿›ï¼šå½»åº•é¿å…åŒé‡åŒ…è£¹ï¼Œexamx ç»Ÿä¸€ä½¿ç”¨ \(...\)
    """
    if not text:
        return text
    
    # æ­¥éª¤1: ä¿æŠ¤å·²æœ‰çš„è¡Œå†…å…¬å¼ \(...\)ï¼ˆé¿å…é‡å¤è½¬æ¢ï¼‰
    inline_math_blocks = []
    def save_inline(match):
        inline_math_blocks.append(match.group(0))
        return f"@@INLINEMATH{len(inline_math_blocks)-1}@@"
    text = re.sub(r'\\\((.+?)\\\)', save_inline, text, flags=re.DOTALL)
    
    # æ­¥éª¤2: ä¿æŠ¤å·²æœ‰çš„æ˜¾ç¤ºå…¬å¼ \[...\]ï¼ˆä¿æŒä¸å˜ï¼‰
    display_math_blocks = []
    def save_display(match):
        display_math_blocks.append(match.group(0))
        return f"@@DISPLAYMATH{len(display_math_blocks)-1}@@"
    text = re.sub(r'\\\[(.+?)\\\]', save_display, text, flags=re.DOTALL)
    
    # æ­¥éª¤3: ä¿æŠ¤TikZåæ ‡ $(A)$ æˆ– $(A)!0.5!(B)$ æˆ– $(A)+(1,2)$
    tikz_coords = []
    def save_tikz_coord(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)!0.5!(B)$' æˆ– '$(0,1)$'
        inner = block[2:-2]         # å»æ‰å¤–å±‚ '$(' å’Œ ')$'
        # ä»…å½“å†…éƒ¨åŒ…å« '!' æˆ– å¤§å†™å­—æ¯ æ—¶ï¼Œè®¤ä¸ºæ˜¯ TikZ åæ ‡è¡¨è¾¾å¼
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZCOORD{len(tikz_coords)-1}@@"
        else:
            # å¦åˆ™è®¤ä¸ºæ˜¯æ™®é€šæ•°å­¦åæ ‡/åŒºé—´ï¼ŒåŸæ ·è¿”å›
            return block
    # åŒ¹é… TikZ åæ ‡ï¼š$(...)$ å†…éƒ¨æ˜¯ç®€å•çš„åæ ‡è®¡ç®—è¡¨è¾¾å¼
    # åŒ…å«å­—æ¯ã€æ•°å­—ã€æ‹¬å·ã€åŠ å‡ä¹˜é™¤ã€ç‚¹ã€æ„Ÿå¹å·ã€å†’å·ç­‰ä½†ä¸åŒ…å«å¤æ‚æ•°å­¦
    text = re.sub(r'\$\([A-Za-z0-9!+\-*/\.\(\):,\s]+\)\$', save_tikz_coord, text)
    
    # æ­¥éª¤4: è½¬æ¢æ˜¾ç¤ºå…¬å¼ $$ ... $$ ä¸º \(...\)ï¼ˆexamx é£æ ¼ï¼‰
    # ä¼˜å…ˆå¤„ç†å¤šè¡Œæ˜¾ç¤ºå…¬å¼
    text = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # æ­¥éª¤5: è½¬æ¢å• $ ... $ ä¸º \(...\)
    text = re.sub(r'(?<!\\)\$([^\$]+?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤6: å…œåº•æ£€æŸ¥ï¼Œæ¸…ç†æ®‹ç•™çš„å• $ï¼ˆå•è¡Œå†…ï¼Œé™åˆ¶200å­—ç¬¦ï¼‰
    text = re.sub(r'(?<!\\)\$([^\$\n]{1,200}?)\$', r'\\(\1\\)', text)
    
    # æ­¥éª¤7: æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, block in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZCOORD{i}@@", block)
    for i, block in enumerate(display_math_blocks):
        text = text.replace(f"@@DISPLAYMATH{i}@@", block)
    for i, block in enumerate(inline_math_blocks):
        text = text.replace(f"@@INLINEMATH{i}@@", block)
    
    return text


def fix_double_wrapped_math(text: str) -> str:
    r"""ä¿®æ­£åŒé‡åŒ…è£¹çš„æ•°å­¦å…¬å¼
    
    ğŸ†• v1.5 æ–°å¢ï¼šæ¸…ç†å¯èƒ½æ®‹ç•™çš„åµŒå¥—æ ¼å¼
    ä¾‹å¦‚ï¼š$$\(...\)$$ â†’ \(...\)
    """
    if not text:
        return text
    
    # ä¿®æ­£ $$\(...\)$$ æˆ– $$\[...\]$$
    # æ³¨æ„ï¼š\\\( åŒ¹é…å­—é¢çš„ \(
    text = re.sub(r'\$\$\s*\\\((.+?)\\\)\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\$\s*\\\[(.+?)\\\]\s*\$\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ $\(...\)$ æˆ– $\[...\]$
    text = re.sub(r'\$\s*\\\((.+?)\\\)\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    text = re.sub(r'\$\s*\\\[(.+?)\\\]\s*\$', r'\\(\1\\)', text, flags=re.DOTALL)
    
    # ä¿®æ­£ä¸‰é‡åµŒå¥—ï¼ˆæç«¯æƒ…å†µï¼‰
    text = re.sub(r'\\\(\s*\\\((.+?)\\\)\s*\\\)', r'\\(\1\\)', text, flags=re.DOTALL)
    
    return text


def wrap_math_variables(text: str) -> str:
    """æ™ºèƒ½åŒ…è£¹æ•°å­¦å˜é‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    # ä¿æŠ¤å·²æœ‰çš„æ•°å­¦æ¨¡å¼
    protected = []
    def save_math(match):
        protected.append(match.group(0))
        return f"@@MATH{len(protected)-1}@@"
    
    text = re.sub(r'\\\(.*?\\\)', save_math, text, flags=re.DOTALL)
    text = re.sub(r'\\\[.*?\\\]', save_math, text, flags=re.DOTALL)
    
    # ä¿æŠ¤ TikZ åæ ‡
    tikz_coords = []
    def save_tikz(match):
        block = match.group(0)      # å½¢å¦‚ '$(A)$' æˆ– '$(0,1)$'
        inner = block[2:-2]
        if '!' in inner or re.search(r'[A-Z]', inner):
            tikz_coords.append(block)
            return f"@@TIKZ{len(tikz_coords)-1}@@"
        else:
            return block
    text = re.sub(r'\$\([\d\w\s,+\-*/\.]+\)\$', save_tikz, text)
    
    # è§„åˆ™1ï¼šå•å­—æ¯å˜é‡ + è¿ç®—ç¬¦/ä¸‹æ ‡/ä¸Šæ ‡
    text = re.sub(
        r'\b([a-zA-Z])(?=\s*[=+\-*/^<>]|_{|\^{)',
        r'\\(\1\\)',
        text
    )
    
    # è§„åˆ™2ï¼šæ•°å­¦å‡½æ•°å¿…é¡»æœ‰åæ–œæ 
    math_functions = [
        'sin', 'cos', 'tan', 'cot', 'sec', 'csc',
        'arcsin', 'arccos', 'arctan',
        'sinh', 'cosh', 'tanh',
        'log', 'ln', 'lg', 'exp',
        'lim', 'sup', 'inf',
        'max', 'min', 'det', 'dim', 'ker'
    ]
    for func in math_functions:
        text = re.sub(rf'(?<!\\)\b{func}\b(?!\w)', rf'\\{func}', text)
    
    # è§„åˆ™3ï¼šè™šæ•°å•ä½ i
    text = re.sub(r'(?<!\\)\bi\b(?=[^a-zA-Z])', r'\\mathrm{i}', text)
    
    # æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, coord in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZ{i}@@", coord)
    for i, math in enumerate(protected):
        text = text.replace(f"@@MATH{i}@@", math)
    
    return text


def _sanitize_math_block(block: str) -> str:
    """ä¿®æ­£æ•°å­¦å—å†…éƒ¨çš„ OCR é”™è¯¯
    
    ä¿®å¤ï¼š
    - \\left / \\right ä¸åŒ¹é…æ—¶é™çº§ä¸ºæ™®é€šæ‹¬å·
    - \\right.\\ ) ç­‰ç•¸å½¢ç»„åˆ
    """
    if not block:
        return block
    
    # ç»Ÿä¸€æ•°å­¦ç¯å¢ƒå†…çš„ä¸­æ–‡æ ‡ç‚¹ä¸ºè‹±æ–‡æ ‡ç‚¹
    block = (block
             .replace('ï¼Œ', ',')
             .replace('ï¼š', ':')
             .replace('ï¼›', ';')
             .replace('ã€‚', '.')
             .replace('ã€', ','))

    # æ›¿æ¢å¸¸è§çš„ Unicode ç¬¦å·ä¸º LaTeX å‘½ä»¤ï¼ˆé¿å…ç¼ºå­—å½¢ï¼‰
    block = block.replace('âˆµ', r'\\because').replace('âˆ´', r'\\therefore')

    # å°†ä¸Šä¸‹æ ‡ä¸­çš„ä¸­æ–‡åŒ…è£…ä¸º \text{...}
    # å½¢å¼ä¸€ï¼š_[{...ä¸­æ–‡...}] æˆ– ^[{...ä¸­æ–‡...}]
    def _wrap_cjk_in_braced_subsup(m: re.Match) -> str:
        lead = m.group(1)
        inner = m.group(2)
        if '\\text{' in inner:
            return f"{lead}{{{inner}}}"
        return f"{lead}{{\\text{{{inner}}}}}"
    block = re.sub(r'([_^])\{([^{}]*?[\u4e00-\u9fff]+[^{}]*?)\}', _wrap_cjk_in_braced_subsup, block)

    # å½¢å¼äºŒï¼šå•å­—ç¬¦ä¸Šä¸‹æ ‡ï¼š_æ°´ æˆ– ^é«˜
    block = re.sub(r'([_^])([\u4e00-\u9fff])', r'\1{\\text{\2}}', block)

    # æ•°å­¦å†…å¸¸è§ä¸­æ–‡è¿æ¥è¯ï¼Œæ›¿æ¢ä¸º \text{...}ï¼ˆä¿å®ˆé›†ï¼‰
    for w in ['ä¸”', 'æˆ–', 'åˆ™', 'å³', 'æ•…', 'æ‰€ä»¥', 'å› ä¸º']:
        block = re.sub(fr'(?<!\\text\{{){re.escape(w)}(?![^\{{]*\}})', rf'\\text{{{w}}}', block)
    
    # ç»Ÿè®¡ left/right æ•°é‡
    left_count = len(re.findall(r'\\left\b', block))
    right_count = len(re.findall(r'\\right\b', block))
    
    # ä¿®å¤ç•¸å½¢ \right.\ ) å’Œ \right.\\)
    block = re.sub(r'\\right\.\s*\\\s*\)', r'\\right.', block)
    # ä¿®å¤ \right.\\\) æ¨¡å¼ï¼ˆarrayç»“å°¾çš„å¸¸è§OCRé”™è¯¯ï¼‰
    block = re.sub(r'\\right\.\\\\+\)', r'\\right.', block)
    
    # å¦‚æœ left/right ä¸åŒ¹é…ï¼Œé™çº§ä¸ºæ™®é€šæ‹¬å·
    if left_count != right_count:
        block = re.sub(r'\\left\s*([\(\[\{])', r'\1', block)
        block = re.sub(r'\\right\s*([\)\]\}])', r'\1', block)
        block = re.sub(r'\\left\.', '', block)
        block = re.sub(r'\\right\.', '', block)
    
    return block


def sanitize_math(text: str) -> str:
    """æ‰«æå…¨æ–‡ï¼Œä»…ä¿®æ­£æ•°å­¦ç¯å¢ƒå†…çš„ OCR é”™è¯¯
    
    åªå¤„ç† \\(...\\) å’Œ \\[...\\] å†…éƒ¨çš„å†…å®¹ã€‚
    """
    if not text:
        return text
    
    result = []
    i = 0
    n = len(text)
    
    while i < n:
        # åŒ¹é… \(..\)
        if text.startswith(r"\(", i):
            j = text.find(r"\)", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\(" + inner + r"\)")
            i = j + 2
            continue
        
        # åŒ¹é… \[..\]
        if text.startswith(r"\[", i):
            j = text.find(r"\]", i + 2)
            if j == -1:
                result.append(text[i:])
                break
            inner = text[i+2:j]
            inner = _sanitize_math_block(inner)
            result.append(r"\[" + inner + r"\]")
            i = j + 2
            continue
        
        result.append(text[i])
        i += 1
    
    return "".join(result)


def remove_blank_lines_in_macro_args(text: str) -> str:
    """åˆ é™¤å®å‚æ•°ä¸­çš„ç©ºè¡Œï¼ˆå¢å¼ºç‰ˆï¼‰"""
    macros = ['explain', 'topics', 'answer', 'difficulty', 'source']
    
    for macro in macros:
        pattern = rf'(\\{macro}\{{)([^{{}}]*(?:\{{[^{{}}]*\}}[^{{}}]*)*?)(\}})'
        
        def clean_arg(match):
            prefix = match.group(1)
            arg = match.group(2)
            suffix = match.group(3)
            
            # æ”¹è¿›1ï¼šåˆ é™¤è¿ç»­ç©ºè¡Œ
            arg = re.sub(r'\n\s*\n+', '\n', arg)
            
            # æ”¹è¿›2ï¼šåˆ é™¤æ®µé¦–æ®µå°¾ç©ºè¡Œ
            arg = arg.strip()
            
            # æ”¹è¿›3ï¼šæ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼ˆä¿ç•™ç¼©è¿›ï¼‰
            lines = arg.split('\n')
            lines = [line.rstrip() for line in lines]
            arg = '\n'.join(lines)
            
            return prefix + arg + suffix
        
        text = re.sub(pattern, clean_arg, text, flags=re.DOTALL)
    
    return text


def clean_question_environments(text: str) -> str:
    """æ¸…ç† question ç¯å¢ƒå†…éƒ¨çš„å¤šä½™ç©ºè¡Œ"""
    pattern = r'(\\begin\{question\})(.*?)(\\end\{question\})'
    
    def clean_env(match):
        begin = match.group(1)
        content = match.group(2)
        end = match.group(3)
        
        # åˆ é™¤è¿ç»­çš„3ä¸ªä»¥ä¸Šæ¢è¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        return begin + content + end
    
    return re.sub(pattern, clean_env, text, flags=re.DOTALL)


def split_long_lines_in_explain(text: str, max_length: int = 800) -> str:
    """åœ¨ explain{} ä¸­è‡ªåŠ¨åˆ†å‰²è¶…é•¿è¡Œ"""
    pattern = r'(\\explain\{)([^{}]*(?:\{[^{}]*\}[^{}]*)*?)(\})'
    
    def split_content(match):
        prefix = match.group(1)
        content = match.group(2)
        suffix = match.group(3)
        
        lines = content.split('\n')
        new_lines = []
        
        for line in lines:
            if len(line) <= max_length:
                new_lines.append(line)
            else:
                # åœ¨æ ‡ç‚¹ååˆ†å‰²
                segments = re.split(r'([ï¼Œã€‚ï¼›ï¼ï¼Ÿ])', line)
                current = ""
                for seg in segments:
                    if len(current + seg) > max_length and current:
                        new_lines.append(current.rstrip())
                        current = seg
                    else:
                        current += seg
                if current:
                    new_lines.append(current.rstrip())
        
        return prefix + '\n'.join(new_lines) + suffix
    
    return re.sub(pattern, split_content, text, flags=re.DOTALL)


def remove_par_breaks_in_explain(text: str) -> str:
    r"""ç§»é™¤ \explain{...} ä¸­çš„ç©ºæ®µè½ï¼ˆä¸¥æ ¼åŸºäºå¤§æ‹¬å·è®¡æ•°ï¼‰
    è§£å†³ TeX ä¸­æ®µè½æ–­å¼€å¯¼è‡´çš„ "Paragraph ended before \explain code was complete"ã€‚
    """
    # è§„èŒƒåŒ–æ¢è¡Œç¬¦
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    
    out = []
    i = 0
    n = len(text)
    while i < n:
        if text.startswith("\\explain{", i):
            # å¤åˆ¶å®å
            out.append("\\explain{")
            i += len("\\explain{")
            depth = 1
            buf = []
            while i < n and depth > 0:
                ch = text[i]
                # å¤„ç†è½¬ä¹‰çš„å¤§æ‹¬å· \{ æˆ– \}ï¼šä½œä¸ºæ™®é€šå­—ç¬¦ï¼Œä¸è®¡å…¥æ·±åº¦
                if ch == '\\' and i + 1 < n and text[i + 1] in '{}':
                    buf.append(text[i:i+2])
                    i += 2
                    continue
                # å¤„ç†æ¢è¡Œï¼šè‹¥é‡åˆ°ç©ºæ®µè½ï¼ˆ\n\s*\nï¼‰ï¼Œå‹ç¼©ä¸ºå•æ¢è¡Œ
                if ch == '\n':
                    # æŸ¥çœ‹æ˜¯å¦ä¸ºç©ºæ®µè½
                    j = i + 1
                    while j < n and text[j] in ' \t':
                        j += 1
                    if j < n and text[j] == '\n':
                        # è·³è¿‡ç¬¬äºŒä¸ªæ¢è¡Œå‰çš„ç©ºç™½ï¼Œåªä¿ç•™ä¸€ä¸ªæ¢è¡Œ
                        buf.append('\n')
                        i = j + 1
                        continue
                if ch == '{':
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        # å…³é—­ï¼Œå†™å…¥ç¼“å†²å¹¶ç»“æŸæ­¤ explain
                        out.append(''.join(buf))
                        out.append('}')
                        i += 1
                        break
                buf.append(ch)
                i += 1
            continue
        else:
            out.append(text[i])
            i += 1
    return ''.join(out)


def cleanup_remaining_image_markers(text: str) -> str:
    """ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    
    ğŸ†• v1.6.2ï¼šå¢å¼ºå†…è”å…¬å¼å¤„ç†
    - ç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡ â†’ TikZå ä½ç¬¦å—ï¼ˆå¤§å›¾ï¼‰
    - å†…è”å›¾ç‰‡ï¼ˆå…¬å¼ï¼‰â†’ ç®€å•æ–‡æœ¬å ä½ç¬¦ [å…¬å¼:filename]
    
    å°†æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…åœ¨ PDF ä¸­æ˜¾ç¤º
    ä¸ºåŸå§‹è·¯å¾„æ–‡æœ¬ã€‚æ”¯æŒï¼š
      - ![@@@id](path){...}
      - ![](path){...}
    """
    if not text:
        return text
    
    def _make_tikz_placeholder(label: str) -> str:
        """åˆ›å»º TikZ å ä½ç¬¦å—ï¼ˆç”¨äºç‹¬ç«‹å›¾ç‰‡ï¼‰"""
        label = label.strip() or "image"
        return (
            "\n\\begin{center}\n"
            "\\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]\n"
            f"  \\node[draw, minimum width=6cm, minimum height=4cm] {{å›¾ç•¥ï¼ˆå›¾ ID: {label}ï¼‰}};\n"
            "\\end{tikzpicture}\n"
            "\\end{center}\n"
        )
    
    def _make_inline_placeholder(label: str) -> str:
        """åˆ›å»ºå†…è”å ä½ç¬¦ï¼ˆç”¨äºå…¬å¼å›¾ç‰‡ï¼‰"""
        label = label.strip() or "formula"
        # ä½¿ç”¨ç®€å•çš„æ–‡æœ¬å ä½ç¬¦ï¼Œå¯ä»¥åœ¨åç»­è¢«è¯†åˆ«å’Œæ›¿æ¢
        return f"[å…¬å¼:{label}]"
    
    def is_standalone_line(match_obj: re.Match, full_text: str) -> bool:
        """åˆ¤æ–­åŒ¹é…æ˜¯å¦ä¸ºç‹¬ç«‹æˆè¡Œçš„å›¾ç‰‡"""
        # è·å–åŒ¹é…å‰åçš„æ–‡æœ¬
        start = match_obj.start()
        end = match_obj.end()
        
        # å‘å‰æŸ¥æ‰¾åˆ°è¡Œé¦–
        line_start = start
        while line_start > 0 and full_text[line_start - 1] not in '\n':
            line_start -= 1
        
        # å‘åæŸ¥æ‰¾åˆ°è¡Œå°¾
        line_end = end
        while line_end < len(full_text) and full_text[line_end] not in '\n':
            line_end += 1
        
        # æ£€æŸ¥è¡Œå†…å®¹ï¼šå»é™¤ç©ºç™½åæ˜¯å¦åªæœ‰è¿™ä¸ªå›¾ç‰‡æ ‡è®°
        line_content = full_text[line_start:line_end].strip()
        match_content = match_obj.group(0).strip()
        
        return line_content == match_content
    
    # å¤„ç†å¸¦IDçš„å›¾ç‰‡æ ‡è®°
    def repl_with_id(m: re.Match) -> str:
        img_id = m.group(1)
        basename = os.path.basename(img_id) if img_id else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(basename)
        else:
            return _make_inline_placeholder(basename)
    
    import os  # ç¡®ä¿å¯¼å…¥
    text = IMAGE_PATTERN_WITH_ID.sub(repl_with_id, text)
    
    # å¤„ç†æ— IDçš„å›¾ç‰‡æ ‡è®°
    def repl_no_id(m: re.Match) -> str:
        path = m.group(1).strip()
        basename = os.path.basename(path)
        label = basename if basename else "image"
        if is_standalone_line(m, text):
            return _make_tikz_placeholder(label)
        else:
            return _make_inline_placeholder(label)
    
    text = IMAGE_PATTERN_NO_ID.sub(repl_no_id, text)
    
    return text


def cleanup_guxuan_in_macros(text: str) -> str:
    """ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™
    
    é’ˆå¯¹ \\topics{...} å’Œ \\explain{...} ç­‰å®å‚æ•°å†…çš„"æ•…é€‰ï¼šX"è¿›è¡Œæ¸…ç†ã€‚
    
    Args:
        text: LaTeX æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text or 'æ•…é€‰' not in text:
        return text
    
    # å®šä¹‰è¦æ¸…ç†çš„å®åˆ—è¡¨
    macros = ['topics', 'explain', 'keywords', 'analysis']
    
    for macro_name in macros:
        # åŒ¹é… \macro{content}ï¼Œä½¿ç”¨é€’å½’åŒ¹é…åµŒå¥—å¤§æ‹¬å·
        # ç”±äºPython reä¸æ”¯æŒé€’å½’ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…+æ‰‹å·¥è§£æ
        pattern = rf'\\{macro_name}\{{'
        
        pos = 0
        result_parts = []
        
        while True:
            start_idx = text.find(pattern, pos)
            if start_idx == -1:
                result_parts.append(text[pos:])
                break
            
            # æ·»åŠ å‰é¢çš„æ–‡æœ¬
            result_parts.append(text[pos:start_idx])
            
            # æ‰‹å·¥è§£æåµŒå¥—å¤§æ‹¬å·
            brace_count = 0
            content_start = start_idx + len(pattern)
            i = content_start
            
            while i < len(text):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    if brace_count == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·
                        content = text[content_start:i]
                        
                        # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰"
                        # 1. æ¸…ç†è¡Œæœ«çš„"æ•…é€‰ï¼šX"ï¼ˆå«å„ç§æ ‡ç‚¹ç»„åˆå’Œå¯èƒ½çš„åç»­æ–‡æœ¬ï¼‰
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 2. æ¸…ç†å•ç‹¬ä¸€è¡Œçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*$', '', content, flags=re.MULTILINE)
                        # 3. æ¸…ç†æ¢è¡Œç¬¦åçš„"æ•…é€‰ï¼šX"
                        content = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*(?=\n|$)', '', content)
                        # 4. æ¸…ç†ä»»æ„ä½ç½®çš„"æ•…é€‰ï¼šX"ï¼ˆæ›´æ¿€è¿›ï¼‰
                        content = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n]*', '', content)
                        # 5. æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
                        content = re.sub(r'[,ï¼Œã€‚\.;ï¼›ã€]?\s*æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*[.ã€‚]?\s*', '', content)
                        
                        result_parts.append(rf'\{macro_name}{{{content}}}')
                        pos = i + 1
                        break
                    else:
                        brace_count -= 1
                elif text[i] == '\\' and i + 1 < len(text):
                    # è·³è¿‡è½¬ä¹‰å­—ç¬¦
                    i += 1
                i += 1
            else:
                # æ²¡æ‰¾åˆ°åŒ¹é…çš„å³å¤§æ‹¬å·ï¼Œä¿ç•™åŸæ–‡
                result_parts.append(text[start_idx:])
                break
        
        text = ''.join(result_parts)
    
    return text


def convert_markdown_table_to_latex(text: str) -> str:
    """å°† Markdown è¡¨æ ¼è½¬æ¢ä¸º LaTeX tabular"""
    table_pattern = r'(\|[^\n]+\|\n)+\|[-:\s|]+\|\n(\|[^\n]+\|\n)+'
    
    def convert_one_table(match):
        table_text = match.group(0)
        lines = [line.strip() for line in table_text.split('\n') if line.strip()]
        
        data_lines = [line for line in lines if not re.match(r'^\|[-:\s|]+\|$', line)]
        
        if not data_lines:
            return table_text
        
        rows = []
        for line in data_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            rows.append(cells)
        
        if not rows:
            return table_text
        
        ncols = len(rows[0])
        latex = "\\begin{center}\n"
        latex += f"\\begin{{tabular}}{{{'c' * ncols}}}\n"
        latex += "\\hline\n"
        
        header = rows[0]
        latex += " & ".join(escape_latex_special(cell, False) for cell in header)
        latex += " \\\\\n\\hline\n"
        
        for row in rows[1:]:
            latex += " & ".join(escape_latex_special(cell, False) for cell in row)
            latex += " \\\\\n"
        
        latex += "\\hline\n\\end{tabular}\n\\end{center}"
        return latex
    
    return re.sub(table_pattern, convert_one_table, text)


def clean_markdown(text: str) -> str:
    """æ¸…ç† markdown åƒåœ¾
    
    ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    """
    text = re.sub(
        r"<br><span class='markdown-page-line'>.*?</span><br><br>",
        "\n", text, flags=re.S,
    )
    text = re.sub(
        r"<span id='page\d+' class='markdown-page-text'>\[.*?\]</span>",
        "", text,
    )
    
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text)
    
    # ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    # ä¿æŠ¤å·²æœ‰çš„LaTeXå‘½ä»¤
    protected = []
    def save_latex_cmd(match):
        protected.append(match.group(0))
        return f"@@LATEXCMD{len(protected)-1}@@"
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', save_latex_cmd, text)
    
    # ç»Ÿä¸€æ‹¬å·ï¼ˆå…¨è§’â†’åŠè§’ï¼‰
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    # ç»Ÿä¸€å¼•å·ï¼ˆå¼¯å¼•å·â†’ç›´å¼•å·ï¼‰
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")
    
    # æ¢å¤LaTeXå‘½ä»¤
    for i, cmd in enumerate(protected):
        text = text.replace(f"@@LATEXCMD{i}@@", cmd)
    
    # æ¸…ç†ä»£ç å—æ ‡è®°
    text = re.sub(r'```[a-z]*\n?', '', text)
    text = re.sub(r'```', '', text)
    
    # è½¬æ¢è¡¨æ ¼
    if '|' in text and '---' in text:
        text = convert_markdown_table_to_latex(text)
    
    # å¤„ç†ä¸‹åˆ’çº¿
    text = text.replace(r'\_', '@@ESCAPED_UNDERSCORE@@')
    text = re.sub(r'(?<!\\)_(?![{_])', r'\\_', text)
    text = text.replace('@@ESCAPED_UNDERSCORE@@', r'\_')
    
    return text.strip()


# ==================== é¢˜ç›®è§£æå‡½æ•° ====================

def split_sections(text: str) -> List[Tuple[str, str]]:
    """æ‹†åˆ†ç« èŠ‚"""
    lines = text.splitlines()
    sections = []
    current_title = None
    current_lines = []

    for line in lines:
        stripped = line.strip()
        m = re.match(
            r"^#+\s*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)",
            stripped,
        )
        if m:
            if current_title is not None:
                sections.append((current_title, "\n".join(current_lines).strip()))
                current_lines = []
            current_title = m.group(1)
        else:
            if current_title is not None:
                current_lines.append(line)

    if current_title is not None and current_lines:
        sections.append((current_title, "\n".join(current_lines).strip()))

    return sections


def split_questions(section_body: str) -> List[str]:
    """æ‹†åˆ†é¢˜ç›®"""
    lines = section_body.splitlines()
    blocks = []
    current = []

    def flush():
        if current:
            blocks.append("\n".join(current).strip())
            current.clear()

    for line in lines:
        stripped = line.strip()
        if re.match(r"^\d+[\.ï¼ã€]\s*", stripped):
            flush()
            current.append(line)
        else:
            current.append(line)

    flush()
    return blocks


def extract_meta_and_images(block: str) -> Tuple[str, Dict, List]:
    r"""æå–å…ƒä¿¡æ¯ä¸å›¾ç‰‡ï¼ˆçŠ¶æ€æœºé‡æ„ï¼šé˜²æ­¢è·¨é¢˜ç´¯ç§¯ï¼‰

    ç›®æ ‡ï¼šé¿å…ä¸Šä¸€é¢˜çš„å¤šè¡Œã€è¯¦è§£ã€‘/ã€åˆ†æã€‘é”™è¯¯åå¹¶ä¸‹ä¸€é¢˜é¢˜å¹²ã€‚
    å…³é”®è¾¹ç•Œï¼š
      - æ–°çš„ meta å¼€å§‹ï¼ˆç­”æ¡ˆ/éš¾åº¦/çŸ¥è¯†ç‚¹/è¯¦è§£/åˆ†æï¼‰
      - é¢˜å·å¼€å§‹ï¼š^\s*>?\s*(?:\d+[\.ï¼ã€]\s+|ï¼ˆ\d+ï¼‰\s+|\d+\)\s+)
      - ç« èŠ‚æ ‡é¢˜ï¼š^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$
      - ç©ºè¡Œ + lookahead ä¸ºé¢˜å·æ—¶ï¼Œä½œä¸ºå®‰å…¨è¾¹ç•Œï¼ˆè‹¥ä¸Šä¸€è¡Œåƒç¯å¢ƒç»­è¡Œåˆ™è·³è¿‡è¯¥ç©ºè¡Œè¾¹ç•Œï¼‰
      - å¼•è¿°ç©ºè¡Œ ^>\s*$ å¿½ç•¥
    """
    # è§„èŒƒåŒ–å¹¶åˆ‡åˆ†è¡Œ
    lines = block.splitlines()

    # ç»“æœå®¹å™¨
    meta = {k: "" for k in META_PATTERNS}
    # å°† analysis ä¸ explain ç»Ÿä¸€ï¼šåç»­æŠŠ analysis å¹¶å…¥ explain
    meta_alias_map = {
        "analysis": "explain",
        "explain": "explain",
        "answer": "answer",
        "difficulty": "difficulty",
        "topics": "topics",
    }

    content_lines: List[str] = []
    images: List[Dict] = []

    # ç¼–è¯‘è¾¹ç•Œæ­£åˆ™ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šé¢˜å·æ ¼å¼å’Œç« èŠ‚æ ‡é¢˜ï¼‰
    question_start_perm = re.compile(r"^\s*>?\s*(?:\d{1,3}[\.ï¼ã€]\s+|ï¼ˆ\d{1,3}ï¼‰\s+|\d{1,3}\)\s+)")
    section_header = re.compile(r"^#{1,6}\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.].*)$")
    quote_blank = re.compile(r"^>\s*$")
    env_cont_hint = re.compile(r"(\\\\\s*$)|\\begin\{|\\left|\\right")

    # å°† META_PATTERNS ç¼–è¯‘ï¼Œå¹¶åˆå¹¶åŒä¹‰è¯â€œè€ƒç‚¹â€â†’topicsï¼Œâ€œåˆ†æ/è¯¦è§£â€â†’explain
    meta_starts = [
        ("answer", re.compile(r"^ã€\s*ç­”æ¡ˆ\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("difficulty", re.compile(r"^ã€\s*éš¾åº¦\s*ã€‘[:ï¼š]?\s*([\d.]+).*")),
        ("topics", re.compile(r"^ã€\s*(çŸ¥è¯†ç‚¹|è€ƒç‚¹)\s*ã€‘[:ï¼š]?\s*(.*)$")),
        ("explain", re.compile(r"^ã€\s*(è¯¦è§£|åˆ†æ)\s*ã€‘[:ï¼š]?\s*(.*)$")),
    ]

    # çŠ¶æ€
    state = "NORMAL"  # or "IN_META"
    current_meta_key: Optional[str] = None
    current_meta_lines: List[str] = []

    def flush_meta():
        nonlocal current_meta_key, current_meta_lines
        if current_meta_key is None:
            return
        # åˆå¹¶æ¸…ç†
        text = "\n".join(current_meta_lines)
        # å»æ‰å¯èƒ½æ®‹ç•™çš„æ ‡ç­¾å‰ç¼€
        text = re.sub(r"^ã€?(?:ç­”æ¡ˆ|éš¾åº¦|çŸ¥è¯†ç‚¹|è€ƒç‚¹|è¯¦è§£|åˆ†æ)ã€‘?[:ï¼š]?\s*", "", text)
        # å½’ä¸€åŒ–åˆ°åˆ«åé”®
        key = meta_alias_map.get(current_meta_key, current_meta_key)
        # å¯¹äº explain å­—æ®µï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼ˆä¸æŠ˜å ç©ºè¡Œï¼‰ï¼Œè®©åç»­ remove_par_breaks_in_explain å¤„ç†
        # å…¶ä»–å­—æ®µå‹ç¼©ç©ºè¡Œ
        if key != "explain":
            text = re.sub(r"\n\s*\n+", "\n", text)
        # åˆå¹¶ï¼šè‹¥å·²æœ‰ explainï¼Œåˆ™è¿½åŠ ä¸€è¡Œ
        if key == "explain" and meta.get("explain"):
            meta["explain"] = (meta["explain"] + "\n" + text.strip()).strip()
        else:
            meta[key] = text.strip()
        # é‡ç½®
        current_meta_key = None
        current_meta_lines = []

    def is_question_start(s: str) -> bool:
        return bool(question_start_perm.match(s))

    def is_section_header(s: str) -> bool:
        return bool(section_header.match(s))

    def image_match(s: str):
        # ä¼˜å…ˆåŒ¹é…å¸¦IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_WITH_ID.search(s)
        if m:
            return ('with_id', m)
        # ç„¶ååŒ¹é…æ— IDçš„å›¾ç‰‡
        m = IMAGE_PATTERN_NO_ID.search(s)
        if m:
            return ('no_id', m)
        # æœ€åå°è¯•æ—§ç‰ˆç®€å•æ ¼å¼
        m = IMAGE_PATTERN.search(s)
        if m:
            return ('simple', m)
        return None

    # æŸ¥æ‰¾ä¸Šä¸€æ¡éç©ºè¡Œï¼ˆç”¨äºç¯å¢ƒç»­è¡Œåˆ¤æ–­ï¼‰
    def find_prev_nonempty(idx: int) -> Optional[str]:
        j = idx - 1
        while j >= 0:
            if lines[j].strip():
                return lines[j]
            j -= 1
        return None

    # æŸ¥æ‰¾ä¸‹ä¸€æ¡éç©ºè¡Œï¼ˆç”¨äº blank+lookahead åˆ¤æ–­ï¼‰
    def find_next_nonempty(idx: int) -> Optional[str]:
        j = idx + 1
        while j < len(lines):
            if lines[j].strip():
                return lines[j]
            j += 1
        return None

    i = 0
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # ğŸ†• v1.6.2ï¼šå›¾ç‰‡è¡Œè¯†åˆ«å¢å¼º - åŒºåˆ†ç‹¬ç«‹å›¾ç‰‡å— vs å†…è”å…¬å¼å›¾ç‰‡
        # åªæœ‰å½“å›¾ç‰‡"ç‹¬å ä¸€è¡Œ"ä¸”æ˜¯å®Œæ•´åŒ¹é…æ—¶ï¼Œæ‰æå–ä¸ºå›¾ç‰‡å—
        # å†…è”å›¾ç‰‡ï¼ˆå¦‚ "å·²çŸ¥é›†åˆ![](image2.wmf)ï¼Œåˆ™..."ï¼‰ä¿ç•™åœ¨æ–‡æœ¬ä¸­
        img_result = image_match(stripped)
        if img_result:
            img_type, m_img = img_result
            # æ£€æŸ¥æ˜¯å¦ä¸ºç‹¬ç«‹å›¾ç‰‡è¡Œï¼šæ•´è¡Œåªæœ‰ä¸€ä¸ªå›¾ç‰‡æ ‡è®°
            is_standalone = (m_img.group(0).strip() == stripped)
            
            if is_standalone:
                # ç‹¬ç«‹å›¾ç‰‡å—ï¼šæå–åˆ°imagesåˆ—è¡¨
                if img_type == 'with_id':
                    # ![@@@id](path){...}
                    img_id = m_img.group(1)
                    path = m_img.group(2).strip()
                    images.append({"path": path, "width": 50, "id": img_id})
                elif img_type == 'no_id':
                    # ![](path){...}
                    path = m_img.group(1).strip()
                    images.append({"path": path, "width": 50})
                else:
                    # ç®€å•æ ¼å¼: ![](images/...)
                    path = m_img.group(1)
                    width = int(m_img.group(2)) if m_img.group(2) else 50
                    images.append({"path": path, "width": width})
                i += 1
                continue
            # else: å†…è”å›¾ç‰‡ï¼Œä¿ç•™åœ¨æ–‡æœ¬æµä¸­ï¼Œä¸åšç‰¹æ®Šå¤„ç†ï¼ˆfallthroughï¼‰

        # å¼•è¿°ç©ºè¡Œï¼šä¸¢å¼ƒ
        if quote_blank.match(stripped):
            i += 1
            continue

        if state == "NORMAL":
            # æ–°çš„ meta å¼€å§‹ï¼Ÿ
            started = False
            for key, pat in meta_starts:
                m = pat.match(stripped)
                if m:
                    state = "IN_META"
                    current_meta_key = key
                    seed = m.group(m.lastindex or 1) if m.groups() else ""
                    current_meta_lines = [seed.strip()] if seed.strip() else []
                    started = True
                    break
            if started:
                i += 1
                continue

            # æ™®é€šå†…å®¹
            content_lines.append(line)
            i += 1
            continue

        # state == IN_META
        # 1) æ–° meta å¼€å§‹ -> åˆ·æ–°å¹¶åˆ‡æ¢
        started = False
        for key, pat in meta_starts:
            m = pat.match(stripped)
            if m:
                flush_meta()
                state = "IN_META"
                current_meta_key = key
                seed = m.group(m.lastindex or 1) if m.groups() else ""
                current_meta_lines = [seed.strip()] if seed.strip() else []
                started = True
                break
        if started:
            i += 1
            continue

        # 2) ç¡®è®¤é¢˜å·æˆ–ç« èŠ‚è¾¹ç•Œ -> ç»“æŸ metaï¼Œä¿ç•™è¯¥è¡Œç»™é¢˜å¹²
        if is_question_start(stripped) or is_section_header(stripped):
            flush_meta()
            state = "NORMAL"
            content_lines.append(line)
            i += 1
            continue

        # 3) ç©ºè¡Œ + lookahead ä¸ºé¢˜å· -> å®‰å…¨åœ°ç»“æŸ meta
        if stripped == "":
            next_ne = find_next_nonempty(i)
            if next_ne and is_question_start(next_ne.strip()):
                prev_ne = find_prev_nonempty(i)
                # è‹¥ä¸Šä¸€éç©ºè¡Œçœ‹èµ·æ¥æ˜¯ç¯å¢ƒç»­è¡Œï¼Œåˆ™ä¸è¦åœ¨æ­¤ç©ºè¡Œåˆ‡æ–­
                if prev_ne and env_cont_hint.search(prev_ne):
                    # ç»§ç»­æŠŠç©ºè¡Œä¹Ÿå¹¶å…¥ metaï¼ˆä¿æŒåŸæ ·ï¼‰
                    current_meta_lines.append(line)
                    i += 1
                    continue
                # å¦åˆ™åˆ‡æ–­ metaï¼ˆä¸æ¶ˆè´¹ç©ºè¡Œï¼‰
                flush_meta()
                state = "NORMAL"
                i += 1  # è·³è¿‡è¯¥ç©ºè¡Œï¼Œä¸‹ä¸€è½®çœ‹åˆ°é¢˜å·è¡Œä¼šè¿›å…¥ NORMAL æµç¨‹
                continue

        # 4) ç»§ç»­ç´¯ç§¯ meta å†…å®¹
        current_meta_lines.append(line)
        i += 1

    # å¾ªç¯ç»“æŸï¼Œè‹¥è¿˜åœ¨ meta çŠ¶æ€åˆ™åˆ·æ–°
    if state == "IN_META":
        flush_meta()

    content = "\n".join(content_lines).strip()
    return content, meta, images


def parse_question_structure(content: str) -> Dict:
    """æ™ºèƒ½è¯†åˆ«é¢˜ç›®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    è§£æé¢˜å¹²ã€é€‰é¡¹ã€è§£æä¸‰éƒ¨åˆ†ï¼Œé¿å…å°†è§£ææ–‡æœ¬æ··å…¥é€‰é¡¹
    """
    lines = content.splitlines()
    
    structure = {
        'stem_lines': [],
        'choices': [],
        'analysis_lines': [],
        'in_choice': False,
        'in_analysis': False,
        'current_choice': '',
    }
    
    choice_pattern = re.compile(r'^([A-D])[\.ï¼ã€]\s*(.*)$')
    
    for line in lines:
        stripped = line.strip()
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦è¿›å…¥è§£æéƒ¨åˆ†ï¼ˆé¿å…è§£ææ–‡æœ¬æ··å…¥é€‰é¡¹ï¼‰
        # ä»…å½“è¡Œä»¥è§£æèµ·å§‹è¯å¼€å¤´æˆ–æ˜¾å¼ä»¥ã€è¯¦è§£ã€‘ã€åˆ†æã€‘ã€ç­”æ¡ˆã€‘ç­‰æ ‡ç­¾å¼€å¤´æ—¶ï¼Œæ‰åˆ¤å®šä¸ºè§£ææ®µè½ã€‚
        if any(stripped.startswith(marker) for marker in ANALYSIS_START_MARKERS) \
           or re.match(r'^(?:ã€?è¯¦è§£ã€‘|ã€?åˆ†æã€‘|ã€?ç­”æ¡ˆã€‘)[:ï¼š]?', stripped):
            # ä¿å­˜å½“å‰ç´¯ç§¯çš„é€‰é¡¹
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
                structure['current_choice'] = ''
            structure['in_choice'] = False
            structure['in_analysis'] = True
            structure['analysis_lines'].append(stripped)
            continue
        
        # åŒ¹é…é€‰é¡¹æ ‡è®° (A. B. C. D.)
        m = choice_pattern.match(stripped)
        if m:
            # ä¿å­˜ä¸Šä¸€ä¸ªé€‰é¡¹
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'].strip())
            
            structure['current_choice'] = m.group(2)
            structure['in_choice'] = True
            structure['in_analysis'] = False
            continue
        
        # æ ¹æ®å½“å‰çŠ¶æ€åˆ†é…è¡Œ
        if structure['in_analysis']:
            structure['analysis_lines'].append(line)
        elif structure['in_choice']:
            # é€‰é¡¹ç»­è¡Œï¼ˆå¤šè¡Œé€‰é¡¹å†…å®¹ï¼‰
            structure['current_choice'] += ' ' + stripped
        else:
            # é¢˜å¹²éƒ¨åˆ†
            structure['stem_lines'].append(line)
    
    # ä¿å­˜æœ«å°¾ç´¯ç§¯çš„é€‰é¡¹
    if structure['current_choice']:
        structure['choices'].append(structure['current_choice'].strip())
    
    return structure


def expand_inline_choices(content: str) -> str:
    """å±•å¼€å•è¡Œ/å¤šè¡Œå¼•è¿°é€‰é¡¹å¹¶å»é™¤'>'å‰ç¼€
    - å•è¡Œï¼š> A... B... C... D... â†’ å¤šè¡Œç‹¬ç«‹é€‰é¡¹
    - å¤šè¡Œï¼š> A... B... / > C... D... â†’ åˆå¹¶åå±•å¼€ä¸ºç‹¬ç«‹é€‰é¡¹
    - ç©ºè¡Œï¼š> (ç©º) â†’ è·³è¿‡
    """
    lines = []
    accumulated_choice_text = ""
    
    for line in content.splitlines():
        stripped = line.strip()
        
        # å¤„ç†ä»¥'>'å¼€å¤´çš„è¡Œï¼ˆå¼•è¿°å—ï¼‰
        if stripped.startswith('>'):
            choice_text = stripped[1:].strip()
            
            # è·³è¿‡ç©ºçš„å¼•è¿°è¡Œ
            if not choice_text:
                continue
            
            # å¦‚æœè¿™ä¸€è¡Œæœ‰é€‰é¡¹æ ‡è®°ï¼Œç´¯ç§¯åˆ°ç¼“å†²åŒº
            if re.search(r'[A-D][ï¼\.\ã€]', choice_text):
                accumulated_choice_text += " " + choice_text if accumulated_choice_text else choice_text
                continue
            
            # éé€‰é¡¹å¼•è¿°ï¼ˆå¦‚å›¾ç‰‡è¯´æ˜ç­‰ï¼‰ï¼Œä¿ç•™åŸæ ·
            lines.append(line)
        else:
            # éå¼•è¿°è¡Œï¼šå¦‚æœæœ‰ç´¯ç§¯çš„é€‰é¡¹æ–‡æœ¬ï¼Œå…ˆå¤„ç†
            if accumulated_choice_text:
                # æ£€æŸ¥ç´¯ç§¯æ–‡æœ¬ä¸­æœ‰å¤šå°‘ä¸ªé€‰é¡¹æ ‡è®°
                choice_markers = re.findall(r'[A-D][ï¼\.\ã€]', accumulated_choice_text)
                if len(choice_markers) >= 2:
                    # åˆ†å‰²ä¸ºç‹¬ç«‹é€‰é¡¹
                    parts = re.split(r'(?=[A-D][ï¼\.\ã€])', accumulated_choice_text)
                    for part in parts:
                        part = part.strip()
                        if part and re.match(r'^[A-D][ï¼\.\ã€]', part):
                            lines.append(part)
                elif len(choice_markers) == 1:
                    # å•ä¸ªé€‰é¡¹ï¼Œç›´æ¥æ·»åŠ 
                    lines.append(accumulated_choice_text.strip())
                
                accumulated_choice_text = ""
            
            # æ·»åŠ å½“å‰è¡Œ
            lines.append(line)
    
    # å¤„ç†æœ«å°¾æ®‹ç•™çš„ç´¯ç§¯æ–‡æœ¬
    if accumulated_choice_text:
        choice_markers = re.findall(r'[A-D][ï¼\.\ã€]', accumulated_choice_text)
        if len(choice_markers) >= 2:
            parts = re.split(r'(?=[A-D][ï¼\.\ã€])', accumulated_choice_text)
            for part in parts:
                part = part.strip()
                if part and re.match(r'^[A-D][ï¼\.\ã€]', part):
                    lines.append(part)
        elif len(choice_markers) == 1:
            lines.append(accumulated_choice_text.strip())
    
    return '\n'.join(lines)


def convert_choices(content: str) -> Tuple[str, List[str], str]:
    """æ‹†åˆ†é¢˜å¹²ã€é€‰é¡¹ã€è§£æï¼ˆå¢å¼ºç‰ˆï¼‰
    
    ğŸ†• v1.4 æ”¹è¿›ï¼šå…ˆå±•å¼€å•è¡Œé€‰é¡¹å†è§£æ
    """
    # ğŸ†• å…ˆå±•å¼€å¯èƒ½çš„å•è¡Œé€‰é¡¹
    content = expand_inline_choices(content)
    
    structure = parse_question_structure(content)
    
    stem = '\n'.join(structure['stem_lines']).strip()
    stem = re.sub(r"^\s*\d+[\.ï¼ã€]\s*", "", stem)
    
    # æå–çš„è§£æå†…å®¹
    analysis = '\n'.join(structure['analysis_lines']).strip()
    
    return stem, structure['choices'], analysis


def handle_subquestions(content: str) -> str:
    """å¤„ç†è§£ç­”é¢˜çš„å°é¢˜ç¼–å·"""
    if not re.search(r'\(\d+\)', content):
        return content
    
    subquestions = re.findall(r'\((\d+)\)(.*?)(?=\(\d+\)|$)', content, re.DOTALL)
    
    if len(subquestions) < 2:
        return content
    
    result_lines = []
    for num, content_text in subquestions:
        result_lines.append(f"\\item {content_text.strip()}")
    
    return '\n'.join(result_lines)


def process_text_for_latex(text: str, is_math_heavy: bool = False) -> str:
    """ç»Ÿä¸€å¤„ç†æ–‡æœ¬
    
    ğŸ†• v1.5 æ”¹è¿›ï¼šæ·»åŠ åŒé‡åŒ…è£¹ä¿®æ­£
    ğŸ†• v1.3 æ”¹è¿›ï¼šæ›´å¼ºçš„"æ•…é€‰"æ¸…ç†è§„åˆ™
    ğŸ†• v1.5.1ï¼šä¿®æ­£æ•°å­¦ç¯å¢ƒå†…çš„ OCR é”™è¯¯ï¼ˆdelimiter mismatchesï¼‰
    """
    if not text:
        return text
    
    # ğŸ†• v1.3 æ”¹è¿›ï¼šæ›´å¼ºçš„"æ•…é€‰"æ¸…ç†è§„åˆ™
    # æ¸…ç†ç»“å°¾çš„"æ•…é€‰"ï¼ˆæ”¯æŒå¤šç§æ ‡ç‚¹ï¼‰
    text = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    # æ¸…ç†å•ç‹¬ä¸€è¡Œçš„"æ•…é€‰"
    text = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    # æ¸…ç†å¼€å¤´çš„"æ•…é€‰"ï¼ˆç½•è§ä½†å¯èƒ½ï¼‰
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', text)
    # æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
    text = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', text)
    # é¢å¤–ï¼šåˆ é™¤"å•ç‹¬ä¸€è¡Œ"çš„"æ•…é€‰ï¼šX"
    text = re.sub(
        r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$',
        '',
        text,
        flags=re.MULTILINE,
    )
    # è¿›ä¸€æ­¥ï¼šæ¸…ç†å¥æœ«çš„â€œï¼Œæ•…é€‰ï¼šXâ€ä¹‹ç±»å°¾å·´ï¼ˆä¿ç•™å‰é¢çš„è§£æå†…å®¹ï¼‰
    text = re.sub(
        r'[ï¼Œ,]?\s*æ•…é€‰[:ï¼š]\s*[ABCD]+[ã€‚ï¼.]*\s*$',
        '',
        text,
        flags=re.MULTILINE,
    )
    # æ¸…ç†"ã€è¯¦è§£ã€‘"æ ‡è®°
    text = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', text)
    
    # ğŸ†• v1.5.1ï¼šé¢„å¤„ç† - ä¿®å¤ OCR å¸¸è§çš„ \right.\\) æ¨¡å¼
    # è¿™ä¸ªé—®é¢˜å‡ºç°åœ¨ array ç¯å¢ƒç»“å°¾ï¼Œéœ€è¦åœ¨ smart_inline_math ä¹‹å‰ä¿®å¤
    text = re.sub(r'\\\\right\.\s*\\\\\\\)', r'\\\\right.', text)
    text = re.sub(r'\\\\right\.\\\\\+\)', r'\\\\right.', text)

    # å°†æ–‡æœ¬ä¸­çš„ Unicode âˆµ/âˆ´ æ›¿æ¢ä¸ºå¯ç¼–è¯‘çš„æ•°å­¦ç¬¦å·ï¼ˆåŒ…è£¹ä¸ºè¡Œå†…æ•°å­¦ï¼‰
    # æ•°å­¦ç¯å¢ƒå†…çš„æ›¿æ¢ç”± sanitize_math å†æ¬¡ä¿è¯
    if 'âˆµ' in text or 'âˆ´' in text:
        text = text.replace('âˆµ', '$\\because$').replace('âˆ´', '$\\therefore$')
    
    if not is_math_heavy:
        text = escape_latex_special(text, in_math_mode=False)
    
    text = smart_inline_math(text)
    # ğŸ†• v1.5 æ–°å¢ï¼šä¿®æ­£å¯èƒ½çš„åŒé‡åŒ…è£¹
    text = fix_double_wrapped_math(text)
    text = wrap_math_variables(text)
    
    # ğŸ†• v1.5.1ï¼šä¿®æ­£æ•°å­¦ç¯å¢ƒå†…çš„ OCR é”™è¯¯ï¼ˆdelimiter mismatchesï¼‰
    if is_math_heavy:
        text = sanitize_math(text)
    
    return text


def build_question_tex(stem: str, options: List, meta: Dict, images: List, 
                       section_type: str) -> str:
    """ç”Ÿæˆ question ç¯å¢ƒ"""
    stem = process_text_for_latex(stem, is_math_heavy=True)
    
    if section_type == "è§£ç­”é¢˜" and re.search(r'\(\d+\)', stem):
        stem = handle_subquestions(stem)
    
    explain_raw = meta.get("explain", "").strip()
    if explain_raw:
        explain_raw = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', explain_raw)
        explain_raw = process_text_for_latex(explain_raw, is_math_heavy=True)
    
    topics_raw = meta.get("topics", "").strip()
    if topics_raw:
        topics_raw = topics_raw.replace("ã€", "ï¼›")
        topics_raw = escape_latex_special(topics_raw, in_math_mode=False)

    lines = []
    lines.append(r"\begin{question}")
    
    if stem:
        lines.append(stem)

    if options:
        lines.append(r"\begin{choices}")
        for opt in options:
            opt_processed = process_text_for_latex(opt, is_math_heavy=True)
            lines.append(f"  \\item {opt_processed}")
        lines.append(r"\end{choices}")

    for img in images:
        lines.append("")
        lines.append(r"\begin{center}")
        lines.append(f"% IMAGE_TODO: {img['path']} (width={img['width']}%)")
        lines.append(r"\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]")
        lines.append(r"  % TODO: AI Agent å°†ä½¿ç”¨ view å·¥å…·æŸ¥çœ‹æ­¤å›¾ç‰‡å¹¶ç”Ÿæˆ TikZ ä»£ç ")
        lines.append(f"  % view {img['path']}")
        lines.append(r"\end{tikzpicture}")
        lines.append(r"\end{center}")

    if topics_raw:
        lines.append(f"\\topics{{{topics_raw}}}")
    if meta.get("difficulty"):
        lines.append(f"\\difficulty{{{meta['difficulty']}}}")
    if meta.get("answer"):
        # ä½¿ç”¨ä¸é¢˜å¹²/è§£æä¸€è‡´çš„å¤„ç†ï¼Œä»¥è§„èŒƒæ•°å­¦æ ¼å¼ï¼Œé¿å… $$...$$ æ®‹ç•™
        ans = process_text_for_latex(meta["answer"], is_math_heavy=True)
        lines.append(f"\\answer{{{ans}}}")
    if explain_raw:
        lines.append(f"\\explain{{{explain_raw}}}")

    lines.append(r"\end{question}")
    return "\n".join(lines)


def convert_md_to_examx(md_text: str, title: str) -> str:
    """ä¸»è½¬æ¢å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    md_text = clean_markdown(md_text)
    sections = split_sections(md_text)

    out_lines = []
    out_lines.append(f"\\examxtitle{{{title}}}")

    for raw_title, body in sections:
        sec_label = SECTION_MAP.get(raw_title, raw_title)
        out_lines.append("")
        out_lines.append(f"\\section{{{sec_label}}}")

        for block in split_questions(body):
            if not block.strip():
                continue
            
            content, meta, images = extract_meta_and_images(block)
            
            # ä½¿ç”¨å¢å¼ºçš„è½¬æ¢å‡½æ•°ï¼ˆè¿”å›3ä¸ªå€¼ï¼‰
            stem, options, extracted_analysis = convert_choices(content)
            
            # åˆå¹¶æå–çš„è§£æå’Œå…ƒä¿¡æ¯ä¸­çš„è§£æ
            if extracted_analysis and not meta.get('explain'):
                meta['explain'] = extracted_analysis
            elif extracted_analysis:
                meta['explain'] = meta['explain'] + '\n' + extracted_analysis
            
            q_tex = build_question_tex(stem, options, meta, images, sec_label)
            out_lines.append("")
            out_lines.append(q_tex)

    out_lines.append("")
    
    # æœ€ç»ˆå¤„ç†ï¼šæ¸…ç†ç©ºè¡Œå’Œåˆ†å‰²è¶…é•¿è¡Œ
    result = "\n".join(out_lines)
    result = remove_blank_lines_in_macro_args(result)
    result = clean_question_environments(result)
    result = split_long_lines_in_explain(result, max_length=800)
    # è¿›ä¸€æ­¥ä¸¥æ ¼ç§»é™¤ explain{} ä¸­çš„ç©ºæ®µè½ï¼Œé¿å…æ®µè½ä¸­æ–­å¯¼è‡´çš„å®å‚æ•°æŠ¥é”™
    result = remove_par_breaks_in_explain(result)

    # æœ€ç»ˆå…œåº•ï¼šè§„èŒƒ/ç§»é™¤æ®‹ç•™çš„ $$ æ˜¾ç¤ºæ•°å­¦æ ‡è®°
    # 1) å°†æˆå¯¹ $$...$$ ç»Ÿä¸€ä¸ºè¡Œå†… \\(...\\)
    result = re.sub(r'\$\$\s*(.+?)\s*\$\$', r'\\(\1\\)', result, flags=re.DOTALL)
    # 2) æ¸…ç†ä»»ä½•æ®‹ç•™çš„å­¤ç«‹ $$ï¼ˆé¿å…ç¼–è¯‘é”™è¯¯ï¼‰
    result = result.replace('$$', '')
    
    # ğŸ†• åå¤‡å ä½ç¬¦è½¬æ¢ï¼šæ¸…ç†ä»»ä½•æ®‹ç•™çš„ Markdown å›¾ç‰‡æ ‡è®°
    result = cleanup_remaining_image_markers(result)
    
    # ğŸ†• v1.6ï¼šæ¸…ç†å®å‚æ•°å†…çš„"æ•…é€‰"æ®‹ç•™ï¼ˆåˆ†ä¸¤æ­¥ï¼‰
    result = cleanup_guxuan_in_macros(result)
    
    # ğŸ†• v1.6.1ï¼šå…¨å±€æ¸…ç†ä»»ä½•æ®‹ç•™çš„"æ•…é€‰"ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
    # æ¸…ç†å„ç§å½¢å¼çš„"æ•…é€‰ï¼šX"ï¼Œæ— è®ºåœ¨ä»€ä¹ˆä½ç½®
    result = re.sub(r'æ•…é€‰[:ï¼š][ABCD]+\.?[^\n}]*', '', result)
    # æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
    result = re.sub(r'æ•…ç­”æ¡ˆä¸º[:ï¼š]?[ABCD]*\.?', '', result)
    
    return result


# ==================== ğŸ†• v1.3 æ–°å¢ï¼šè‡ªåŠ¨éªŒè¯å‡½æ•° ====================

def validate_latex_output(tex_content: str) -> List[str]:
    """
    ğŸ†• v1.3 æ–°å¢ï¼šéªŒè¯LaTeXè¾“å‡ºï¼Œè¿”å›è­¦å‘Šåˆ—è¡¨
    
    Args:
        tex_content: ç”Ÿæˆçš„LaTeXå†…å®¹
    
    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    warnings = []
    
    # æ£€æŸ¥1ï¼šæ®‹ç•™çš„ $ ç¬¦å·
    dollar_matches = re.findall(r'(?<!\\)\$[^\$]+\$', tex_content)
    if dollar_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(dollar_matches)} å¤„æ®‹ç•™çš„ $ æ ¼å¼")
        for i, match in enumerate(dollar_matches[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            warnings.append(f"     ç¤ºä¾‹{i}: {match}")
    
    # æ£€æŸ¥2ï¼šæ®‹ç•™çš„"æ•…é€‰"
    guxuan_matches = re.findall(r'æ•…é€‰[:ï¼š][ABCD]+', tex_content)
    if guxuan_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(guxuan_matches)} å¤„æ®‹ç•™çš„'æ•…é€‰'")
    
    # æ£€æŸ¥3ï¼šä¸­æ–‡æ‹¬å·
    chinese_paren = re.findall(r'[ï¼ˆï¼‰]', tex_content)
    if chinese_paren:
        warnings.append(f"âš ï¸  å‘ç° {len(chinese_paren)} å¤„ä¸­æ–‡æ‹¬å·")
    
    # æ£€æŸ¥4ï¼šç¯å¢ƒé—­åˆ
    begin_count = tex_content.count(r'\begin{question}')
    end_count = tex_content.count(r'\end{question}')
    if begin_count != end_count:
        warnings.append(f"âŒ question ç¯å¢ƒä¸åŒ¹é…: {begin_count} ä¸ª begin, {end_count} ä¸ª end")
    
    begin_choices = tex_content.count(r'\begin{choices}')
    end_choices = tex_content.count(r'\end{choices}')
    if begin_choices != end_choices:
        warnings.append(f"âŒ choices ç¯å¢ƒä¸åŒ¹é…: {begin_choices} ä¸ª begin, {end_choices} ä¸ª end")
    
    # æ£€æŸ¥5ï¼šç©ºè¡Œåœ¨å®å‚æ•°ä¸­
    problematic_macros = []
    for macro in ['explain', 'topics', 'answer']:
        pattern = rf'\\{macro}\{{[^}}]*\n\s*\n[^}}]*\}}'
        if re.search(pattern, tex_content):
            problematic_macros.append(macro)
    if problematic_macros:
        warnings.append(f"âš ï¸  ä»¥ä¸‹å®å‚æ•°ä¸­å¯èƒ½æœ‰ç©ºè¡Œ: {', '.join(problematic_macros)}")
    
    return warnings


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description=f"OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ†• v1.4 æ–°å¢åŠŸèƒ½ï¼š
  - ä¿®å¤æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ï¼ˆ$$\\(...\\)$$ â†’ \\(...\\)ï¼‰
  - è‡ªåŠ¨å±•å¼€å•è¡Œé€‰é¡¹ï¼ˆ> A... B... â†’ å¤šè¡Œï¼‰
  - æ­£ç¡®å¤„ç†æ˜¾ç¤ºå…¬å¼ï¼ˆ$$ â†’ \\[...\\]ï¼‰

âœ… v1.3 æ”¹è¿›å›é¡¾ï¼š
  - ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢
  - æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™
  - ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆæ‹¬å·ã€å¼•å·ï¼‰
  - æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

âœ… v1.2 æ”¹è¿›å›é¡¾ï¼š
  - åŠ å¼ºç©ºè¡Œæ¸…ç†ï¼ˆè§£å†³80%çš„Runaway argumenté”™è¯¯ï¼‰
  - è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²ï¼ˆè§£å†³ç¼–è¯‘æ…¢é—®é¢˜ï¼‰
  - å¢å¼ºæ•°å­¦å˜é‡æ£€æµ‹ï¼ˆå‡å°‘Missing $é”™è¯¯ï¼‰
  - å¢å¼ºé€‰é¡¹è§£æï¼ˆå¤„ç†åµŒå…¥çš„è§£æå†…å®¹ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
  python3 ocr_to_examx.py "æµ™æ±Ÿçœé‡‘ååæ ¡/" output/
        """
    )
    
    parser.add_argument("input", help="è¾“å…¥è·¯å¾„ï¼ˆ.md æ–‡ä»¶æˆ– OCR æ–‡ä»¶å¤¹ï¼‰")
    parser.add_argument("output", help="è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ– .tex æ–‡ä»¶ï¼‰")
    parser.add_argument("--title", help="è¯•å·æ ‡é¢˜", default=None)
    parser.add_argument("--version", action="version", version=f"%(prog)s {VERSION}")
    
    args = parser.parse_args()
    
    try:
        print(f"ğŸ” OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}")
        print("â”" * 60)
        md_file, images_dir = find_markdown_and_images(args.input)
        
        print(f"ğŸ“„ Markdown: {md_file.name}")
        if images_dir:
            img_count = len(list(images_dir.glob('*')))
            print(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {images_dir} ({img_count} ä¸ªæ–‡ä»¶)")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•")
        
        output_path = Path(args.output)
        if output_path.suffix == '.tex':
            output_tex = output_path
            output_dir = output_path.parent
        else:
            output_dir = output_path
            output_tex = output_dir / f"{md_file.stem.replace('_local', '_raw')}.tex"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if images_dir:
            img_count = copy_images_to_output(images_dir, output_dir)
            print(f"âœ… å·²å¤åˆ¶ {img_count} ä¸ªå›¾ç‰‡åˆ° {output_dir}/images/")
        
        title = args.title
        if title is None:
            input_path = Path(args.input)
            if input_path.is_dir():
                title = input_path.name
            else:
                title = md_file.stem.replace('_local', '')
        
        print(f"\nğŸ“– æ­£åœ¨è½¬æ¢...")
        print(f"ğŸ“ æ ‡é¢˜: {title}")
        
        md_text = md_file.read_text(encoding='utf-8')
        tex_text = convert_md_to_examx(md_text, title)
        
        # ğŸ†• v1.3ï¼šéªŒè¯è¾“å‡º
        warnings = validate_latex_output(tex_text)
        
        output_tex.write_text(tex_text, encoding='utf-8')
        
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
        print("â”" * 60)
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶: {output_tex}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(tex_text):,} å­—èŠ‚")
        
        question_count = tex_text.count(r'\begin{question}')
        image_count = tex_text.count('IMAGE_TODO')
        print(f"ğŸ“‹ é¢˜ç›®æ•°é‡: {question_count}")
        if image_count > 0:
            print(f"ğŸ–¼ï¸  å›¾ç‰‡å ä½: {image_count}")
        
        print(f"\nğŸ†• v1.4 æ”¹è¿›å·²åº”ç”¨:")
        print(f"  âœ… æ•°å­¦å…¬å¼åŒé‡åŒ…è£¹ä¿®å¤")
        print(f"  âœ… å•è¡Œé€‰é¡¹è‡ªåŠ¨å±•å¼€")
        print(f"  âœ… æ˜¾ç¤ºå…¬å¼æ­£ç¡®å¤„ç†")
        
        print(f"\nâœ… v1.3 æ”¹è¿›ï¼ˆå·²ä¿ç•™ï¼‰:")
        print(f"  âœ… $ æ ¼å¼å…œåº•è½¬æ¢")
        print(f"  âœ… å¢å¼ºçš„'æ•…é€‰'æ¸…ç†")
        print(f"  âœ… ä¸­è‹±æ–‡æ ‡ç‚¹ç»Ÿä¸€")
        print(f"  âœ… è‡ªåŠ¨éªŒè¯åŠŸèƒ½")
        
        print(f"\nâœ… v1.2 æ”¹è¿›ï¼ˆå·²ä¿ç•™ï¼‰:")
        print(f"  âœ… ç©ºè¡Œæ¸…ç†å¢å¼º")
        print(f"  âœ… è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²")
        print(f"  âœ… æ•°å­¦å˜é‡æ™ºèƒ½æ£€æµ‹")
        print(f"  âœ… é€‰é¡¹è§£æå¢å¼º")
        
        # ğŸ†• v1.3ï¼šæ˜¾ç¤ºéªŒè¯ç»“æœ
        if warnings:
            print(f"\nâš ï¸  éªŒè¯å‘ç° {len(warnings)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for warning in warnings:
                print(f"  {warning}")
            print("\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ AI Agent è¿›è¡Œäººå·¥æ£€æŸ¥")
        else:
            print(f"\nâœ… éªŒè¯é€šè¿‡ï¼šæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. AI Agent è¯»å–æ­¤æ–‡ä»¶è¿›è¡Œç²¾ä¿®")
        print("  2. AI Agent æŸ¥çœ‹ images/ ä¸­çš„å›¾ç‰‡")
        print("  3. AI Agent ç”Ÿæˆ TikZ ä»£ç ")
        print("  4. è¾“å‡ºæœ€ç»ˆçš„ exam_final.tex")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

