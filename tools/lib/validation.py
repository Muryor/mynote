#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
validation.py - éªŒè¯æ¨¡å— - LaTeXè¯­æ³•æ£€æŸ¥ã€é”™è¯¯æ£€æµ‹

ä» ocr_to_examx.py æå–çš„å…±äº«å·¥å…·å‡½æ•°ï¼Œä¾› exam å’Œ handout è½¬æ¢å™¨ä½¿ç”¨ã€‚

ç”Ÿæˆæ—¶é—´: è‡ªåŠ¨æå–
æºæ–‡ä»¶: tools/core/ocr_to_examx.py
"""

from typing import List, Dict, Tuple, Optional
import re

# ============================================================
# éªŒè¯æ¨¡å— - LaTeXè¯­æ³•æ£€æŸ¥ã€é”™è¯¯æ£€æµ‹
# ============================================================

def validate_math_integrity(tex: str) -> List[str]:
    r"""åˆ†ææœ€ç»ˆ TeX æ•°å­¦å®Œæ•´æ€§é—®é¢˜å¹¶è¿”å›è­¦å‘Šåˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰

    æ£€æŸ¥é¡¹ï¼š
    - è¡Œå†…æ•°å­¦å®šç•Œç¬¦æ•°é‡å·®å¼‚ï¼ˆopens vs closesï¼‰- ğŸ†• v1.8.7ï¼šå¿½ç•¥æ³¨é‡Šä¸­çš„å®šç•Œç¬¦
    - è£¸éœ²ç¾å…ƒç¬¦å·
    - åŒé‡åŒ…è£¹æ®‹ç•™
    - å³è¾¹ç•Œç•¸å½¢ï¼ˆ\right. $$ ç­‰ï¼‰
    - ç©ºæ•°å­¦å—
    - ğŸ†• æˆªæ–­/æœªé—­åˆçš„æ•°å­¦ç‰‡æ®µï¼ˆæ”¶é›†å‰è‹¥å¹²æ ·æœ¬ï¼‰
      å…¸å‹æ¥æºï¼šå›¾ç‰‡å ä½ç¬¦æˆ– explain åˆå¹¶æ—¶è·¨è¡Œè¢«æˆªæ–­ï¼Œå¯¼è‡´ç¼ºå¤± \)
    - ğŸ†• v1.8.7ï¼šæ£€æµ‹ \) åœ¨ \( å‰é¢çš„åå‘æ¨¡å¼
    """
    issues: List[str] = []
    tex_no_comments_lines: List[str] = []
    for raw_line in tex.splitlines():
        tex_no_comments_lines.append(raw_line.split('%', 1)[0])
    tex_no_comments = "\n".join(tex_no_comments_lines)

    # ğŸ†• v1.8.7ï¼šç»Ÿè®¡æ—¶å¿½ç•¥æ³¨é‡Šä¸­çš„å®šç•Œç¬¦
    opens = 0
    closes = 0
    left_total = 0
    right_total = 0
    left_right_samples: List[str] = []
    reversed_pairs: List[Tuple[int, str]] = []  # (line_num, line_content)

    for lineno, code_part in enumerate(tex_no_comments_lines, start=1):
        line_opens = code_part.count('\\(')
        line_closes = code_part.count('\\)')
        line_left = code_part.count('\\left')
        line_right = code_part.count('\\right')
        opens += line_opens
        closes += line_closes
        left_total += line_left
        right_total += line_right

        if line_left != line_right and (line_left or line_right):
            snippet = code_part.strip()
            if len(snippet) > 80:
                snippet = snippet[:77] + '...'
            left_right_samples.append(
                f"Line {lineno}: \\left={line_left}, \\right={line_right} â†’ {snippet}"
            )

        if line_opens >= 1 and line_closes >= 1:
            idx_open = code_part.find(r'\(')
            idx_close = code_part.find(r'\)')
            if idx_close < idx_open:
                display_line = code_part.strip()
                if len(display_line) > 80:
                    display_line = display_line[:77] + '...'
                reversed_pairs.append((lineno, display_line))

    if opens != closes:
        issues.append(f"Math delimiter imbalance: opens={opens} closes={closes} diff={opens - closes}")
    if left_total != right_total:
        issues.append(f"\\left/\\right imbalance: left={left_total}, right={right_total}")
        if left_right_samples:
            issues.extend(left_right_samples[:5])

    stray = len(re.findall(r'(?<!\\)\$', tex_no_comments))
    if stray:
        issues.append(f"Stray dollar signs detected: {stray}")

    double_wrapped = (
        len(re.findall(r'\$\s*\\\(.*?\\\)\s*\$', tex_no_comments, flags=re.DOTALL)) +
        len(re.findall(r'\$\$\s*\\\(.*?\\\)\s*\$\$', tex_no_comments, flags=re.DOTALL))
    )
    if double_wrapped:
        issues.append(f"Double-wrapped math segments: {double_wrapped}")

    right_glitch = (
        len(re.findall(r'\\right\.\s*\$\$', tex_no_comments)) +
        len(re.findall(r'\\right\.\\\\\)', tex_no_comments))
    )
    if right_glitch:
        issues.append(f"Right boundary glitches: {right_glitch}")

    empty_math = (
        len(re.findall(r'\\\(\s*\\\)', tex_no_comments)) +
        len(re.findall(r'\\\[\s*\\\]', tex_no_comments))
    )
    if empty_math:
        issues.append(f"Empty math blocks: {empty_math}")

    unmatched_open_positions: List[int] = []
    unmatched_close_positions: List[int] = []

    token_iter = list(re.finditer(r'(\\\(|\\\))', tex_no_comments))
    stack: List[int] = []
    for m in token_iter:
        tok = m.group(0)
        pos = m.start()
        if tok == '\\(':
            stack.append(pos)
        else:
            if stack:
                stack.pop()
            else:
                unmatched_close_positions.append(pos)
    unmatched_open_positions.extend(stack)

    def _sample_at(pos: int, direction: str = 'forward', span: int = 140) -> str:
        if direction == 'forward':
            raw = tex_no_comments[pos:pos+span]
        else:
            start = max(0, pos-span)
            raw = tex_no_comments[start:pos+10]
        end_delim = raw.find('\\)')
        if end_delim != -1:
            raw = raw[:end_delim+2]
        raw = re.sub(r'\s+', ' ', raw).strip()
        return raw

    def _get_line_number(pos: int) -> int:
        return tex_no_comments[:pos].count('\n') + 1

    def _has_priority_keywords(sample: str) -> bool:
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦åŒ…å«ä¼˜å…ˆå…³é”®è¯ï¼ˆ\\right.ã€arrayã€casesã€é¢˜å·æ ‡è®°ç­‰ï¼‰"""
        priority_patterns = [
            r'\\right\.',
            r'\\begin\{array\}',
            r'\\end\{array\}',
            r'\\begin\{cases\}',
            r'\\end\{cases\}',
            r'\(\d+\)',  # (1) (2) ç­‰å°é—®æ ‡è®°
            r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # åœ†åœˆæ•°å­—
        ]
        return any(re.search(pat, sample) for pat in priority_patterns)

    # ğŸ†• v1.8.6ï¼šè¿›ä¸€æ­¥ç”„åˆ«"ç–‘ä¼¼æˆªæ–­"ï¼Œä¼˜å…ˆè¾“å‡ºåŒ…å«å…³é”®è¯çš„æ ·æœ¬
    truncated_open_samples: List[str] = []
    priority_open_samples: List[str] = []

    for p in unmatched_open_positions:
        segment = tex_no_comments[p:p+300]
        if '\\)' not in segment:  # æ˜æ˜¾æ²¡æœ‰é—­åˆ
            sample = _sample_at(p, 'forward')
            line_num = _get_line_number(p)
            sample_with_line = f"Line {line_num}: {sample}"

            if _has_priority_keywords(sample):
                priority_open_samples.append(sample_with_line)
            else:
                truncated_open_samples.append(sample_with_line)
        else:
            # å¯èƒ½é—­æ‹¬å·è¿œåœ¨è¶…è¿‡ 120 ä¹‹åï¼Œä¹Ÿè®¤ä¸ºå¯ç–‘
            close_rel = segment.find('\\)')
            if close_rel > 120:
                sample = _sample_at(p, 'forward')
                line_num = _get_line_number(p)
                sample_with_line = f"Line {line_num}: {sample}"

                if _has_priority_keywords(sample):
                    priority_open_samples.append(sample_with_line)
                else:
                    truncated_open_samples.append(sample_with_line)

        # é™åˆ¶æ€»æ•°
        if len(priority_open_samples) + len(truncated_open_samples) >= 10:
            break

    # ä¼˜å…ˆå±•ç¤ºåŒ…å«å…³é”®è¯çš„æ ·æœ¬ï¼Œç„¶åæ˜¯æ™®é€šæ ·æœ¬
    final_open_samples = priority_open_samples[:5] + truncated_open_samples[:max(0, 5 - len(priority_open_samples))]

    truncated_close_samples: List[str] = []
    priority_close_samples: List[str] = []

    for p in unmatched_close_positions[:10]:
        sample = _sample_at(p, 'backward')
        line_num = _get_line_number(p)
        sample_with_line = f"Line {line_num}: {sample}"

        if _has_priority_keywords(sample):
            priority_close_samples.append(sample_with_line)
        else:
            truncated_close_samples.append(sample_with_line)

    final_close_samples = priority_close_samples[:5] + truncated_close_samples[:max(0, 5 - len(priority_close_samples))]

    if final_open_samples:
        issues.append(
            "Unmatched opens (samples): " +
            '; '.join(final_open_samples)
        )
    if final_close_samples:
        issues.append(
            "Unmatched closes (samples): " +
            '; '.join(final_close_samples)
        )

    # é’ˆå¯¹å›¾ç‰‡å ä½ç¬¦é™„è¿‘çš„æˆªæ–­ï¼š\( ... IMAGE_TODO_START æœªé—­åˆ
    image_trunc = re.findall(r'\\\([^\\)]{0,200}?% IMAGE_TODO_START', tex_no_comments)
    if image_trunc:
        issues.append(f"Potential image-adjacent truncated math segments: {len(image_trunc)}")

    # ğŸ†• v1.8.7ï¼šæŠ¥å‘Šåå‘æ¨¡å¼ï¼ˆ\) åœ¨ \( å‰é¢ï¼‰
    if reversed_pairs:
        issues.append(f"Reversed inline math pairs detected: {len(reversed_pairs)} lines")
        for lineno, line_content in reversed_pairs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            issues.append(f"  Line {lineno}: {line_content}")

    return issues




def validate_brace_balance(tex: str) -> List[str]:
    """ğŸ†• v1.8.6ï¼šå…¨å±€èŠ±æ‹¬å·æ£€æŸ¥ - å¿½ç•¥ \\{ \\} å’Œæ³¨é‡Šï¼Œåªç»Ÿè®¡è£¸ { }

    è¿”å›å½¢å¦‚ï¼š
    - "Line 555: extra '}' (brace balance went negative)"
    - "Global brace imbalance at EOF: balance=..."
    """
    issues: List[str] = []
    balance = 0

    for lineno, raw_line in enumerate(tex.splitlines(), start=1):
        # å»æ‰æ³¨é‡Š
        line = raw_line.split('%', 1)[0]
        # å»æ‰è½¬ä¹‰çš„ \{ \}
        line_wo_esc = re.sub(r'\\[{}]', '', line)

        for ch in line_wo_esc:
            if ch == '{':
                balance += 1
            elif ch == '}':
                balance -= 1
                if balance < 0:
                    issues.append(f"Line {lineno}: extra '}}' (brace balance went negative)")
                    balance = 0

    if balance != 0:
        issues.append(f"Global brace imbalance at EOF: balance={balance}")

    return issues




def validate_latex_output(tex_content: str) -> List[str]:
    """
    ğŸ†• v1.3 æ–°å¢ï¼šéªŒè¯LaTeXè¾“å‡ºï¼Œè¿”å›è­¦å‘Šåˆ—è¡¨
    ğŸ†• v1.6.3ï¼šå¢åŠ ã€åˆ†æã€‘æ®‹ç•™æ£€æŸ¥

    Args:
        tex_content: ç”Ÿæˆçš„LaTeXå†…å®¹

    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    warnings = []

    # å»é™¤æ³¨é‡Šå†…å®¹ï¼Œé¿å… IMAGE_TODO çš„ CONTEXT æ³¨é‡Šè§¦å‘è¯¯æŠ¥
    tex_no_comments_lines: List[str] = []
    for line in tex_content.splitlines():
        tex_no_comments_lines.append(line.split('%', 1)[0])
    tex_no_comments = "\n".join(tex_no_comments_lines)

    # ğŸ†• æ£€æŸ¥0ï¼šã€åˆ†æã€‘meta æ®µæ®‹ç•™
    analysis_meta = re.findall(r'ã€\s*åˆ†æ\s*ã€‘', tex_no_comments)
    if analysis_meta:
        warnings.append(f"âŒ å‘ç° {len(analysis_meta)} å¤„ã€åˆ†æã€‘meta æ®µæ®‹ç•™ï¼ˆåº”å·²è¢«ä¸¢å¼ƒï¼‰")

    # æ£€æŸ¥1ï¼šæ®‹ç•™çš„ $ ç¬¦å·
    dollar_matches = re.findall(r'(?<!\\)\$[^\$]+\$', tex_no_comments)
    if dollar_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(dollar_matches)} å¤„æ®‹ç•™çš„ $ æ ¼å¼")
        for i, match in enumerate(dollar_matches[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            warnings.append(f"     ç¤ºä¾‹{i}: {match}")

    # æ£€æŸ¥2ï¼šæ®‹ç•™çš„"æ•…é€‰"
    guxuan_matches = re.findall(r'æ•…é€‰[:ï¼š][ABCD]+', tex_no_comments)
    if guxuan_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(guxuan_matches)} å¤„æ®‹ç•™çš„'æ•…é€‰'")

    # æ£€æŸ¥3ï¼šä¸­æ–‡æ‹¬å·
    chinese_paren = re.findall(r'[ï¼ˆï¼‰]', tex_no_comments)
    if chinese_paren:
        warnings.append(f"âš ï¸  å‘ç° {len(chinese_paren)} å¤„ä¸­æ–‡æ‹¬å·")

    # æ£€æŸ¥4ï¼šç¯å¢ƒé—­åˆ
    begin_count = tex_content.count(r'\begin{question}')
    end_count = tex_content.count(r'\end{question}')
    if begin_count != end_count:
        warnings.append(f"âŒ question ç¯å¢ƒä¸åŒ¹é…: {begin_count} ä¸ª begin, {end_count} ä¸ª end")

    begin_choices = tex_content.count(r'\begin{choices}')
    end_choices = tex_content.count(r'\end{choices}')
    if begin_choices != end_choices:
        warnings.append(f"âŒ choices ç¯å¢ƒä¸åŒ¹é…: {begin_choices} ä¸ª begin, {end_choices} ä¸ª end")

    # æ£€æŸ¥5ï¼šç©ºè¡Œåœ¨å®å‚æ•°ä¸­
    problematic_macros = []
    for macro in ['explain', 'topics', 'answer']:
        pattern = rf'\\{macro}\{{[^}}]*\n\s*\n[^}}]*\}}'
        if re.search(pattern, tex_content):
            problematic_macros.append(macro)
    if problematic_macros:
        warnings.append(f"âš ï¸  ä»¥ä¸‹å®å‚æ•°ä¸­å¯èƒ½æœ‰ç©ºè¡Œ: {', '.join(problematic_macros)}")

    return warnings


# ==================== ä¸»å‡½æ•° ====================



def validate_and_fix_image_todo_blocks(text: str) -> str:
    """ğŸ†• v1.8.5ï¼šéªŒè¯å¹¶ä¿®å¤ IMAGE_TODO å—æ ¼å¼é”™è¯¯

    æ£€æŸ¥å¹¶ä¿®å¤ï¼š
    1. IMAGE_TODO_END åçš„å¤šä½™èŠ±æ‹¬å·
    2. IMAGE_TODO_START å‚æ•°æ ¼å¼é”™è¯¯
    3. ç¼ºå¤±çš„å¿…éœ€å‚æ•°

    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š% IMAGE_TODO_END id=xxx{
        è¾“å‡ºï¼š% IMAGE_TODO_END id=xxx
    """
    if not text:
        return text

    issues = []

    # ä¿®å¤1ï¼šIMAGE_TODO_END åçš„å¤šä½™å­—ç¬¦ï¼ˆèŠ±æ‹¬å·æˆ–å…¶ä»–ï¼‰
    # åŒ¹é…ï¼š% IMAGE_TODO_END id=xxx{ æˆ– % IMAGE_TODO_END id=xxx {
    pattern = r'(% IMAGE_TODO_END id=[a-zA-Z0-9_-]+)\s*\{[^}]*\}'
    matches = list(re.finditer(pattern, text))
    for match in matches:
        line_num = text[:match.start()].count('\n') + 1
        issues.append(f"Line {line_num}: IMAGE_TODO_END has extra brace")

    # æ‰§è¡Œä¿®å¤
    text = re.sub(pattern, r'\1', text)

    # ä¿®å¤2ï¼šIMAGE_TODO_END åçš„å•ä¸ªèŠ±æ‹¬å·ï¼ˆæ— é…å¯¹ï¼‰
    text = re.sub(
        r'(% IMAGE_TODO_END id=[a-zA-Z0-9_-]+)\s*\{',
        r'\1',
        text
    )

    # ä¿®å¤3ï¼šIMAGE_TODO_START è¡Œæœ«çš„å¤šä½™å­—ç¬¦
    text = re.sub(
        r'(% IMAGE_TODO_START[^\n]+)\s*\{[^}]*\}',
        r'\1',
        text
    )

    # ä¿®å¤4ï¼šIMAGE_TODO_END ä¸æ­£æ–‡åŒå¤„ä¸€è¡Œï¼Œè‡ªåŠ¨æ‹†åˆ†
    # ğŸ”§ v1.9.9ï¼šä¿®å¤æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯æˆªæ–­ ID çš„é—®é¢˜
    # åŸæ­£åˆ™ r'(% IMAGE_TODO_END id=[^\n]+)([^\n]+)' ä¼šé”™è¯¯åœ°å°† ID æœ«å°¾çš„æ•°å­—
    # ï¼ˆå¦‚ img2 çš„ 2ï¼‰å½“ä½œ"å°¾éšå†…å®¹"æ‹†åˆ†åˆ°ä¸‹ä¸€è¡Œ
    # ä¿®å¤ï¼šID æ ¼å¼ä¸º slug-QN-imgNï¼Œä»¥å­—æ¯æ•°å­—ç»“å°¾ï¼Œåé¢å¿…é¡»æœ‰éå­—æ¯æ•°å­—å­—ç¬¦æ‰ç®—å°¾éšå†…å®¹
    def _split_image_end(match: re.Match) -> str:
        trailing = match.group(2)
        if not trailing.strip():
            return match.group(1)
        return f"{match.group(1)}\n{trailing.lstrip()}"

    text = re.sub(
        r'(% IMAGE_TODO_END id=[a-zA-Z0-9_-]+)([^a-zA-Z0-9_\n-][^\n]*)',
        _split_image_end,
        text
    )

    # é™é»˜ä¿®å¤ IMAGE_TODO æ ¼å¼é”™è¯¯
    return text





# ============================================================
# å¯¼å‡ºåˆ—è¡¨
# ============================================================

__all__ = [
    'validate_math_integrity',
    'validate_brace_balance',
    'validate_latex_output',
    'validate_and_fix_image_todo_blocks',
]
