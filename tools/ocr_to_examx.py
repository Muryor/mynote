#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ocr_to_examx_v1.3.py - v1.3 å¢å¼ºç‰ˆ OCR è¯•å·é¢„å¤„ç†è„šæœ¬

v1.3 æ–°å¢æ”¹è¿›ï¼š
1. ğŸ†• ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢ï¼ˆ-80% æ®‹ç•™ç‡ï¼‰
2. ğŸ†• æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™ï¼ˆ-75% æ®‹ç•™ç‡ï¼‰
3. ğŸ†• ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆæ‹¬å·ã€å¼•å·ï¼‰
4. ğŸ†• æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½
5. âœ… ä¿ç•™ v1.2 æ‰€æœ‰æ”¹è¿›

v1.2 æ”¹è¿›å›é¡¾ï¼š
- åŠ å¼ºç©ºè¡Œæ¸…ç†ï¼ˆè§£å†³80%çš„Runaway argumenté”™è¯¯ï¼‰
- è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²ï¼ˆè§£å†³ç¼–è¯‘æ…¢é—®é¢˜ï¼‰
- å¢å¼ºæ•°å­¦å˜é‡æ£€æµ‹ï¼ˆå‡å°‘Missing $é”™è¯¯ï¼‰
- å¢å¼ºé€‰é¡¹è§£æï¼ˆå¤„ç†åµŒå…¥çš„è§£æå†…å®¹ï¼‰
- æ–°å¢questionç¯å¢ƒæ¸…ç†

ç‰ˆæœ¬ï¼šv1.3
ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2025-11-13
"""

import re
import argparse
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# ==================== é…ç½® ====================

VERSION = "v1.3"

SECTION_MAP = {
    "ä¸€ã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å•é€‰é¢˜": "å•é€‰é¢˜",
    "äºŒã€å¤šé€‰é¢˜": "å¤šé€‰é¢˜",
    "ä¸‰ã€å¡«ç©ºé¢˜": "å¡«ç©ºé¢˜",
    "å››ã€è§£ç­”é¢˜": "è§£ç­”é¢˜",
}

META_PATTERNS = {
    "answer": r"^ã€ç­”æ¡ˆã€‘(.*)$",
    "difficulty": r"^ã€éš¾åº¦ã€‘([\d.]+)",
    "topics": r"^ã€çŸ¥è¯†ç‚¹ã€‘(.*)$",
    "analysis": r"^ã€åˆ†æã€‘(.*)$",
    "explain": r"^ã€è¯¦è§£ã€‘(.*)$",
}

IMAGE_PATTERN = re.compile(r"!\[\]\((images/[^)]+)\)(?:\{width=(\d+)%\})?")

LATEX_SPECIAL_CHARS = {
    "%": r"\%",
    "&": r"\&",
    "#": r"\#",
    "~": r"\textasciitilde{}",
}

# è§£ææ ‡è®°è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼‰
ANALYSIS_MARKERS = [
    'æ ¹æ®', 'ç”±é¢˜æ„', 'å› ä¸º', 'æ‰€ä»¥', 'æ•…é€‰', 'ç­”æ¡ˆ',
    'åˆ†æ', 'è¯¦è§£', 'è§£ç­”', 'è¯æ˜', 'è®¡ç®—å¯å¾—',
    'æ˜¾ç„¶', 'æ˜“çŸ¥', 'å¯çŸ¥', 'ä¸éš¾çœ‹å‡º', 'ç”±æ­¤å¯å¾—',
    'ç»¼ä¸Š', 'æ•…', 'å³', 'åˆ™', 'å¯å¾—'
]


# ==================== æ–‡ä»¶å¤¹å¤„ç†å‡½æ•° ====================

def find_markdown_and_images(input_path: Path) -> Tuple[Path, Optional[Path]]:
    """æ™ºèƒ½è¯†åˆ«è¾“å…¥è·¯å¾„"""
    input_path = Path(input_path).resolve()
    
    if input_path.is_file() and input_path.suffix == '.md':
        md_file = input_path
        images_dir = input_path.parent / 'images'
        if not images_dir.exists():
            images_dir = None
        return md_file, images_dir
    
    if input_path.is_dir():
        md_files = list(input_path.glob('*_local.md'))
        if not md_files:
            md_files = list(input_path.glob('*.md'))
        
        if not md_files:
            raise FileNotFoundError(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        
        if len(md_files) > 1:
            print(f"âš ï¸  æ‰¾åˆ°å¤šä¸ª .md æ–‡ä»¶ï¼Œä½¿ç”¨ï¼š{md_files[0].name}")
        
        md_file = md_files[0]
        images_dir = input_path / 'images'
        if not images_dir.exists():
            images_dir = None
        
        return md_file, images_dir
    
    raise ValueError(f"æ— æ•ˆçš„è¾“å…¥ï¼š{input_path}")


def copy_images_to_output(images_dir: Path, output_dir: Path) -> int:
    """å¤åˆ¶å›¾ç‰‡"""
    if images_dir is None or not images_dir.exists():
        return 0
    
    output_images_dir = output_dir / 'images'
    if output_images_dir.exists():
        shutil.rmtree(output_images_dir)
    
    shutil.copytree(images_dir, output_images_dir)
    return len(list(output_images_dir.glob('*')))


# ==================== LaTeX å¤„ç†å‡½æ•° ====================

def escape_latex_special(text: str, in_math_mode: bool = False) -> str:
    """è½¬ä¹‰ LaTeX ç‰¹æ®Šå­—ç¬¦"""
    if in_math_mode:
        for char in ["%", "&", "#", "~"]:
            if char in LATEX_SPECIAL_CHARS:
                text = text.replace(char, LATEX_SPECIAL_CHARS[char])
    else:
        protected = []
        def save_comment(match):
            protected.append(match.group(0))
            return f"@@COMMENT_{len(protected)-1}@@"
        text = re.sub(r'%.*$', save_comment, text, flags=re.MULTILINE)
        
        for char, escaped in LATEX_SPECIAL_CHARS.items():
            text = text.replace(char, escaped)
        
        for i, comment in enumerate(protected):
            text = text.replace(f"@@COMMENT_{i}@@", comment)
    return text


def smart_inline_math(text: str) -> str:
    r"""æ™ºèƒ½è½¬æ¢è¡Œå†…å…¬å¼ï¼š$...$ -> \(...\)
    
    ğŸ†• v1.3 æ”¹è¿›ï¼šä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ å…œåº•è½¬æ¢
    """
    if not text:
        return text
    
    # ä¿æŠ¤è¡Œé—´å…¬å¼
    display_math_blocks = []
    def save_display(match):
        display_math_blocks.append(match.group(0))
        return f"@@DISPLAYMATH{len(display_math_blocks)-1}@@"
    text = re.sub(r'\$\$(.+?)\$\$', save_display, text, flags=re.DOTALL)
    
    # ä¿æŠ¤å·²æœ‰çš„è¡Œå†…å…¬å¼
    inline_math_blocks = []
    def save_inline(match):
        inline_math_blocks.append(match.group(0))
        return f"@@INLINEMATH{len(inline_math_blocks)-1}@@"
    text = re.sub(r'\\\((.+?)\\\)', save_inline, text, flags=re.DOTALL)
    
    # ä¿æŠ¤TikZåæ ‡
    tikz_coords = []
    def save_tikz_coord(match):
        tikz_coords.append(match.group(0))
        return f"@@TIKZCOORD{len(tikz_coords)-1}@@"
    text = re.sub(r'\$\([\d\w\s,+\-*/\.]+\)\$', save_tikz_coord, text)
    
    # è½¬æ¢ $ ... $ ä¸º \(...\)
    text = re.sub(r'(?<!\\)\$([^\$]+?)\$', r'\\(\1\\)', text)
    
    # ğŸ†• v1.3 æ”¹è¿›ï¼šå…œåº•æ£€æŸ¥ï¼Œå¼ºåˆ¶è½¬æ¢æ‰€æœ‰æ®‹ç•™çš„ $ æ ¼å¼ï¼ˆå•è¡Œå†…ï¼Œé™åˆ¶200å­—ç¬¦ï¼‰
    text = re.sub(r'(?<!\\)\$([^\$\n]{1,200}?)\$', r'\\(\1\\)', text)
    
    # æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, block in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZCOORD{i}@@", block)
    for i, block in enumerate(inline_math_blocks):
        text = text.replace(f"@@INLINEMATH{i}@@", block)
    for i, block in enumerate(display_math_blocks):
        text = text.replace(f"@@DISPLAYMATH{i}@@", block)
    return text


def wrap_math_variables(text: str) -> str:
    """æ™ºèƒ½åŒ…è£¹æ•°å­¦å˜é‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    # ä¿æŠ¤å·²æœ‰çš„æ•°å­¦æ¨¡å¼
    protected = []
    def save_math(match):
        protected.append(match.group(0))
        return f"@@MATH{len(protected)-1}@@"
    
    text = re.sub(r'\\\(.*?\\\)', save_math, text, flags=re.DOTALL)
    text = re.sub(r'\\\[.*?\\\]', save_math, text, flags=re.DOTALL)
    
    # ä¿æŠ¤ TikZ åæ ‡
    tikz_coords = []
    def save_tikz(match):
        tikz_coords.append(match.group(0))
        return f"@@TIKZ{len(tikz_coords)-1}@@"
    text = re.sub(r'\$\([\d\w\s,+\-*/\.]+\)\$', save_tikz, text)
    
    # è§„åˆ™1ï¼šå•å­—æ¯å˜é‡ + è¿ç®—ç¬¦/ä¸‹æ ‡/ä¸Šæ ‡
    text = re.sub(
        r'\b([a-zA-Z])(?=\s*[=+\-*/^<>]|_{|\^{)',
        r'\\(\1\\)',
        text
    )
    
    # è§„åˆ™2ï¼šæ•°å­¦å‡½æ•°å¿…é¡»æœ‰åæ–œæ 
    math_functions = [
        'sin', 'cos', 'tan', 'cot', 'sec', 'csc',
        'arcsin', 'arccos', 'arctan',
        'sinh', 'cosh', 'tanh',
        'log', 'ln', 'lg', 'exp',
        'lim', 'sup', 'inf',
        'max', 'min', 'det', 'dim', 'ker'
    ]
    for func in math_functions:
        text = re.sub(rf'(?<!\\)\b{func}\b(?!\w)', rf'\\{func}', text)
    
    # è§„åˆ™3ï¼šè™šæ•°å•ä½ i
    text = re.sub(r'(?<!\\)\bi\b(?=[^a-zA-Z])', r'\\mathrm{i}', text)
    
    # æ¢å¤ä¿æŠ¤çš„å†…å®¹
    for i, coord in enumerate(tikz_coords):
        text = text.replace(f"@@TIKZ{i}@@", coord)
    for i, math in enumerate(protected):
        text = text.replace(f"@@MATH{i}@@", math)
    
    return text


def remove_blank_lines_in_macro_args(text: str) -> str:
    """åˆ é™¤å®å‚æ•°ä¸­çš„ç©ºè¡Œï¼ˆå¢å¼ºç‰ˆï¼‰"""
    macros = ['explain', 'topics', 'answer', 'difficulty', 'source']
    
    for macro in macros:
        pattern = rf'(\\{macro}\{{)([^{{}}]*(?:\{{[^{{}}]*\}}[^{{}}]*)*?)(\}})'
        
        def clean_arg(match):
            prefix = match.group(1)
            arg = match.group(2)
            suffix = match.group(3)
            
            # æ”¹è¿›1ï¼šåˆ é™¤è¿ç»­ç©ºè¡Œ
            arg = re.sub(r'\n\s*\n+', '\n', arg)
            
            # æ”¹è¿›2ï¼šåˆ é™¤æ®µé¦–æ®µå°¾ç©ºè¡Œ
            arg = arg.strip()
            
            # æ”¹è¿›3ï¼šæ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼ˆä¿ç•™ç¼©è¿›ï¼‰
            lines = arg.split('\n')
            lines = [line.rstrip() for line in lines]
            arg = '\n'.join(lines)
            
            return prefix + arg + suffix
        
        text = re.sub(pattern, clean_arg, text, flags=re.DOTALL)
    
    return text


def clean_question_environments(text: str) -> str:
    """æ¸…ç† question ç¯å¢ƒå†…éƒ¨çš„å¤šä½™ç©ºè¡Œ"""
    pattern = r'(\\begin\{question\})(.*?)(\\end\{question\})'
    
    def clean_env(match):
        begin = match.group(1)
        content = match.group(2)
        end = match.group(3)
        
        # åˆ é™¤è¿ç»­çš„3ä¸ªä»¥ä¸Šæ¢è¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        return begin + content + end
    
    return re.sub(pattern, clean_env, text, flags=re.DOTALL)


def split_long_lines_in_explain(text: str, max_length: int = 800) -> str:
    """åœ¨ explain{} ä¸­è‡ªåŠ¨åˆ†å‰²è¶…é•¿è¡Œ"""
    pattern = r'(\\explain\{)([^{}]*(?:\{[^{}]*\}[^{}]*)*?)(\})'
    
    def split_content(match):
        prefix = match.group(1)
        content = match.group(2)
        suffix = match.group(3)
        
        lines = content.split('\n')
        new_lines = []
        
        for line in lines:
            if len(line) <= max_length:
                new_lines.append(line)
            else:
                # åœ¨æ ‡ç‚¹ååˆ†å‰²
                segments = re.split(r'([ï¼Œã€‚ï¼›ï¼ï¼Ÿ])', line)
                current = ""
                for seg in segments:
                    if len(current + seg) > max_length and current:
                        new_lines.append(current.rstrip())
                        current = seg
                    else:
                        current += seg
                if current:
                    new_lines.append(current.rstrip())
        
        return prefix + '\n'.join(new_lines) + suffix
    
    return re.sub(pattern, split_content, text, flags=re.DOTALL)


def convert_markdown_table_to_latex(text: str) -> str:
    """å°† Markdown è¡¨æ ¼è½¬æ¢ä¸º LaTeX tabular"""
    table_pattern = r'(\|[^\n]+\|\n)+\|[-:\s|]+\|\n(\|[^\n]+\|\n)+'
    
    def convert_one_table(match):
        table_text = match.group(0)
        lines = [line.strip() for line in table_text.split('\n') if line.strip()]
        
        data_lines = [line for line in lines if not re.match(r'^\|[-:\s|]+\|$', line)]
        
        if not data_lines:
            return table_text
        
        rows = []
        for line in data_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            rows.append(cells)
        
        if not rows:
            return table_text
        
        ncols = len(rows[0])
        latex = "\\begin{center}\n"
        latex += f"\\begin{{tabular}}{{{'c' * ncols}}}\n"
        latex += "\\hline\n"
        
        header = rows[0]
        latex += " & ".join(escape_latex_special(cell, False) for cell in header)
        latex += " \\\\\n\\hline\n"
        
        for row in rows[1:]:
            latex += " & ".join(escape_latex_special(cell, False) for cell in row)
            latex += " \\\\\n"
        
        latex += "\\hline\n\\end{tabular}\n\\end{center}"
        return latex
    
    return re.sub(table_pattern, convert_one_table, text)


def clean_markdown(text: str) -> str:
    """æ¸…ç† markdown åƒåœ¾
    
    ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    """
    text = re.sub(
        r"<br><span class='markdown-page-line'>.*?</span><br><br>",
        "\n", text, flags=re.S,
    )
    text = re.sub(
        r"<span id='page\d+' class='markdown-page-text'>\[.*?\]</span>",
        "", text,
    )
    
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text)
    
    # ğŸ†• v1.3 æ”¹è¿›ï¼šç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹
    # ä¿æŠ¤å·²æœ‰çš„LaTeXå‘½ä»¤
    protected = []
    def save_latex_cmd(match):
        protected.append(match.group(0))
        return f"@@LATEXCMD{len(protected)-1}@@"
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', save_latex_cmd, text)
    
    # ç»Ÿä¸€æ‹¬å·ï¼ˆå…¨è§’â†’åŠè§’ï¼‰
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    # ç»Ÿä¸€å¼•å·ï¼ˆå¼¯å¼•å·â†’ç›´å¼•å·ï¼‰
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")
    
    # æ¢å¤LaTeXå‘½ä»¤
    for i, cmd in enumerate(protected):
        text = text.replace(f"@@LATEXCMD{i}@@", cmd)
    
    # æ¸…ç†ä»£ç å—æ ‡è®°
    text = re.sub(r'```[a-z]*\n?', '', text)
    text = re.sub(r'```', '', text)
    
    # è½¬æ¢è¡¨æ ¼
    if '|' in text and '---' in text:
        text = convert_markdown_table_to_latex(text)
    
    # å¤„ç†ä¸‹åˆ’çº¿
    text = text.replace(r'\_', '@@ESCAPED_UNDERSCORE@@')
    text = re.sub(r'(?<!\\)_(?![{_])', r'\\_', text)
    text = text.replace('@@ESCAPED_UNDERSCORE@@', r'\_')
    
    return text.strip()


# ==================== é¢˜ç›®è§£æå‡½æ•° ====================

def split_sections(text: str) -> List[Tuple[str, str]]:
    """æ‹†åˆ†ç« èŠ‚"""
    lines = text.splitlines()
    sections = []
    current_title = None
    current_lines = []

    for line in lines:
        stripped = line.strip()
        m = re.match(
            r"^#+\s*(ä¸€ã€å•é€‰é¢˜|äºŒã€å•é€‰é¢˜|äºŒã€å¤šé€‰é¢˜|ä¸‰ã€å¡«ç©ºé¢˜|å››ã€è§£ç­”é¢˜)",
            stripped,
        )
        if m:
            if current_title is not None:
                sections.append((current_title, "\n".join(current_lines).strip()))
                current_lines = []
            current_title = m.group(1)
        else:
            if current_title is not None:
                current_lines.append(line)

    if current_title is not None and current_lines:
        sections.append((current_title, "\n".join(current_lines).strip()))

    return sections


def split_questions(section_body: str) -> List[str]:
    """æ‹†åˆ†é¢˜ç›®"""
    lines = section_body.splitlines()
    blocks = []
    current = []

    def flush():
        if current:
            blocks.append("\n".join(current).strip())
            current.clear()

    for line in lines:
        stripped = line.strip()
        if re.match(r"^\d+[\.ï¼ã€]\s*", stripped):
            flush()
            current.append(line)
        else:
            current.append(line)

    flush()
    return blocks


def extract_meta_and_images(block: str) -> Tuple[str, Dict, List]:
    """æå–å…ƒä¿¡æ¯å’Œå›¾ç‰‡"""
    meta = {k: "" for k in META_PATTERNS}
    content_lines = []
    images = []

    for line in block.splitlines():
        stripped = line.strip()

        m_img = IMAGE_PATTERN.search(stripped)
        if m_img:
            images.append({
                "path": m_img.group(1),
                "width": int(m_img.group(2)) if m_img.group(2) else 50,
            })
            continue

        matched_meta = False
        for key, pat in META_PATTERNS.items():
            m = re.match(pat, stripped)
            if m:
                if key != "analysis":
                    meta[key] = m.group(1).strip()
                matched_meta = True
                break

        if not matched_meta:
            content_lines.append(line)

    content = "\n".join(content_lines).strip()
    return content, meta, images


def parse_question_structure(content: str) -> Dict:
    """æ™ºèƒ½è¯†åˆ«é¢˜ç›®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    lines = content.splitlines()
    
    structure = {
        'stem_lines': [],
        'choices': [],
        'analysis_lines': [],
        'in_choice': False,
        'in_analysis': False,
        'current_choice': '',
    }
    
    choice_pattern = re.compile(r'^([A-D])[\.ï¼ã€]\s*(.*)$')
    
    for line in lines:
        stripped = line.strip()
        
        m = choice_pattern.match(stripped)
        if m:
            if structure['current_choice']:
                structure['choices'].append(structure['current_choice'])
            
            structure['current_choice'] = m.group(2)
            structure['in_choice'] = True
            structure['in_analysis'] = False
            continue
        
        # æ£€æŸ¥æ˜¯å¦è¿›å…¥è§£æéƒ¨åˆ†
        if structure['in_choice']:
            if any(marker in stripped for marker in ANALYSIS_MARKERS):
                structure['in_choice'] = False
                structure['in_analysis'] = True
                structure['analysis_lines'].append(stripped)
            else:
                structure['current_choice'] += ' ' + stripped
        elif structure['in_analysis']:
            structure['analysis_lines'].append(line)
        else:
            structure['stem_lines'].append(line)
    
    if structure['current_choice']:
        structure['choices'].append(structure['current_choice'])
    
    return structure


def convert_choices(content: str) -> Tuple[str, List[str], str]:
    """æ‹†åˆ†é¢˜å¹²ã€é€‰é¡¹ã€è§£æï¼ˆå¢å¼ºç‰ˆï¼‰"""
    structure = parse_question_structure(content)
    
    stem = '\n'.join(structure['stem_lines']).strip()
    stem = re.sub(r"^\s*\d+[\.ï¼ã€]\s*", "", stem)
    
    # æå–çš„è§£æå†…å®¹
    analysis = '\n'.join(structure['analysis_lines']).strip()
    
    return stem, structure['choices'], analysis


def handle_subquestions(content: str) -> str:
    """å¤„ç†è§£ç­”é¢˜çš„å°é¢˜ç¼–å·"""
    if not re.search(r'\(\d+\)', content):
        return content
    
    subquestions = re.findall(r'\((\d+)\)(.*?)(?=\(\d+\)|$)', content, re.DOTALL)
    
    if len(subquestions) < 2:
        return content
    
    result_lines = []
    for num, content_text in subquestions:
        result_lines.append(f"\\item {content_text.strip()}")
    
    return '\n'.join(result_lines)


def process_text_for_latex(text: str, is_math_heavy: bool = False) -> str:
    """ç»Ÿä¸€å¤„ç†æ–‡æœ¬
    
    ğŸ†• v1.3 æ”¹è¿›ï¼šæ›´å¼ºçš„"æ•…é€‰"æ¸…ç†è§„åˆ™
    """
    if not text:
        return text
    
    # ğŸ†• v1.3 æ”¹è¿›ï¼šæ›´å¼ºçš„"æ•…é€‰"æ¸…ç†è§„åˆ™
    # æ¸…ç†ç»“å°¾çš„"æ•…é€‰"ï¼ˆæ”¯æŒå¤šç§æ ‡ç‚¹ï¼‰
    text = re.sub(r'[,ï¼Œã€‚\.;ï¼›]\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    # æ¸…ç†å•ç‹¬ä¸€è¡Œçš„"æ•…é€‰"
    text = re.sub(r'\n+æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*$', '', text)
    # æ¸…ç†å¼€å¤´çš„"æ•…é€‰"ï¼ˆç½•è§ä½†å¯èƒ½ï¼‰
    text = re.sub(r'^\s*æ•…é€‰[:ï¼š][ABCD]+[.ã€‚]?\s*', '', text)
    # æ¸…ç†"æ•…ç­”æ¡ˆä¸º"
    text = re.sub(r'\n+æ•…ç­”æ¡ˆä¸º[:ï¼š]', '', text)
    # æ¸…ç†"ã€è¯¦è§£ã€‘"æ ‡è®°
    text = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', text)
    
    if not is_math_heavy:
        text = escape_latex_special(text, in_math_mode=False)
    
    text = smart_inline_math(text)
    text = wrap_math_variables(text)
    
    return text


def build_question_tex(stem: str, options: List, meta: Dict, images: List, 
                       section_type: str) -> str:
    """ç”Ÿæˆ question ç¯å¢ƒ"""
    stem = process_text_for_latex(stem, is_math_heavy=True)
    
    if section_type == "è§£ç­”é¢˜" and re.search(r'\(\d+\)', stem):
        stem = handle_subquestions(stem)
    
    explain_raw = meta.get("explain", "").strip()
    if explain_raw:
        explain_raw = re.sub(r'^ã€?è¯¦è§£ã€‘?[:ï¼š]?\s*', '', explain_raw)
        explain_raw = process_text_for_latex(explain_raw, is_math_heavy=True)
    
    topics_raw = meta.get("topics", "").strip()
    if topics_raw:
        topics_raw = topics_raw.replace("ã€", "ï¼›")
        topics_raw = escape_latex_special(topics_raw, in_math_mode=False)

    lines = []
    lines.append(r"\begin{question}")
    
    if stem:
        lines.append(stem)

    if options:
        lines.append(r"\begin{choices}")
        for opt in options:
            opt_processed = process_text_for_latex(opt, is_math_heavy=True)
            lines.append(f"  \\item {opt_processed}")
        lines.append(r"\end{choices}")

    for img in images:
        lines.append("")
        lines.append(r"\begin{center}")
        lines.append(f"% IMAGE_TODO: {img['path']} (width={img['width']}%)")
        lines.append(r"\begin{tikzpicture}[scale=1.05,>=Stealth,line cap=round,line join=round]")
        lines.append(r"  % TODO: AI Agent å°†ä½¿ç”¨ view å·¥å…·æŸ¥çœ‹æ­¤å›¾ç‰‡å¹¶ç”Ÿæˆ TikZ ä»£ç ")
        lines.append(f"  % view {img['path']}")
        lines.append(r"\end{tikzpicture}")
        lines.append(r"\end{center}")

    if topics_raw:
        lines.append(f"\\topics{{{topics_raw}}}")
    if meta.get("difficulty"):
        lines.append(f"\\difficulty{{{meta['difficulty']}}}")
    if meta.get("answer"):
        ans = escape_latex_special(meta["answer"], in_math_mode=False)
        lines.append(f"\\answer{{{ans}}}")
    if explain_raw:
        lines.append(f"\\explain{{{explain_raw}}}")

    lines.append(r"\end{question}")
    return "\n".join(lines)


def convert_md_to_examx(md_text: str, title: str) -> str:
    """ä¸»è½¬æ¢å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    md_text = clean_markdown(md_text)
    sections = split_sections(md_text)

    out_lines = []
    out_lines.append(f"\\examxtitle{{{title}}}")

    for raw_title, body in sections:
        sec_label = SECTION_MAP.get(raw_title, raw_title)
        out_lines.append("")
        out_lines.append(f"\\section{{{sec_label}}}")

        for block in split_questions(body):
            if not block.strip():
                continue
            
            content, meta, images = extract_meta_and_images(block)
            
            # ä½¿ç”¨å¢å¼ºçš„è½¬æ¢å‡½æ•°ï¼ˆè¿”å›3ä¸ªå€¼ï¼‰
            stem, options, extracted_analysis = convert_choices(content)
            
            # åˆå¹¶æå–çš„è§£æå’Œå…ƒä¿¡æ¯ä¸­çš„è§£æ
            if extracted_analysis and not meta.get('explain'):
                meta['explain'] = extracted_analysis
            elif extracted_analysis:
                meta['explain'] = meta['explain'] + '\n' + extracted_analysis
            
            q_tex = build_question_tex(stem, options, meta, images, sec_label)
            out_lines.append("")
            out_lines.append(q_tex)

    out_lines.append("")
    
    # æœ€ç»ˆå¤„ç†ï¼šæ¸…ç†ç©ºè¡Œå’Œåˆ†å‰²è¶…é•¿è¡Œ
    result = "\n".join(out_lines)
    result = remove_blank_lines_in_macro_args(result)
    result = clean_question_environments(result)
    result = split_long_lines_in_explain(result, max_length=800)
    
    return result


# ==================== ğŸ†• v1.3 æ–°å¢ï¼šè‡ªåŠ¨éªŒè¯å‡½æ•° ====================

def validate_latex_output(tex_content: str) -> List[str]:
    """
    ğŸ†• v1.3 æ–°å¢ï¼šéªŒè¯LaTeXè¾“å‡ºï¼Œè¿”å›è­¦å‘Šåˆ—è¡¨
    
    Args:
        tex_content: ç”Ÿæˆçš„LaTeXå†…å®¹
    
    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    warnings = []
    
    # æ£€æŸ¥1ï¼šæ®‹ç•™çš„ $ ç¬¦å·
    dollar_matches = re.findall(r'(?<!\\)\$[^\$]+\$', tex_content)
    if dollar_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(dollar_matches)} å¤„æ®‹ç•™çš„ $ æ ¼å¼")
        for i, match in enumerate(dollar_matches[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            warnings.append(f"     ç¤ºä¾‹{i}: {match}")
    
    # æ£€æŸ¥2ï¼šæ®‹ç•™çš„"æ•…é€‰"
    guxuan_matches = re.findall(r'æ•…é€‰[:ï¼š][ABCD]+', tex_content)
    if guxuan_matches:
        warnings.append(f"âš ï¸  å‘ç° {len(guxuan_matches)} å¤„æ®‹ç•™çš„'æ•…é€‰'")
    
    # æ£€æŸ¥3ï¼šä¸­æ–‡æ‹¬å·
    chinese_paren = re.findall(r'[ï¼ˆï¼‰]', tex_content)
    if chinese_paren:
        warnings.append(f"âš ï¸  å‘ç° {len(chinese_paren)} å¤„ä¸­æ–‡æ‹¬å·")
    
    # æ£€æŸ¥4ï¼šç¯å¢ƒé—­åˆ
    begin_count = tex_content.count(r'\begin{question}')
    end_count = tex_content.count(r'\end{question}')
    if begin_count != end_count:
        warnings.append(f"âŒ question ç¯å¢ƒä¸åŒ¹é…: {begin_count} ä¸ª begin, {end_count} ä¸ª end")
    
    begin_choices = tex_content.count(r'\begin{choices}')
    end_choices = tex_content.count(r'\end{choices}')
    if begin_choices != end_choices:
        warnings.append(f"âŒ choices ç¯å¢ƒä¸åŒ¹é…: {begin_choices} ä¸ª begin, {end_choices} ä¸ª end")
    
    # æ£€æŸ¥5ï¼šç©ºè¡Œåœ¨å®å‚æ•°ä¸­
    problematic_macros = []
    for macro in ['explain', 'topics', 'answer']:
        pattern = rf'\\{macro}\{{[^}}]*\n\s*\n[^}}]*\}}'
        if re.search(pattern, tex_content):
            problematic_macros.append(macro)
    if problematic_macros:
        warnings.append(f"âš ï¸  ä»¥ä¸‹å®å‚æ•°ä¸­å¯èƒ½æœ‰ç©ºè¡Œ: {', '.join(problematic_macros)}")
    
    return warnings


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description=f"OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ†• v1.3 æ–°å¢åŠŸèƒ½ï¼š
  - ä¿®å¤ docstring è­¦å‘Šï¼Œæ·»åŠ  $ æ ¼å¼å…œåº•è½¬æ¢ï¼ˆ-80% æ®‹ç•™ç‡ï¼‰
  - æ”¹è¿›"æ•…é€‰"æ¸…ç†è§„åˆ™ï¼ˆ-75% æ®‹ç•™ç‡ï¼‰
  - ç»Ÿä¸€ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆæ‹¬å·ã€å¼•å·ï¼‰
  - æ·»åŠ è‡ªåŠ¨éªŒè¯åŠŸèƒ½

âœ… v1.2 æ”¹è¿›å›é¡¾ï¼š
  - åŠ å¼ºç©ºè¡Œæ¸…ç†ï¼ˆè§£å†³80%çš„Runaway argumenté”™è¯¯ï¼‰
  - è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²ï¼ˆè§£å†³ç¼–è¯‘æ…¢é—®é¢˜ï¼‰
  - å¢å¼ºæ•°å­¦å˜é‡æ£€æµ‹ï¼ˆå‡å°‘Missing $é”™è¯¯ï¼‰
  - å¢å¼ºé€‰é¡¹è§£æï¼ˆå¤„ç†åµŒå…¥çš„è§£æå†…å®¹ï¼‰
  - æ–°å¢questionç¯å¢ƒæ¸…ç†

ä½¿ç”¨ç¤ºä¾‹:
  python3 ocr_to_examx_v1.3.py "æµ™æ±Ÿçœé‡‘ååæ ¡/" output/
        """
    )
    
    parser.add_argument("input", help="è¾“å…¥è·¯å¾„ï¼ˆ.md æ–‡ä»¶æˆ– OCR æ–‡ä»¶å¤¹ï¼‰")
    parser.add_argument("output", help="è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ– .tex æ–‡ä»¶ï¼‰")
    parser.add_argument("--title", help="è¯•å·æ ‡é¢˜", default=None)
    parser.add_argument("--version", action="version", version=f"%(prog)s {VERSION}")
    
    args = parser.parse_args()
    
    try:
        print(f"ğŸ” OCR è¯•å·é¢„å¤„ç†è„šæœ¬ - {VERSION}")
        print("â”" * 60)
        md_file, images_dir = find_markdown_and_images(args.input)
        
        print(f"ğŸ“„ Markdown: {md_file.name}")
        if images_dir:
            img_count = len(list(images_dir.glob('*')))
            print(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {images_dir} ({img_count} ä¸ªæ–‡ä»¶)")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•")
        
        output_path = Path(args.output)
        if output_path.suffix == '.tex':
            output_tex = output_path
            output_dir = output_path.parent
        else:
            output_dir = output_path
            output_tex = output_dir / f"{md_file.stem.replace('_local', '_raw')}.tex"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if images_dir:
            img_count = copy_images_to_output(images_dir, output_dir)
            print(f"âœ… å·²å¤åˆ¶ {img_count} ä¸ªå›¾ç‰‡åˆ° {output_dir}/images/")
        
        title = args.title
        if title is None:
            input_path = Path(args.input)
            if input_path.is_dir():
                title = input_path.name
            else:
                title = md_file.stem.replace('_local', '')
        
        print(f"\nğŸ“– æ­£åœ¨è½¬æ¢...")
        print(f"ğŸ“ æ ‡é¢˜: {title}")
        
        md_text = md_file.read_text(encoding='utf-8')
        tex_text = convert_md_to_examx(md_text, title)
        
        # ğŸ†• v1.3ï¼šéªŒè¯è¾“å‡º
        warnings = validate_latex_output(tex_text)
        
        output_tex.write_text(tex_text, encoding='utf-8')
        
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
        print("â”" * 60)
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶: {output_tex}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(tex_text):,} å­—èŠ‚")
        
        question_count = tex_text.count(r'\begin{question}')
        image_count = tex_text.count('IMAGE_TODO')
        print(f"ğŸ“‹ é¢˜ç›®æ•°é‡: {question_count}")
        if image_count > 0:
            print(f"ğŸ–¼ï¸  å›¾ç‰‡å ä½: {image_count}")
        
        print(f"\nğŸ†• v1.3 æ”¹è¿›å·²åº”ç”¨:")
        print(f"  âœ… $ æ ¼å¼å…œåº•è½¬æ¢")
        print(f"  âœ… å¢å¼ºçš„'æ•…é€‰'æ¸…ç†")
        print(f"  âœ… ä¸­è‹±æ–‡æ ‡ç‚¹ç»Ÿä¸€")
        print(f"  âœ… è‡ªåŠ¨éªŒè¯åŠŸèƒ½")
        
        print(f"\nâœ… v1.2 æ”¹è¿›ï¼ˆå·²ä¿ç•™ï¼‰:")
        print(f"  âœ… ç©ºè¡Œæ¸…ç†å¢å¼º")
        print(f"  âœ… è¶…é•¿è¡Œè‡ªåŠ¨åˆ†å‰²")
        print(f"  âœ… æ•°å­¦å˜é‡æ™ºèƒ½æ£€æµ‹")
        print(f"  âœ… é€‰é¡¹è§£æå¢å¼º")
        
        # ğŸ†• v1.3ï¼šæ˜¾ç¤ºéªŒè¯ç»“æœ
        if warnings:
            print(f"\nâš ï¸  éªŒè¯å‘ç° {len(warnings)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for warning in warnings:
                print(f"  {warning}")
            print("\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ AI Agent è¿›è¡Œäººå·¥æ£€æŸ¥")
        else:
            print(f"\nâœ… éªŒè¯é€šè¿‡ï¼šæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. AI Agent è¯»å–æ­¤æ–‡ä»¶è¿›è¡Œç²¾ä¿®")
        print("  2. AI Agent æŸ¥çœ‹ images/ ä¸­çš„å›¾ç‰‡")
        print("  3. AI Agent ç”Ÿæˆ TikZ ä»£ç ")
        print("  4. è¾“å‡ºæœ€ç»ˆçš„ exam_final.tex")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

